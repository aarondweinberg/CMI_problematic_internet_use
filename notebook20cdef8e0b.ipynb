{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":81933,"databundleVersionId":9643020,"sourceType":"competition"}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n#import numpy as np # linear algebra\n#import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#import os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-13T12:24:13.440233Z","iopub.execute_input":"2024-12-13T12:24:13.441416Z","iopub.status.idle":"2024-12-13T12:24:13.446473Z","shell.execute_reply.started":"2024-12-13T12:24:13.441366Z","shell.execute_reply":"2024-12-13T12:24:13.445249Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"**Import Packages and Classes**","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os #reading and writing files\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.impute import KNNImputer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.base import BaseEstimator, TransformerMixin","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T15:03:28.589108Z","iopub.execute_input":"2024-12-13T15:03:28.590044Z","iopub.status.idle":"2024-12-13T15:03:30.461291Z","shell.execute_reply.started":"2024-12-13T15:03:28.589996Z","shell.execute_reply":"2024-12-13T15:03:30.459853Z"}},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":"**Our Custom Imputer and Zone Computer**","metadata":{}},{"cell_type":"code","source":"#Define two custom imputer objects.\n\n## Define KNN custom imputer\nclass Custom_KNN_Imputer(BaseEstimator, TransformerMixin):\n    # Class Constructor \n    # This allows you to initiate the class when you call Custom_KNN_Imputer\n    def __init__(self):\n        # I want to initiate each object with both a KNNImputer and StandardScaler object/method\n        self.KNNImputer = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')\n        self.StandardScaler = StandardScaler()\n\n    \n    # For my fit method I'm just going to \"steal\" KNNImputers's fit method using a curated collection of predictors\n    def fit(self, X, y = None ):\n        feature_list = X.columns.tolist()\n        if 'id' in feature_list:\n            feature_list.remove('id')\n        if 'sii' in feature_list:\n            feature_list.remove('sii')\n        feature_list = [x for x in feature_list if 'PCIAT' not in x]\n        feature_list = [x for x in feature_list if 'Zone' not in x]\n        feature_list = [x for x in feature_list if 'Season' not in x]\n        # Reset the index in X\n        X = X.reset_index(drop=True)\n        self.StandardScaler.fit(X[feature_list])\n        # I'm never sure if we need the .values and/or .reshape(-1,1)\n        #self.KNNImputer.fit(X[feature_list].values.reshape(-1,1))\n        self.KNNImputer.fit(X[feature_list])\n        return self\n    \n    # Now I want to transform the columns in feature list and return it with imputed values that have been un-transformed\n    def transform(self, X, y = None):\n        feature_list = X.columns.tolist()\n        if 'id' in feature_list:\n            feature_list.remove('id')\n        if 'sii' in feature_list:\n            feature_list.remove('sii')\n        feature_list = [x for x in feature_list if 'PCIAT' not in x]\n        feature_list = [x for x in feature_list if 'Zone' not in x]\n        feature_list = [x for x in feature_list if 'Season' not in x]\n        copy_X = X.copy().reset_index(drop=True)\n        copy_X[feature_list] = self.KNNImputer.transform(copy_X[feature_list])\n        copy_X2 = self.StandardScaler.inverse_transform(copy_X[feature_list])\n        df2 = pd.DataFrame(copy_X2, columns=feature_list)\n        copy_X[feature_list]=copy_X[feature_list].fillna(df2[feature_list])\n        return copy_X\n\n\n\n\n## Define MICE custom imputer\nclass Custom_MICE_Imputer(BaseEstimator, TransformerMixin):\n    # Class Constructor \n    # This allows you to initiate the class when you call Custom_KNN_Imputer\n    def __init__(self):\n        # I want to initiate each object with both a KNNImputer and StandardScaler object/method\n        self.MICEImputer = IterativeImputer(max_iter=10, random_state=497)\n\n    \n    # For my fit method I'm just going to \"steal\" IterativeImputers's fit method using a curated collection of predictors\n    def fit(self, Z, y = None):\n        feature_list = Z.columns.tolist()\n        if 'id' in feature_list:\n            feature_list.remove('id')\n        if 'sii' in feature_list:\n            feature_list.remove('sii')\n        feature_list = [x for x in feature_list if 'PCIAT' not in x]\n        feature_list = [x for x in feature_list if 'Zone' not in x]\n        feature_list = [x for x in feature_list if 'Season' not in x]\n        Z = Z.reset_index(drop=True)\n        self.MICEImputer.fit(Z[feature_list])\n        return self\n    \n    # Now I want to transform the columns in feature list and return it with imputed values that have been un-transformed\n    def transform(self, Z, y = None):\n        feature_list = Z.columns.tolist()\n        if 'id' in feature_list:\n            feature_list.remove('id')\n        if 'sii' in feature_list:\n            feature_list.remove('sii')\n        feature_list = [x for x in feature_list if 'PCIAT' not in x]\n        feature_list = [x for x in feature_list if 'Zone' not in x]\n        feature_list = [x for x in feature_list if 'Season' not in x]\n        copy_Z = Z.copy()\n        copy_Z = copy_Z.reset_index(drop=True)\n        df2 = self.MICEImputer.transform(copy_Z[feature_list])\n        df3 = pd.DataFrame(df2, columns=feature_list)\n        copy_Z[feature_list]=copy_Z[feature_list].fillna(df3[feature_list])\n        return copy_Z\n    \n\n####Now defining zone functions.\n\n# Compute values for the 'FGC-FGC_SR_Zone' that is equal to 1 if any of the following are true:\n# Basic_Demos-Sex==0 and FGC-FGC_SR >= 8\n# Basic_Demos-Sex==1 and FGC-FGC_SR >= 9 and Basic_Demos-Age is between 5 and 10\n# Basic_Demos-Sex==1 and FGC-FGC_SR >= 10 and Basic_Demos-Age is between 11 and 14\n# Basic_Demos-Sex==1 and FGC-FGC_SR >= 12 and Basic_Demos-Age is at least 15\n# Note that Basic_Demos-Sex is coded as 0=Male and 1=Female\n\ndef sitreachzone(sex, age, sr):\n    try:\n        if np.isnan(sr) or np.isnan(sex) or np.isnan(age):\n            return np.nan\n        elif sex == 0 and sr>=8:\n            return 1\n        elif sex == 1 and age >= 15 and sr >= 12:\n            return 1\n        elif sex == 1 and age >= 11 and sr >= 10:\n            return 1\n        elif sex == 1 and age >= 5 and sr >= 9:\n            return 1\n        else:\n            return 0\n    except:\n        return np.nan\n\n# Compute values for the 'FGC-FGC_CU_Zone' that is equal to 1 if any of the following are true:\n# Basic_Demos-Sex==0 and FGC-FGC_CU >= 2 and Basic_Demos-Age is between 5 and 6\n# Basic_Demos-Sex==0 and FGC-FGC_CU >= 4 and Basic_Demos-Age is 7\n# Basic_Demos-Sex==0 and FGC-FGC_CU >= 6 and Basic_Demos-Age is 8\n# Basic_Demos-Sex==0 and FGC-FGC_CU >= 9 and Basic_Demos-Age is 9\n# Basic_Demos-Sex==0 and FGC-FGC_CU >= 12 and Basic_Demos-Age is 10\n# Basic_Demos-Sex==0 and FGC-FGC_CU >= 15 and Basic_Demos-Age is 11\n# Basic_Demos-Sex==0 and FGC-FGC_CU >= 18 and Basic_Demos-Age is 12\n# Basic_Demos-Sex==0 and FGC-FGC_CU >= 21 and Basic_Demos-Age is 13\n# Basic_Demos-Sex==0 and FGC-FGC_CU >= 24 and Basic_Demos-Age is at least 14\n# Basic_Demos-Sex==1 and FGC-FGC_CU >= 2 and Basic_Demos-Age is between 5 and 6\n# Basic_Demos-Sex==1 and FGC-FGC_CU >= 4 and Basic_Demos-Age is 7\n# Basic_Demos-Sex==1 and FGC-FGC_CU >= 6 and Basic_Demos-Age is 8\n# Basic_Demos-Sex==1 and FGC-FGC_CU >= 9 and Basic_Demos-Age is 9\n# Basic_Demos-Sex==1 and FGC-FGC_CU >= 12 and Basic_Demos-Age is 10\n# Basic_Demos-Sex==1 and FGC-FGC_CU >= 15 and Basic_Demos-Age is 11\n# Basic_Demos-Sex==1 and FGC-FGC_CU >= 18 and Basic_Demos-Age is at least 12\n\ndef curlupzone(sex, age, cu):\n    try:\n        if np.isnan(sex) or np.isnan(age) or np.isnan(cu):\n            return np.nan\n        elif sex == 0:\n            if (age >= 14 and cu >= 24) or (age == 13 and cu >= 21) or (age == 12 and cu >= 18) or (age == 11 and cu >= 15) or (age == 10 and cu >= 12) or (age == 9 and cu >= 9) or (age == 8 and cu >= 6) or (age == 7 and cu >= 4) or (age <= 6 and cu >= 2):\n                return 1\n            else:\n                return 0\n        elif sex == 1:\n            if (age >= 12 and cu >= 18) or (age == 11 and cu >= 15) or (age == 10 and cu >= 12) or (age == 9 and cu >= 9) or (age == 8 and cu >= 6) or (age == 7 and cu >= 4) or (age <= 6 and cu >= 2):\n                return 1\n            else:\n                return 0\n    except:\n        return np.nan\n\n# Compute values for the 'FGC-FGC_PU_Zone' that is equal to 1 if any of the following are true:\n# Basic_Demos-Sex==0 and FGC-FGC_PU >= 3 and Basic_Demos-Age is between 5 and 6\n# Basic_Demos-Sex==0 and FGC-FGC_PU >= 4 and Basic_Demos-Age is 7\n# Basic_Demos-Sex==0 and FGC-FGC_PU >= 5 and Basic_Demos-Age is 8\n# Basic_Demos-Sex==0 and FGC-FGC_PU >= 6 and Basic_Demos-Age is 9\n# Basic_Demos-Sex==0 and FGC-FGC_PU >= 7 and Basic_Demos-Age is 10\n# Basic_Demos-Sex==0 and FGC-FGC_PU >= 8 and Basic_Demos-Age is 11\n# Basic_Demos-Sex==0 and FGC-FGC_PU >= 10 and Basic_Demos-Age is 12\n# Basic_Demos-Sex==0 and FGC-FGC_PU >= 12 and Basic_Demos-Age is 13\n# Basic_Demos-Sex==0 and FGC-FGC_PU >= 14 and Basic_Demos-Age is 14\n# Basic_Demos-Sex==0 and FGC-FGC_PU >= 16 and Basic_Demos-Age is 15\n# Basic_Demos-Sex==0 and FGC-FGC_PU >= 18 and Basic_Demos-Age is at least 16\n# Basic_Demos-Sex==1 and FGC-FGC_PU >= 3 and Basic_Demos-Age is between 5 and 6\n# Basic_Demos-Sex==1 and FGC-FGC_PU >= 4 and Basic_Demos-Age is 7\n# Basic_Demos-Sex==1 and FGC-FGC_PU >= 5 and Basic_Demos-Age is 8\n# Basic_Demos-Sex==1 and FGC-FGC_PU >= 6 and Basic_Demos-Age is 9\n# Basic_Demos-Sex==1 and FGC-FGC_PU >= 7 and Basic_Demos-Age is at least 10\n\ndef pullupzone(sex, age, pu):\n    try:\n        if np.isnan(sex) or np.isnan(age) or np.isnan(pu):\n            return np.nan\n        elif sex == 0:\n            if (age >= 16 and pu >= 18) or (age == 15 and pu >= 16) or (age == 14 and pu >= 14) or (age == 13 and pu >= 12) or (age == 12 and pu >= 10) or (age == 11 and pu >= 8) or (age == 10 and pu >= 7) or (age == 9 and pu >= 6) or (age == 8 and pu >= 5) or (age == 7 and pu >= 4) or (age <= 6 and pu >= 2):\n                return 1\n            else:\n                return 0\n        elif sex == 1:\n            if (age >= 10 and pu >= 7) or (age == 9 and pu >= 6) or (age == 8 and pu >= 5) or (age == 7 and pu >= 4) or (age <= 6 and pu >= 3):\n                return 1\n            else:\n                return 0\n    except:\n        return np.nan\n\n# Comtlte values for the 'FGC-FGC_TL_Zone' that is equal to 1 if any of the following are true:\n# FGC-FGC_TL >= 6 and Basic_Demos-Age is between 5 and 9\n# FGC-FGC_TL >= 9 and Basic_Demos-Age is at least 10\n\ndef tlzone(age, tl):\n    try:\n        if np.isnan(tl) or np.isnan(age):\n            return np.nan\n        elif (age >= 10 and tl >= 9) or (age <= 9 and tl >= 6):\n            return 1\n        else:\n            return 0\n    except:\n        return np.nan\n\n# Comtlte values for the 'PAQ_MVPA' that is equal to 1 if any of the following are true:\n# PAQ_Total >= 2.73 and Basic_Demos-Age is between 5 and 13\n# PAQ_Total >= 2.75 and Basic_Demos-Age is at least 14\n\ndef paqzone(age, paq):\n    try:\n        if np.isnan(paq) or np.isnan(age):\n            return np.nan\n        elif (age >= 14 and paq >= 2.75) or (age <= 13 and paq >= 2.73):\n            return 1\n        else:\n            return 0\n    except:\n        return np.nan\n\n###Custom encoder function\n# \ndef zone_encoder(df):\n    df_copy = df.copy()\n\n    if 'FGC-FGC_SR_Zone' in df_copy.columns:\n        if 'Basic_Demos-Age' in df_copy.columns and 'Basic_Demos-Sex' in df_copy.columns and 'FGC-FGC_SR' in df_copy.columns:\n            df_copy['FGC-FGC_SR_Zone'] = df_copy.apply(lambda x: sitreachzone(x['Basic_Demos-Sex'], x['Basic_Demos-Age'], x['FGC-FGC_SR']), axis=1)\n        else:\n            df_copy['FGC-FGC_SR_Zone'] = df_copy['FGC-FGC_SR_Zone'].fillna(df_copy['FGC-FGC_SR_Zone'].mean())\n    if 'FGC-FGC_CU_Zone' in df_copy.columns:\n        if 'Basic_Demos-Age' in df_copy.columns and 'Basic_Demos-Sex' in df_copy.columns and 'FGC-FGC_CU' in df_copy.columns:\n            df_copy['FGC-FGC_CU_Zone'] = df_copy.apply(lambda x: curlupzone(x['Basic_Demos-Sex'], x['Basic_Demos-Age'], x['FGC-FGC_CU']), axis=1)\n        else:\n            df_copy['FGC-FGC_CU_Zone'] = df_copy['FGC-FGC_CU_Zone'].fillna(df_copy['FGC-FGC_CU_Zone'].mean())\n    if 'FGC-FGC_PU_Zone' in df_copy.columns:\n        if 'Basic_Demos-Age' in df_copy.columns and 'Basic_Demos-Sex' in df_copy.columns and 'FGC-FGC_PU' in df_copy.columns:\n            df_copy['FGC-FGC_PU_Zone'] = df_copy.apply(lambda x: pullupzone(x['Basic_Demos-Sex'], x['Basic_Demos-Age'], x['FGC-FGC_PU']), axis=1)\n        else:\n            df_copy['FGC-FGC_PU_Zone'] = df_copy['FGC-FGC_PU_Zone'].fillna(df_copy['FGC-FGC_PU_Zone'].mean())\n    if 'FGC-FGC_TL_Zone' in df_copy.columns:\n        if 'Basic_Demos-Age' in df_copy.columns and 'FGC-FGC_TL' in df_copy.columns:\n            df_copy['FGC-FGC_TL_Zone'] = df_copy.apply(lambda x: tlzone(x['Basic_Demos-Age'], x['FGC-FGC_TL']), axis=1)\n        else:\n            df_copy['FGC-FGC_TL_Zone'] = df_copy['FGC-FGC_TL_Zone'].fillna(df_copy['FGC-FGC_TL_Zone'].mean())\n    if 'PAQ_Zone' in df_copy.columns:\n        if 'Basic_Demos-Age' in df_copy.columns and 'PAQ_Total' in df_copy.columns:\n            df_copy['PAQ_Zone'] = df_copy.apply(lambda x: tlzone(x['Basic_Demos-Age'], x['PAQ_Total']), axis=1)\n        else:\n            df_copy['PAQ_Zone'] = df_copy.apply(lambda x: paqzone(x['Basic_Demos-Age'], x['PAQ_Total']), axis=1)\n    return df_copy   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T15:03:34.249041Z","iopub.execute_input":"2024-12-13T15:03:34.250901Z","iopub.status.idle":"2024-12-13T15:03:34.296970Z","shell.execute_reply.started":"2024-12-13T15:03:34.250851Z","shell.execute_reply":"2024-12-13T15:03:34.295560Z"}},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":"**Code the Accelerometer Data**\n\nWe segment each actigraphy file into 5-minute bouts\n\nFor each bout, we indicate whether the mean ENMO value is above thresholds identified in the research literature\n\nWe also identify how often the anglez was positive\n\nThen we create a dataframe that records these values for each participant ID","metadata":{}},{"cell_type":"code","source":"#ENMO cutoffs in mg for MVPA\nmvpa_cutoff1 = 0.192\nmvpa_cutoff2 = 0.110\n\n# Number of 'active' bouts required for a day to count as 'active'\nactive_bout_cutoff = 150\n\n# Specify the length of the bouts\nboutlength = '5min'\n\n# Maximum number of 5-minute bouts that can be imputed as zeroes to account for the accelerometer not collected data when at rest\nimpute_max = 6\n\n# Minimum number of 5-second intervals (within a 5-minute bout) that need to have data for the bout to be counted\nimpute_sec_min = 29\n\n# Create a new data frame with columns 'ID', 'ENMO_Avg_Active_Days_MVPA192', 'ENMO_Avg_Active_Days_MVPA110', and 'Positive_Anglez_Active_Days\naccel = pd.DataFrame(columns=['ID', 'ENMO_Avg_Active_Days_MVPA192', 'ENMO_Avg_Active_Days_MVPA110', 'Positive_Anglez_Active_Days'])\n\n\n# Walk through the files in the directory\n# For testing purposes, we'll just do this for test data\nfor dirname, _, filenames in os.walk('/kaggle/input/child-mind-institute-problematic-internet-use'):\n#for dirname, _, filenames in os.walk('/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet'):\n    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n        #Check to see if filename is a parquet file; if it is, read the file and extract the ID from the directory name\n        if filename.endswith('.parquet'):\n            data = pd.read_parquet(os.path.join(dirname, filename))\n            id = dirname[-8:]\n\n            # Remove any rows where the variable non-wear_flag is nonzero\n            data = data[data['non-wear_flag'] == 0]\n\n            # Change the time_of_day variable to a datetime and make it into the index\n            data['dt'] = pd.to_datetime(data['time_of_day'])\n            data['dt_mod'] = data['dt'] + pd.to_timedelta(data['relative_date_PCIAT'], unit='D')\n            data.set_index('dt_mod', inplace=True)\n\n            # Create a new data frame that counts the number of valid data points within each 5-minute ('boutlength') interval\n            # This will later be used to exclude intervals that had fewer than 30 (out of 60) valid data points\n            data['count'] = 1\n            number_of_data_points = data.resample(boutlength).agg({'count':'sum'})\n            data.drop('count', axis=1, inplace=True)\n\n            # Create 5-minute \"bouts\" of averaged data and incorporate the number of valid data points within each interval as a new variable 'count'\n            data_resampled_5min = data.resample(boutlength).mean()\n            data_resampled_5min = data_resampled_5min.merge(number_of_data_points, left_index=True, right_index=True)\n\n            # Some of the accelerometers stopped collecting data if they were stationary (but still on/worn)\n            # This next section is an attempt to identify and fill in these seemingly missing values with \"0\" for the enmo value\n            # It does this by identifying the length of each sequence of NaN values and filling them with 0 if thery are at most 30 minutes long\n            # This also restricts this process to 5-minute bouts that had data for at least 30 of the 5-second-intervals within the bout\n            data_resampled_5min['enmogroup'] = data_resampled_5min['enmo'].notna().cumsum()\n            enmogroupcount = data_resampled_5min.groupby(by=[\"enmogroup\"]).size().to_frame()\n            enmogroupcount = enmogroupcount.rename(columns={0: 'enmogroupsize'})\n            data_resampled_5min = data_resampled_5min.merge(enmogroupcount, how='left', left_on='enmogroup', right_index=True)\n            data_resampled_5min['smallinterval'] = (data_resampled_5min['enmogroupsize'] < impute_max+2) & (data_resampled_5min['count']>impute_sec_min)\n            data_resampled_5min['filled_enmo'] = np.where(data_resampled_5min.smallinterval, data_resampled_5min.enmo.fillna(0), data_resampled_5min.enmo)\n\n            # Also fill in only anglez values where the count is large enough\n            data_resampled_5min['filled_anglez'] = np.where(data_resampled_5min['count']>impute_sec_min, data_resampled_5min.anglez, np.nan)\n\n            # The next code chunk will create a new data frame that lists the total number of valid bouts for the participant\n            # and will count the number of bouts with filled_enmo values over a particular threshold\n            # and then count the number of bouts with positive anglez values\n\n            # Start by counting the number of valid bouts in each day as a data frame\n            boutcount_filled = data_resampled_5min.groupby(data_resampled_5min.index.date).count()['filled_enmo'].to_frame()\n            boutcount_filled = boutcount_filled.rename(columns={'filled_enmo': 'valid_bouts'})\n\n            # Count the number of bouts in each day with filled_enmo at least mvpa_cutoff1\n            boutcount_MVPA1 = data_resampled_5min[data_resampled_5min['filled_enmo'] >= mvpa_cutoff1].groupby(data_resampled_5min[data_resampled_5min['filled_enmo'] >= mvpa_cutoff1].index.date).count()['filled_enmo'].to_frame()\n            boutcount_MVPA1 = boutcount_MVPA1.rename(columns={'filled_enmo': 'MVPA_bouts_over_cutoff1'})\n            boutcount = boutcount_filled.merge(boutcount_MVPA1, how='left', left_index=True, right_index=True)\n\n            # Count the number of bouts in each day with filled_enmo at least mvpa_cutoff2\n            boutcount_MVPA2 = data_resampled_5min[data_resampled_5min['filled_enmo'] >= mvpa_cutoff2].groupby(data_resampled_5min[data_resampled_5min['filled_enmo'] >= mvpa_cutoff2].index.date).count()['filled_enmo'].to_frame()\n            boutcount_MVPA2 = boutcount_MVPA2.rename(columns={'filled_enmo': 'MVPA_bouts_over_cutoff2'})\n            boutcount = boutcount.merge(boutcount_MVPA2, how='left', left_index=True, right_index=True)\n\n            # Count the number of bouts in each day with anglez at least 0\n            boutcount_anglez = data_resampled_5min[data_resampled_5min['filled_anglez'] > 0].groupby(data_resampled_5min[data_resampled_5min['filled_anglez'] > 0].index.date).count()['filled_anglez'].to_frame()\n            boutcount_anglez = boutcount_anglez.rename(columns={'filled_anglez': 'Positive_Anglez_Bouts'})\n            boutcount = boutcount.merge(boutcount_anglez, how='left', left_index=True, right_index=True)\n\n            # Compute a new variable 'included_day' to be True if valid_bouts is at least active_bout_cutoff\n            boutcount['included_day'] = boutcount['valid_bouts'] >= active_bout_cutoff\n\n            # Compute the mean of MVPA bouts over each cutoff\n            # Note: We are only using the \"included day\" data in our final analysis, so we'll restrict the output accordingly\n            MVPA_mean1 = boutcount[boutcount['included_day'] == True]['MVPA_bouts_over_cutoff1'].mean()\n            MVPA_mean2 = boutcount[boutcount['included_day'] == True]['MVPA_bouts_over_cutoff2'].mean()\n            Anglez_mean1 = boutcount[boutcount['included_day'] == True]['Positive_Anglez_Bouts'].mean()\n\n            new_row = pd.DataFrame({\"ID\": [id], \"ENMO_Avg_Active_Days_MVPA192\": [MVPA_mean1], \"ENMO_Avg_Active_Days_MVPA110\": [MVPA_mean2], \"Positive_Anglez_Active_Days\": [Anglez_mean1]})\n            \n            #Create a new row in accel where 'ID'=id, 'ENMO_Avg_Active_Days_MVPA192'=MVPA_mean1, 'ENMO_Avg_Active_Days_MVPA110'=MVPA_mean2, and 'Positive_Anglez_Active_Days'=Anglez_mean1\n            accel = pd.concat([accel, new_row], ignore_index=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T15:04:05.406552Z","iopub.execute_input":"2024-12-13T15:04:05.407464Z","iopub.status.idle":"2024-12-13T15:18:23.288570Z","shell.execute_reply.started":"2024-12-13T15:04:05.407421Z","shell.execute_reply":"2024-12-13T15:18:23.286696Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_24/1096188575.py:99: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n  accel = pd.concat([accel, new_row], ignore_index=True)\n","output_type":"stream"}],"execution_count":34},{"cell_type":"markdown","source":"**Load the Data and Join with Accelerometer Data**","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\ntrain = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\n\n# Join train/test and accel  on the 'id' column and accel on the 'ID' column\ntest = test.join(accel.set_index('ID'), on='id', how='left')\ntrain = train.join(accel.set_index('ID'), on='id', how='left')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T15:22:30.639563Z","iopub.execute_input":"2024-12-13T15:22:30.640032Z","iopub.status.idle":"2024-12-13T15:22:30.700599Z","shell.execute_reply.started":"2024-12-13T15:22:30.639991Z","shell.execute_reply":"2024-12-13T15:22:30.699427Z"}},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":"**Data Cleaning**","metadata":{}},{"cell_type":"code","source":"###################################\n#Now we do a variety of data cleaning.\n# Create a new variable 'FGC-FGC_SR' that is the mean of FGC-FGC_SRL and FGC-FGC_SRR\ntrain['FGC-FGC_SR'] = train[['FGC-FGC_SRL', 'FGC-FGC_SRR']].mean(axis=1)\ntest['FGC-FGC_SR'] = test[['FGC-FGC_SRL', 'FGC-FGC_SRR']].mean(axis=1)\n\n# Remove the old sit & reach variables\ntrain = train.drop(columns=['FGC-FGC_SRL', 'FGC-FGC_SRR', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR_Zone'])\ntest = test.drop(columns=['FGC-FGC_SRL', 'FGC-FGC_SRR', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR_Zone'])\n\n# Create a new variable 'FGC-FGC_SR_Zone' that is equal to 1 if any of the following are true:\n# Basic_Demos-Sex==0 and FGC-FGC_SR >= 8\n# Basic_Demos-Sex==1 and FGC-FGC_SR_Zone >= 9 and Basic_Demos-Age is between 5 and 10\n# Basic_Demos-Sex==1 and FGC-FGC_SR_Zone >= 10 and Basic_Demos-Age is between 11 and 14\n# Basic_Demos-Sex==1 and FGC-FGC_SR_Zone >= 12 and Basic_Demos-Age is at least 15\n\n# One way to do this is to define a function that would take sex, age, and SR value as inputs and output 1 or 0\ndef sitreachzone(sex, age, sr):\n    try:\n        if np.isnan(sr):\n            return np.nan\n        elif sex == 0 and sr>=8:\n            return 1\n        elif sex == 1 and age >= 15 and sr >= 12:\n            return 1\n        elif sex == 1 and age >= 11 and sr >= 10:\n            return 1\n        elif sex == 1 and age >= 5 and sr >= 9:\n            return 1\n        else:\n            return 0\n    except:\n        return np.nan\n\n# Apply sitreachzone to create a new column using the columns Basic_Demos-Sex, Basic_Demos-Age, and FGC-FGC_SR as inputs\ntrain['FGC-FGC_SR_Zone'] = train.apply(lambda x: sitreachzone(x['Basic_Demos-Sex'], x['Basic_Demos-Age'], x['FGC-FGC_SR']), axis=1)\ntest['FGC-FGC_SR_Zone'] = test.apply(lambda x: sitreachzone(x['Basic_Demos-Sex'], x['Basic_Demos-Age'], x['FGC-FGC_SR']), axis=1)\n\n# Create a new variable that is 1 when PAQA/C Total is at least 2.75/2.73, 0 if it's less than these cutoffs, and NaN if PAQA/C is NaN\ntrain['PAQA_Zone'] = np.where(train['PAQ_A-PAQ_A_Total']>=2.75, 1, 0)\ntrain['PAQA_Zone'] = np.where(train['PAQ_A-PAQ_A_Total'].isnull(), np.nan, train['PAQA_Zone'])\ntrain['PAQC_Zone'] = np.where(train['PAQ_C-PAQ_C_Total']>=2.73, 1, 0)\ntrain['PAQC_Zone'] = np.where(train['PAQ_C-PAQ_C_Total'].isnull(), np.nan, train['PAQC_Zone'])\ntest['PAQA_Zone'] = np.where(test['PAQ_A-PAQ_A_Total']>=2.75, 1, 0)\ntest['PAQA_Zone'] = np.where(test['PAQ_A-PAQ_A_Total'].isnull(), np.nan, test['PAQA_Zone'])\ntest['PAQC_Zone'] = np.where(test['PAQ_C-PAQ_C_Total']>=2.73, 1, 0)\ntest['PAQC_Zone'] = np.where(test['PAQ_C-PAQ_C_Total'].isnull(), np.nan, test['PAQC_Zone'])\n\n# Create new variables that merge the three PAQA/C variables\ntrain['PAQ_Total']=train['PAQ_C-PAQ_C_Total']\ntrain.loc[train['PAQ_Total'].isnull(),'PAQ_Total']=train['PAQ_A-PAQ_A_Total']\ntest['PAQ_Total']=test['PAQ_C-PAQ_C_Total']\ntest.loc[test['PAQ_Total'].isnull(),'PAQ_Total']=test['PAQ_A-PAQ_A_Total']\n\ntrain['PAQ_Season']=train['PAQ_C-Season']\ntrain.loc[train['PAQ_Season'].isnull(),'PAQ_Season']=train['PAQ_A-Season']\ntest['PAQ_Season']=test['PAQ_C-Season']\ntest.loc[test['PAQ_Season'].isnull(),'PAQ_Season']=test['PAQ_A-Season']\n\ntrain['PAQ_Zone']=train['PAQC_Zone']\ntrain.loc[train['PAQ_Zone'].isnull(),'PAQ_Zone']=train['PAQA_Zone']\ntest['PAQ_Zone']=test['PAQC_Zone']\ntest.loc[test['PAQ_Zone'].isnull(),'PAQ_Zone']=test['PAQA_Zone']\n\n# Drop the PAQ variables we no longer need\ntrain=train.drop(columns=['PAQ_C-PAQ_C_Total', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season', 'PAQ_A-Season', 'PAQA_Zone', 'PAQC_Zone'])\ntest=test.drop(columns=['PAQ_C-PAQ_C_Total', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season', 'PAQ_A-Season', 'PAQA_Zone', 'PAQC_Zone'])\n\n# Combine the minutes and seconds of Fitness_Endurance into a single number (total number of seconds)\ntrain['Fitness_Endurance_Total_Time_Sec'] = train['Fitness_Endurance-Time_Mins'] * 60 + train['Fitness_Endurance-Time_Sec']\ntest['Fitness_Endurance_Total_Time_Sec'] = test['Fitness_Endurance-Time_Mins'] * 60 + test['Fitness_Endurance-Time_Sec']\n\ntrain=train.drop(columns=['Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec'])\ntest=test.drop(columns=['Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec'])\n\n# Remove the SDS-SDS_Total_T variable from train\ntrain=train.drop(columns=['SDS-SDS_Total_T'])\ntest=test.drop(columns=['SDS-SDS_Total_T'])\n\n# Remove the FGC-FGC_GSND, FGC-FGC_GSND_Zone, FGC-FGC_GSD, and FGC-FGC_GSD_Zone variables\ntrain=train.drop(columns=['FGC-FGC_GSND', 'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone'])\ntest=test.drop(columns=['FGC-FGC_GSND', 'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone'])\n\n# Create a list of numerical columns of type float. Note that these columns include the \"Zone\" variables which are really categorical/ordinal:\nfloat_columns = test.select_dtypes(include=['float']).columns\n\n# Change negative values to NaN\ntrain[train[float_columns] < 0] = np.nan\ntest[test[float_columns] < 0] = np.nan\n\n# For each variable that starts with 'Physical-' replace any values that are 0 with NaN\nfor column in train.columns:\n    if column.startswith('Physical-'):\n        train[column] = train[column].replace(0, np.nan)\nfor column in test.columns:\n    if column.startswith('Physical-'):\n        test[column] = test[column].replace(0, np.nan)\n\n# For each column in float_columns, identify entries that are 5 standard deviations above or below the mean and replace them with NaN\nfor column in float_columns:\n    train[column] = train[column].mask(train[column] > train[column].mean() + 5 * train[column].std())\n    train[column] = train[column].mask(train[column] < train[column].mean() - 5 * train[column].std())\n    test[column] = test[column].mask(test[column] > test[column].mean() + 5 * test[column].std())\n    test[column] = test[column].mask(test[column] < test[column].mean() - 5 * test[column].std())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T15:22:32.792772Z","iopub.execute_input":"2024-12-13T15:22:32.793193Z","iopub.status.idle":"2024-12-13T15:22:33.071495Z","shell.execute_reply.started":"2024-12-13T15:22:32.793139Z","shell.execute_reply":"2024-12-13T15:22:33.069104Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[42], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Create new variables that merge the three PAQA/C variables\u001b[39;00m\n\u001b[1;32m     50\u001b[0m train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPAQ_Total\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mtrain[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPAQ_C-PAQ_C_Total\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 51\u001b[0m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPAQ_Total\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misnull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPAQ_Total\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m=\u001b[39mtrain[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPAQ_A-PAQ_A_Total\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     52\u001b[0m test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPAQ_Total\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mtest[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPAQ_C-PAQ_C_Total\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     53\u001b[0m test\u001b[38;5;241m.\u001b[39mloc[test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPAQ_Total\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misnull(),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPAQ_Total\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mtest[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPAQ_A-PAQ_A_Total\u001b[39m\u001b[38;5;124m'\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexing.py:911\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[1;32m    910\u001b[0m iloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc\n\u001b[0;32m--> 911\u001b[0m \u001b[43miloc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexing.py:1937\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1935\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloc\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1936\u001b[0m     \u001b[38;5;66;03m# must come after setting of missing\u001b[39;00m\n\u001b[0;32m-> 1937\u001b[0m     indexer, value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_mask_setitem_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1939\u001b[0m \u001b[38;5;66;03m# align and set the values\u001b[39;00m\n\u001b[1;32m   1940\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m take_split_path:\n\u001b[1;32m   1941\u001b[0m     \u001b[38;5;66;03m# We have to operate column-wise\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexing.py:805\u001b[0m, in \u001b[0;36m_LocationIndexer._maybe_mask_setitem_value\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m    800\u001b[0m newkey \u001b[38;5;241m=\u001b[39m pi\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_scalar_indexer(icols, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m ndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    803\u001b[0m     \u001b[38;5;66;03m# e.g. test_loc_setitem_boolean_mask_allfalse\u001b[39;00m\n\u001b[1;32m    804\u001b[0m     \u001b[38;5;66;03m# test_loc_setitem_ndframe_values_alignment\u001b[39;00m\n\u001b[0;32m--> 805\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_align_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m (newkey, icols)\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(icols, np\u001b[38;5;241m.\u001b[39mndarray)\n\u001b[1;32m    810\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m icols\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    811\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(icols) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    812\u001b[0m ):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexing.py:2427\u001b[0m, in \u001b[0;36m_iLocIndexer._align_series\u001b[0;34m(self, indexer, ser, multiindex_indexer, using_cow)\u001b[0m\n\u001b[1;32m   2424\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m ser\n\u001b[1;32m   2425\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ser\u001b[38;5;241m.\u001b[39m_values\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m-> 2427\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_ix\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m   2429\u001b[0m \u001b[38;5;66;03m# 2 dims\u001b[39;00m\n\u001b[1;32m   2430\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m single_aligner:\n\u001b[1;32m   2431\u001b[0m     \u001b[38;5;66;03m# reindex along index\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/series.py:5153\u001b[0m, in \u001b[0;36mSeries.reindex\u001b[0;34m(self, index, axis, method, copy, level, fill_value, limit, tolerance)\u001b[0m\n\u001b[1;32m   5136\u001b[0m \u001b[38;5;129m@doc\u001b[39m(\n\u001b[1;32m   5137\u001b[0m     NDFrame\u001b[38;5;241m.\u001b[39mreindex,  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m   5138\u001b[0m     klass\u001b[38;5;241m=\u001b[39m_shared_doc_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mklass\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5151\u001b[0m     tolerance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   5152\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series:\n\u001b[0;32m-> 5153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/generic.py:5610\u001b[0m, in \u001b[0;36mNDFrame.reindex\u001b[0;34m(self, labels, index, columns, axis, method, copy, level, fill_value, limit, tolerance)\u001b[0m\n\u001b[1;32m   5607\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_multi(axes, copy, fill_value)\n\u001b[1;32m   5609\u001b[0m \u001b[38;5;66;03m# perform the reindex on the axes\u001b[39;00m\n\u001b[0;32m-> 5610\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reindex_axes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5611\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\n\u001b[1;32m   5612\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreindex\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/generic.py:5633\u001b[0m, in \u001b[0;36mNDFrame._reindex_axes\u001b[0;34m(self, axes, level, limit, tolerance, method, fill_value, copy)\u001b[0m\n\u001b[1;32m   5630\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   5632\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis(a)\n\u001b[0;32m-> 5633\u001b[0m new_index, indexer \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5634\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\n\u001b[1;32m   5635\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5637\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis_number(a)\n\u001b[1;32m   5638\u001b[0m obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(\n\u001b[1;32m   5639\u001b[0m     {axis: [new_index, indexer]},\n\u001b[1;32m   5640\u001b[0m     fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[1;32m   5641\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m   5642\u001b[0m     allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   5643\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexes/base.py:4429\u001b[0m, in \u001b[0;36mIndex.reindex\u001b[0;34m(self, target, method, level, limit, tolerance)\u001b[0m\n\u001b[1;32m   4426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot handle a non-unique multi-index!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4427\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_unique:\n\u001b[1;32m   4428\u001b[0m     \u001b[38;5;66;03m# GH#42568\u001b[39;00m\n\u001b[0;32m-> 4429\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot reindex on an axis with duplicate labels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4430\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4431\u001b[0m     indexer, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_indexer_non_unique(target)\n","\u001b[0;31mValueError\u001b[0m: cannot reindex on an axis with duplicate labels"],"ename":"ValueError","evalue":"cannot reindex on an axis with duplicate labels","output_type":"error"}],"execution_count":42},{"cell_type":"markdown","source":"**Feature Reduction**","metadata":{}},{"cell_type":"code","source":"# Rename the files for future code compatibility\n\ntrain_cleaned = train\ntest_cleaned= test","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Remove some variables we won't need\n\n# Remove the variables 'id', 'BIA-BIA_BMI'\ntrain_cleaned = train_cleaned.drop(['id', 'BIA-BIA_BMI'], axis=1)\n\n# Remove FGC-FGC_CU_Zone, FGC-FGC_PU_Zone, and FGC-FGC_TL_Zone\ntrain_cleaned = train_cleaned.drop(['FGC-FGC_CU_Zone', 'FGC-FGC_PU_Zone', 'FGC-FGC_TL_Zone'], axis=1)\n\n# Remove the following variables from train: BIA-BIA_BMR, BIA-BIA_TBW, BIA-BIA_ECW, BIA-BIA_LDM, BIA-BIA_ICW, BIA-BIA_SMM, BIA-BIA_DEE, BIA-BIA_LST, and BIA-BIA_BMC\ntrain_cleaned=train_cleaned.drop(columns=['BIA-BIA_BMR', 'BIA-BIA_TBW', 'BIA-BIA_ECW', 'BIA-BIA_LDM', 'BIA-BIA_ICW', 'BIA-BIA_SMM', 'BIA-BIA_DEE', 'BIA-BIA_LST', 'BIA-BIA_BMC'])\n\n# Remove the Fitness_Endurance-Max_Stage variable (based on previous exploration)\ntrain_cleaned = train_cleaned.drop(['Fitness_Endurance-Max_Stage'], axis=1)\n\n# Remove all variables with Season in their name\ntrain_cleaned = train_cleaned.loc[:,~train_cleaned.columns.str.contains('Season')]\n\n#We also remove these variables from the test data.\ntest_cleaned = test_cleaned.drop(['id', 'BIA-BIA_BMI'], axis=1)\ntest_cleaned = test_cleaned.drop(['FGC-FGC_CU_Zone', 'FGC-FGC_PU_Zone', 'FGC-FGC_TL_Zone'], axis=1)\ntest_cleaned = test_cleaned.drop(columns=['BIA-BIA_BMR', 'BIA-BIA_TBW', 'BIA-BIA_ECW', 'BIA-BIA_LDM', 'BIA-BIA_ICW', 'BIA-BIA_SMM', 'BIA-BIA_DEE', 'BIA-BIA_LST', 'BIA-BIA_BMC'])\ntest_cleaned = test_cleaned.drop(['Fitness_Endurance-Max_Stage'], axis=1)\ntest_cleaned = test_cleaned.loc[:,~test_cleaned.columns.str.contains('Season')]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**The Model**","metadata":{}},{"cell_type":"code","source":"#Create an initial list of predictor columns\n\npredictors = train_cleaned.columns.tolist()\nif 'id' in predictors:\n    predictors.remove('id')\nif 'sii' in predictors:\n    predictors.remove('sii')\npredictors = [x for x in predictors if 'PCIAT' not in x]\npredictors = [x for x in predictors if 'Season' not in x]\n\n# Create an augmented list that will be used for \npredictors_plus = predictors + ['PCIAT-PCIAT_Total']\n\n# The Fitness_Endurance predictor was creating havoc with the MLR models, so we'll omit it from the predictor list for those models\npredictors_less = [x for x in predictors if 'Fitness_Endurance_Total_Time_Sec' not in x]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set up SMOTE to create 128 (rather than 32) instances of sii=3\nsiiratios = {0: 1228, 1: 619, 2:315, 3:128}\noversample = SMOTE(sampling_strategy=siiratios)\n\n#We now generate a model using the full train set (from our original train-test split.)\n\n# Impute missing values\nmice = Custom_MICE_Imputer()\ntrain_cleaned_imputed = mice.fit_transform(train_cleaned)\ntrain_cleaned_imputed = zone_encoder(train_cleaned_imputed)\ntest_cleaned_imputed = mice.fit_transform(test_cleaned)\ntest_cleaned_imputed = zone_encoder(test_cleaned_imputed)\n\n# Oversample with SMOTE\nX, y = oversample.fit_resample(train_cleaned_imputed[predictors_plus], train_cleaned_imputed['sii'])\n\n# Create the gradient boost model suggested by the tuning\ngrad_pciat=GradientBoostingRegressor(learning_rate=0.1, max_depth= 3, max_features= 10, min_samples_leaf= 8, min_samples_split= 3, n_estimators= 50)\n\n# Fit and make predictions\ngrad_pciat.fit(X[predictors_less], X['PCIAT-PCIAT_Total'])\npred = grad_pciat.predict(test_cleaned_imputed[predictors])\n\n# \"Tuned\" bins.\nbins_mod = [0, 27, 39, 79, 100]\npred_bin_mod = np.digitize(pred, bins_mod)-1\n\nfinal_kappa = cohen_kappa_score(test_cleaned_imputed['sii'], pred_bin_mod, weights='quadratic')\n\nprint('Cohens Kappa Score:', final_kappa)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}