{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goals**\n",
    "\n",
    "The goal of this notebook is to explore feature selection, continuing from the AW EDA Exploration goals:\n",
    "\n",
    "9. Create a linear regression model using a greedy algorithm from the \"bottom up\"\n",
    "    1. Make a list of all numerical predictors and also a new empty data frame with 100(?) rows and the predictors as variables\n",
    "    2. Randomly select a predictor from the list and create a linear model\n",
    "    3. Randomly select a second predictor from the list and add it to the model\n",
    "    4. Perform an F test to see if the new model is significantly better than the old\n",
    "    5. Repeat until the F test is no longer significant\n",
    "    6. Record the predictors that are in the model in the newly-created data frame\n",
    "    7. Repeat the above steps 100 (??) times\n",
    "    8. Compute the mean for each predictor in the data frame. This should give some sense of the \"importance\" of each predictor\n",
    "10. Repeat the previous method but using a \"top down\" algorithm, starting with a full model and removing predictors one-by-one\n",
    "11. *Maybe* Trying to use PCA and either linear or KNN regression to see if it appears to improve prediction\n",
    "    * PCA on the entire set of predictors\n",
    "    * On each set of grouped predictors\n",
    "12. Using RandomForest Regression on the entire set of predictors and examining the importance matrix to try to find a potential list of predictors\n",
    "13. *Maybe* using XGBoost to do stuff. (Need to learn what this is)\n",
    "14. Removing highly-correlated predictors and using LASSO and using LASSO regression (with hyperparameter tuning) to identify important predictors\n",
    "15. Comparing the apparent predictive power of all the previous methods. If none stand out, then stick with linear regression(?)\n",
    "16. Start to engage more formally with the modeling process, using Kfold splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the Data**\n",
    "\n",
    "For the purpose of developing our model(s), we'll work with data that include the imputed outcome (PCIAT_Total and/or sii) scores AND have cleaned predictors.\n",
    "\n",
    "In the final version of our code, we'll work with data with cleaned predictors but won't have any access to the outcome scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the cleaned & predictor-imputed data\n",
    "train_cleaned=pd.read_csv('train_cleaned_outcome_imputed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using KNN to Impute Values of Predictor Variables**\n",
    "\n",
    "Our first code chunk will use a KNN algorithm with all available predictor columns, excluding the Zone and Season columns\n",
    "\n",
    "We'll start by making a list of quantitative predictor variables. Note that:\n",
    "* The Zone variables are computed from others; we'll re-compute their values after doing imputation\n",
    "* The list includes Basic_Demos-Sex. Although this is categorical, all participants have data for this variable, and it's useful for imputing other variables\n",
    "* We *could* convert the Season variables into dummy variables, but this seems like it would over-weight them for KNN imputation. So we're leaving them out.\n",
    "\n",
    "Then, we'll construct and use a KNN imputer with 5 neighbors to impute missing values.\n",
    "\n",
    "We'll wrapp all of this inside a custom imputer that can be called inside a pipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We'll need these\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "## Define our custom imputer\n",
    "class Custom_KNN_Imputer(BaseEstimator, TransformerMixin):\n",
    "    # Class Constructor \n",
    "    # This allows you to initiate the class when you call Custom_KNN_Imputer\n",
    "    def __init__(self):\n",
    "        # I want to initiate each object with both a KNNImputer and StandardScaler object/method\n",
    "        self.KNNImputer = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')\n",
    "        self.StandardScaler = StandardScaler()\n",
    "\n",
    "    \n",
    "    # For my fit method I'm just going to \"steal\" KNNImputers's fit method using a curated collection of predictors\n",
    "    def fit(self, X, y = None ):\n",
    "        feature_list = X.columns.tolist()\n",
    "        if 'id' in feature_list:\n",
    "            feature_list.remove('id')\n",
    "        if 'sii' in feature_list:\n",
    "            feature_list.remove('sii')\n",
    "        feature_list = [x for x in feature_list if 'PCIAT' not in x]\n",
    "        feature_list = [x for x in feature_list if 'Zone' not in x]\n",
    "        feature_list = [x for x in feature_list if 'Season' not in x]\n",
    "        self.StandardScaler.fit(X[feature_list])\n",
    "        # I'm never sure if we need the .values and/or .reshape(-1,1)\n",
    "        #self.KNNImputer.fit(X[feature_list].values.reshape(-1,1))\n",
    "        self.KNNImputer.fit(X[feature_list])\n",
    "        return self\n",
    "    \n",
    "    # Now I want to transform the columns in feature list and return it with imputed values that have been un-transformed\n",
    "    def transform(self, X, y = None):\n",
    "        feature_list = X.columns.tolist()\n",
    "        if 'id' in feature_list:\n",
    "            feature_list.remove('id')\n",
    "        if 'sii' in feature_list:\n",
    "            feature_list.remove('sii')\n",
    "        feature_list = [x for x in feature_list if 'PCIAT' not in x]\n",
    "        feature_list = [x for x in feature_list if 'Zone' not in x]\n",
    "        feature_list = [x for x in feature_list if 'Season' not in x]\n",
    "        copy_X = X.copy()\n",
    "        copy_X[feature_list] = self.KNNImputer.transform(copy_X[feature_list])\n",
    "        copy_X2 = self.StandardScaler.inverse_transform(copy_X[feature_list])\n",
    "        df2 = pd.DataFrame(copy_X2, columns=feature_list)\n",
    "        copy_X[feature_list]=copy_X[feature_list].fillna(df2[feature_list])\n",
    "        return copy_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A Custom MICE Imputer**\n",
    "\n",
    "Next, we'll try to take the above code and turn it into a custom imputer that can be used inside a pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We'll need these\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "## Define our custom imputer\n",
    "class Custom_MICE_Imputer(BaseEstimator, TransformerMixin):\n",
    "    # Class Constructor \n",
    "    # This allows you to initiate the class when you call Custom_KNN_Imputer\n",
    "    def __init__(self):\n",
    "        # I want to initiate each object with both a KNNImputer and StandardScaler object/method\n",
    "        self.MICEImputer = IterativeImputer(max_iter=10, random_state=497)\n",
    "\n",
    "    \n",
    "    # For my fit method I'm just going to \"steal\" IterativeImputers's fit method using a curated collection of predictors\n",
    "    def fit(self, X, y = None ):\n",
    "        feature_list = X.columns.tolist()\n",
    "        if 'id' in feature_list:\n",
    "            feature_list.remove('id')\n",
    "        if 'sii' in feature_list:\n",
    "            feature_list.remove('sii')\n",
    "        feature_list = [x for x in feature_list if 'PCIAT' not in x]\n",
    "        feature_list = [x for x in feature_list if 'Zone' not in x]\n",
    "        feature_list = [x for x in feature_list if 'Season' not in x]\n",
    "        self.MICEImputer.fit(X[feature_list])\n",
    "        return self\n",
    "    \n",
    "    # Now I want to transform the columns in feature list and return it with imputed values that have been un-transformed\n",
    "    def transform(self, X, y = None):\n",
    "        feature_list = X.columns.tolist()\n",
    "        if 'id' in feature_list:\n",
    "            feature_list.remove('id')\n",
    "        if 'sii' in feature_list:\n",
    "            feature_list.remove('sii')\n",
    "        feature_list = [x for x in feature_list if 'PCIAT' not in x]\n",
    "        feature_list = [x for x in feature_list if 'Zone' not in x]\n",
    "        feature_list = [x for x in feature_list if 'Season' not in x]\n",
    "        copy_X = X.copy()\n",
    "        df2 = self.MICEImputer.transform(copy_X[feature_list])\n",
    "        df3 = pd.DataFrame(df2, columns=feature_list)\n",
    "        copy_X[feature_list]=copy_X[feature_list].fillna(df3[feature_list])\n",
    "        return copy_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "\n",
    "predictors = train_cleaned.columns.tolist()\n",
    "if 'id' in predictors:\n",
    "    predictors.remove('id')\n",
    "if 'sii' in predictors:\n",
    "    predictors.remove('sii')\n",
    "predictors = [x for x in predictors if 'PCIAT' not in x]\n",
    "predictors = [x for x in predictors if 'Season' not in x]\n",
    "\n",
    "knn_impute = Custom_KNN_Imputer()\n",
    "mice_impute = Custom_MICE_Imputer()\n",
    "\n",
    "knn_impute.fit(train_cleaned[predictors],train_cleaned['PCIAT-PCIAT_Total'])\n",
    "train_cleaned_imputed_knn = knn_impute.transform(train_cleaned[predictors])\n",
    "\n",
    "mice_impute.fit(train_cleaned[predictors],train_cleaned['PCIAT-PCIAT_Total'])\n",
    "train_cleaned_imputed_mice = mice_impute.transform(train_cleaned[predictors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22290\n",
      "3468\n",
      "3468\n"
     ]
    }
   ],
   "source": [
    "# Make a new data frame of train_cleaned_imputed_knn[predictors]\n",
    "train_cleaned_imputed_knn_df = pd.DataFrame(train_cleaned_imputed_knn, columns=predictors)\n",
    "train_cleaned_imputed_mice_df = pd.DataFrame(train_cleaned_imputed_mice, columns=predictors)\n",
    "\n",
    "# Count the total number of NaN values in train_cleaned_imputed_knn_df\n",
    "print(train_cleaned[predictors].isnull().sum().sum())\n",
    "print(train_cleaned_imputed_knn_df.isnull().sum().sum())\n",
    "print(train_cleaned_imputed_mice_df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic_Demos-Age                              0\n",
      "Basic_Demos-Sex                              0\n",
      "CGAS-CGAS_Score                              0\n",
      "Physical-BMI                                 0\n",
      "Physical-Height                              0\n",
      "Physical-Weight                              0\n",
      "Physical-Waist_Circumference                 0\n",
      "Physical-Diastolic_BP                        0\n",
      "Physical-HeartRate                           0\n",
      "Physical-Systolic_BP                         0\n",
      "Fitness_Endurance-Max_Stage                  0\n",
      "FGC-FGC_CU                                   0\n",
      "FGC-FGC_CU_Zone                           1176\n",
      "FGC-FGC_PU                                   0\n",
      "FGC-FGC_PU_Zone                           1383\n",
      "FGC-FGC_TL                                   0\n",
      "FGC-FGC_TL_Zone                              0\n",
      "BIA-BIA_Activity_Level_num                   0\n",
      "BIA-BIA_FFM                                  0\n",
      "BIA-BIA_FFMI                                 0\n",
      "BIA-BIA_FMI                                  0\n",
      "BIA-BIA_Fat                                  0\n",
      "BIA-BIA_Frame_num                            0\n",
      "SDS-SDS_Total_Raw                            0\n",
      "PreInt_EduHx-computerinternet_hoursday       0\n",
      "ENMO_Avg_Active_Days_MVPA192                 0\n",
      "ENMO_Avg_Active_Days_MVPA110                 0\n",
      "Positive_Anglez_Active_Days                  0\n",
      "FGC-FGC_SR                                   0\n",
      "FGC-FGC_SR_Zone                              0\n",
      "PAQ_Total                                    0\n",
      "PAQ_Zone                                     0\n",
      "Fitness_Endurance_Total_Time_Sec             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count the number of NaN values in each column of train_cleaned_imputed_knn\n",
    "print(train_cleaned_imputed_knn_df_zoned[predictors].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22290\n",
      "3468\n",
      "3468\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "train_cleaned_imputed_knn_df_zoned = zone_encoder(train_cleaned_imputed_knn_df)\n",
    "train_cleaned_imputed_mice_df_zoned = zone_encoder(train_cleaned_imputed_mice_df)\n",
    "print(train_cleaned[predictors].isnull().sum().sum())\n",
    "print(train_cleaned_imputed_knn_df.isnull().sum().sum())\n",
    "print(train_cleaned_imputed_mice_df.isnull().sum().sum())\n",
    "print(train_cleaned_imputed_knn_df_zoned.isnull().sum().sum())\n",
    "print(train_cleaned_imputed_mice_df_zoned.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computing Zone Values**\n",
    "\n",
    "In this section, we'll create functions that compute the FGC Zone and PAQ_Zone values from the corresponding FGC raw and PAQ_Total (imputed) scores\n",
    "\n",
    "FitnessGram Healthy Fitness Zones are documented at https://pftdata.org/files/hfz-standards.pdf for:\n",
    "* FGC-FGC_CU_Zone\n",
    "* FGC-FGC_PU_Zone\n",
    "* FGC-FGC_TL_Zone\n",
    "* FGC-FGC_SR_Zone\n",
    "\n",
    "FitnessGram Grip Strength Zones appear to be documented at https://www.topendsports.com/testing/norms/handgrip.htm. However, these zones are only defined for ages 10 and up. And it appears that no participants under the age of 10 had their grip strength measured. So maybe it doesn't make sense to include this predictor at all?\n",
    "\n",
    "For the PAQ numbers, some research (https://pubmed.ncbi.nlm.nih.gov/27759968/) has identified a cut-off score of 2.75 (ages 14-20) and 2.73 (ages 8-14) to discriminate >60 minutes of MVPA. However, the study suggests that, while the cutoff is significant for the older group, it isn't for for the younger.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute values for the 'FGC-FGC_SR_Zone' that is equal to 1 if any of the following are true:\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_SR >= 8\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_SR >= 9 and Basic_Demos-Age is between 5 and 10\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_SR >= 10 and Basic_Demos-Age is between 11 and 14\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_SR >= 12 and Basic_Demos-Age is at least 15\n",
    "# Note that Basic_Demos-Sex is coded as 0=Male and 1=Female\n",
    "\n",
    "def sitreachzone(sex, age, sr):\n",
    "    try:\n",
    "        if np.isnan(sr) or np.isnan(sex) or np.isnan(age):\n",
    "            return np.nan\n",
    "        elif sex == 0 and sr>=8:\n",
    "            return 1\n",
    "        elif sex == 1 and age >= 15 and sr >= 12:\n",
    "            return 1\n",
    "        elif sex == 1 and age >= 11 and sr >= 10:\n",
    "            return 1\n",
    "        elif sex == 1 and age >= 5 and sr >= 9:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute values for the 'FGC-FGC_CU_Zone' that is equal to 1 if any of the following are true:\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_CU >= 2 and Basic_Demos-Age is between 5 and 6\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_CU >= 4 and Basic_Demos-Age is 7\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_CU >= 6 and Basic_Demos-Age is 8\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_CU >= 9 and Basic_Demos-Age is 9\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_CU >= 12 and Basic_Demos-Age is 10\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_CU >= 15 and Basic_Demos-Age is 11\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_CU >= 18 and Basic_Demos-Age is 12\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_CU >= 21 and Basic_Demos-Age is 13\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_CU >= 24 and Basic_Demos-Age is at least 14\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_CU >= 2 and Basic_Demos-Age is between 5 and 6\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_CU >= 4 and Basic_Demos-Age is 7\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_CU >= 6 and Basic_Demos-Age is 8\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_CU >= 9 and Basic_Demos-Age is 9\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_CU >= 12 and Basic_Demos-Age is 10\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_CU >= 15 and Basic_Demos-Age is 11\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_CU >= 18 and Basic_Demos-Age is at least 12\n",
    "\n",
    "def curlupzone(sex, age, cu):\n",
    "    try:\n",
    "        if np.isnan(sex) or np.isnan(age) or np.isnan(cu):\n",
    "            return np.nan\n",
    "        elif sex == 0:\n",
    "            if (age >= 14 and cu >= 24) or (age == 13 and cu >= 21) or (age == 12 and cu >= 18) or (age == 11 and cu >= 15) or (age == 10 and cu >= 12) or (age == 9 and cu >= 9) or (age == 8 and cu >= 6) or (age == 7 and cu >= 4) or (age <= 6 and cu >= 2):\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        elif sex == 1:\n",
    "            if (age >= 12 and cu >= 18) or (age == 11 and cu >= 15) or (age == 10 and cu >= 12) or (age == 9 and cu >= 9) or (age == 8 and cu >= 6) or (age == 7 and cu >= 4) or (age <= 6 and cu >= 2):\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute values for the 'FGC-FGC_PU_Zone' that is equal to 1 if any of the following are true:\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_PU >= 3 and Basic_Demos-Age is between 5 and 6\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_PU >= 4 and Basic_Demos-Age is 7\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_PU >= 5 and Basic_Demos-Age is 8\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_PU >= 6 and Basic_Demos-Age is 9\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_PU >= 7 and Basic_Demos-Age is 10\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_PU >= 8 and Basic_Demos-Age is 11\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_PU >= 10 and Basic_Demos-Age is 12\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_PU >= 12 and Basic_Demos-Age is 13\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_PU >= 14 and Basic_Demos-Age is 14\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_PU >= 16 and Basic_Demos-Age is 15\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_PU >= 18 and Basic_Demos-Age is at least 16\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_PU >= 3 and Basic_Demos-Age is between 5 and 6\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_PU >= 4 and Basic_Demos-Age is 7\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_PU >= 5 and Basic_Demos-Age is 8\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_PU >= 6 and Basic_Demos-Age is 9\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_PU >= 7 and Basic_Demos-Age is at least 10\n",
    "\n",
    "def pullupzone(sex, age, pu):\n",
    "    try:\n",
    "        if np.isnan(sex) or np.isnan(age) or np.isnan(pu):\n",
    "            return np.nan\n",
    "        elif sex == 0:\n",
    "            if (age >= 16 and pu >= 18) or (age == 15 and pu >= 16) or (age == 14 and pu >= 14) or (age == 13 and pu >= 12) or (age == 12 and pu >= 10) or (age == 11 and pu >= 8) or (age == 10 and pu >= 7) or (age == 9 and pu >= 6) or (age == 8 and pu >= 5) or (age == 7 and pu >= 4) or (age <= 6 and pu >= 2):\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        elif sex == 1:\n",
    "            if (age >= 10 and pu >= 7) or (age == 9 and pu >= 6) or (age == 8 and pu >= 5) or (age == 7 and pu >= 4) or (age <= 6 and pu >= 3):\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comtlte values for the 'FGC-FGC_TL_Zone' that is equal to 1 if any of the following are true:\n",
    "# FGC-FGC_TL >= 6 and Basic_Demos-Age is between 5 and 9\n",
    "# FGC-FGC_TL >= 9 and Basic_Demos-Age is at least 10\n",
    "\n",
    "def tlzone(age, tl):\n",
    "    try:\n",
    "        if np.isnan(tl) or np.isnan(age):\n",
    "            return np.nan\n",
    "        elif (age >= 10 and tl >= 9) or (age <= 9 and tl >= 6):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comtlte values for the 'PAQ_MVPA' that is equal to 1 if any of the following are true:\n",
    "# PAQ_Total >= 2.73 and Basic_Demos-Age is between 5 and 13\n",
    "# PAQ_Total >= 2.75 and Basic_Demos-Age is at least 14\n",
    "\n",
    "def paqzone(age, paq):\n",
    "    try:\n",
    "        if np.isnan(paq) or np.isnan(age):\n",
    "            return np.nan\n",
    "        elif (age >= 14 and paq >= 2.75) or (age <= 13 and paq >= 2.73):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A Custom Encoder for Zone Variables**\n",
    "\n",
    "The goal of this next section is to define a function that will take in a dataframe and return one with the codes for the Zone variables based on the functions defined above\n",
    "\n",
    "It's possible that the dataframe might lack and age, sex, or one of the raw \"score\" variables that we'd use to do this encoding, so the encoder will need to check for the presence of these variables.\n",
    "\n",
    "If any of the variables are missing, the function imputes the mean of the already-present Zone values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zone_encoder(df):\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    if 'FGC-FGC_SR_Zone' in df_copy.columns:\n",
    "        if 'Basic_Demos-Age' in df_copy.columns and 'Basic_Demos-Sex' in df_copy.columns and 'FGC-FGC_SR' in df_copy.columns:\n",
    "            df_copy['FGC-FGC_SR_Zone'] = df_copy.apply(lambda x: sitreachzone(x['Basic_Demos-Sex'], x['Basic_Demos-Age'], x['FGC-FGC_SR']), axis=1)\n",
    "        else:\n",
    "            df_copy['FGC-FGC_SR_Zone'] = df_copy['FGC-FGC_SR_Zone'].fillna(df_copy['FGC-FGC_SR_Zone'].mean())\n",
    "    if 'FGC-FGC_CU_Zone' in df_copy.columns:\n",
    "        if 'Basic_Demos-Age' in df_copy.columns and 'Basic_Demos-Sex' in df_copy.columns and 'FGC-FGC_CU' in df_copy.columns:\n",
    "            df_copy['FGC-FGC_CU_Zone'] = df_copy.apply(lambda x: curlupzone(x['Basic_Demos-Sex'], x['Basic_Demos-Age'], x['FGC-FGC_CU']), axis=1)\n",
    "        else:\n",
    "            df_copy['FGC-FGC_CU_Zone'] = df_copy['FGC-FGC_CU_Zone'].fillna(df_copy['FGC-FGC_CU_Zone'].mean())\n",
    "    if 'FGC-FGC_PU_Zone' in df_copy.columns:\n",
    "        if 'Basic_Demos-Age' in df_copy.columns and 'Basic_Demos-Sex' in df_copy.columns and 'FGC-FGC_PU' in df_copy.columns:\n",
    "            df_copy['FGC-FGC_PU_Zone'] = df_copy.apply(lambda x: pullupzone(x['Basic_Demos-Sex'], x['Basic_Demos-Age'], x['FGC-FGC_PU']), axis=1)\n",
    "        else:\n",
    "            df_copy['FGC-FGC_PU_Zone'] = df_copy['FGC-FGC_PU_Zone'].fillna(df_copy['FGC-FGC_PU_Zone'].mean())\n",
    "    if 'FGC-FGC_TL_Zone' in df_copy.columns:\n",
    "        if 'Basic_Demos-Age' in df_copy.columns and 'FGC-FGC_PU' in df_copy.columns:\n",
    "            df_copy['FGC-FGC_TL_Zone'] = df_copy.apply(lambda x: tlzone(x['Basic_Demos-Age'], x['FGC-FGC_TL']), axis=1)\n",
    "        else:\n",
    "            df_copy['FGC-FGC_TL_Zone'] = df_copy['FGC-FGC_TL_Zone'].fillna(df_copy['FGC-FGC_TL_Zone'].mean())\n",
    "    if 'PAQ_Zone' in df_copy.columns:\n",
    "        if 'Basic_Demos-Age' in df_copy.columns and 'PAQ_Total' in df_copy.columns:\n",
    "            df_copy['PAQ_Zone'] = df_copy.apply(lambda x: tlzone(x['Basic_Demos-Age'], x['PAQ_Total']), axis=1)\n",
    "        else:\n",
    "            df_copy['PAQ_Zone'] = df_copy.apply(lambda x: paqzone(x['Basic_Demos-Age'], x['PAQ_Total']), axis=1)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using LASSO for Feature Selection**\n",
    "\n",
    "First, we'll try using LASSO to identify important features.\n",
    "\n",
    "Note that it isn't possible to use LASSO with pipelines (see https://stackoverflow.com/questions/39466671/use-of-scaler-with-lassocv-ridgecv). So we'll need to do the hyperparameter tuning manually.\n",
    "\n",
    "Some of the code below was suggested by Ali Furkan Kalay: https://alfurka.github.io/2018-11-18-grid-search/\n",
    "\n",
    "Some of the code below was suggested on Medium: https://medium.com/geekculture/regularization-using-pipeline-gridsearchcv-f377946e39d1\n",
    "\n",
    "Some of the code below was suggested on geeksforgeeks (https://www.geeksforgeeks.org/feature-selection-using-selectfrommodel-and-lassocv-in-scikit-learn/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tuning Lasso inside a Pipe with GridSearchCV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nAll the 500 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n500 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/pipeline.py\", line 473, in fit\n    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py\", line 980, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 650, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1301, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1064, in check_array\n    _assert_all_finite(\n  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n    _assert_all_finite_element_wise(\n  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n    raise ValueError(msg_err)\nValueError: Input X contains NaN.\nLasso does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Set up a lasso pipeline\u001b[39;00m\n\u001b[1;32m     22\u001b[0m lasso_pipe \u001b[38;5;241m=\u001b[39m Pipeline([(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimpute\u001b[39m\u001b[38;5;124m'\u001b[39m, Custom_MICE_Imputer()),(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfillzones\u001b[39m\u001b[38;5;124m'\u001b[39m, FunctionTransformer(zone_encoder)), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlasso\u001b[39m\u001b[38;5;124m'\u001b[39m, Lasso())])\n\u001b[0;32m---> 24\u001b[0m gs_lasso_pipe \u001b[38;5;241m=\u001b[39m \u001b[43mGridSearchCV\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlasso_pipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malphas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_cleaned\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpredictors\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_cleaned\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPCIAT-PCIAT_Total\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m gs_lasso_pipe\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[1;32m     27\u001b[0m gs_lasso_pipe\u001b[38;5;241m.\u001b[39mbest_params_\n",
      "File \u001b[0;32m/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1019\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m   1014\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m   1015\u001b[0m     )\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m-> 1019\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1573\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1572\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1573\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_search.py:996\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[1;32m    990\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    992\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    993\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[1;32m    994\u001b[0m     )\n\u001b[0;32m--> 996\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[1;32m    999\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:529\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[1;32m    523\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    524\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    525\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    526\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    527\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    528\u001b[0m     )\n\u001b[0;32m--> 529\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    532\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    539\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: \nAll the 500 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n500 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/pipeline.py\", line 473, in fit\n    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py\", line 980, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 650, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1301, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1064, in check_array\n    _assert_all_finite(\n  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n    _assert_all_finite_element_wise(\n  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n    raise ValueError(msg_err)\nValueError: Input X contains NaN.\nLasso does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "  \n",
    "# Create a list of predictor variables; this eliminates id, sii, PCIAT, and Season variables\n",
    "predictors = train_cleaned.columns.tolist()\n",
    "if 'id' in predictors:\n",
    "    predictors.remove('id')\n",
    "if 'sii' in predictors:\n",
    "    predictors.remove('sii')\n",
    "predictors = [x for x in predictors if 'PCIAT' not in x]\n",
    "predictors = [x for x in predictors if 'Season' not in x]\n",
    "\n",
    "# A list of alpha (lambda) values to try in the hyperparameter tuning\n",
    "# create an array of 10**np.linspace(10,-2,100)*0.5\n",
    "alphas = {'lasso__alpha': 10**np.linspace(10,-2,100)*0.5}\n",
    "\n",
    "\n",
    "# Set up a lasso pipeline\n",
    "lasso_pipe = Pipeline([('impute', Custom_MICE_Imputer()),('fillzones', FunctionTransformer(zone_encoder)), ('lasso', Lasso())])\n",
    "\n",
    "gs_lasso_pipe = GridSearchCV(lasso_pipe, param_grid=alphas, cv=5).fit(train_cleaned[predictors], train_cleaned['PCIAT-PCIAT_Total'])\n",
    "\n",
    "gs_lasso_pipe.best_estimator_\n",
    "gs_lasso_pipe.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.feature_selection import SelectFromModel \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "  \n",
    "# Create a list of predictor variables; this eliminates id, sii, PCIAT, and Season variables\n",
    "predictors = train_cleaned.columns.tolist()\n",
    "if 'id' in predictors:\n",
    "    predictors.remove('id')\n",
    "if 'sii' in predictors:\n",
    "    predictors.remove('sii')\n",
    "predictors = [x for x in predictors if 'PCIAT' not in x]\n",
    "predictors = [x for x in predictors if 'Season' not in x]\n",
    "\n",
    "# Split the data into 80% Train/20% Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_cleaned[predictors], train_cleaned['PCIAT-PCIAT_Total'], test_size=0.2, random_state=216)\n",
    "\n",
    "# A list of alpha (lambda) values to try in the hyperparameter tuning\n",
    "alphas = 10**np.linspace(10,-2,100)*0.5\n",
    "\n",
    "# These will hold our coefficient estimates\n",
    "lasso_coefs = np.empty((len(alpha),n))\n",
    "\n",
    "# Set up a lasso pipeline\n",
    "lasso_pipe = Pipeline([('impute', Custom_MICE_Imputer()),('fillzones', FunctionTransformer(zone_encoder)), ('lasso', Lasso())])\n",
    "\n",
    "GridSearchCV(lasso_pipe, param_grid=alphas).fit(train_cleaned[predictors], train_cleaned['PCIAT-PCIAT_Total']).best_estimator_,\n",
    "\n",
    "def test(models, data, iterations = 100):\n",
    "    results = {}\n",
    "    for i in models:\n",
    "        r2_train = []\n",
    "        r2_test = []\n",
    "        for j in range(iterations):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(data[X], \n",
    "                                                                data[Y], \n",
    "                                                                test_size= 0.2)\n",
    "            r2_test.append(metrics.r2_score(y_test,\n",
    "                                            models[i].fit(X_train, \n",
    "                                                         y_train).predict(X_test)))\n",
    "            r2_train.append(metrics.r2_score(y_train, \n",
    "                                             models[i].fit(X_train, \n",
    "                                                          y_train).predict(X_train)))\n",
    "        results[i] = [np.mean(r2_train), np.mean(r2_test)]\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "models = {'OLS': linear_model.LinearRegression(),\n",
    "           'Lasso': GridSearchCV(linear_model.Lasso(), \n",
    "                               param_grid=lasso_params).fit(df[X], df[Y]).best_estimator_,\n",
    "           'Ridge': GridSearchCV(linear_model.Ridge(), \n",
    "                               param_grid=ridge_params).fit(df[X], df[Y]).best_estimator_,}\n",
    "\n",
    "test(models, df)\n",
    "\n",
    "## for each alpha value\n",
    "for i in range(len(alpha)):\n",
    "    ## set up the lasso pipeline\n",
    "    ## first scale\n",
    "    ## then make polynomial features\n",
    "    ## then fit the lasso regression model\n",
    "    lasso_pipe = Pipeline([('scale',StandardScaler()),\n",
    "                              ('poly',PolynomialFeatures(n, interaction_only=False, include_bias=False)),\n",
    "                              ('lasso', Lasso(alpha=alpha[i], max_iter=5000000))\n",
    "                          ])\n",
    "    \n",
    "    ## fit the lasso\n",
    "    lasso_pipe.fit(x.reshape(-1,1), y)\n",
    "\n",
    "    # record the coefficients\n",
    "    lasso_coefs[i,:] = lasso_pipe['lasso'].coef_\n",
    "\n",
    "\n",
    "# Fit LassoCV model with 5-fold cross-validation. It automatically evaluates performance over several folds in order to get the ideal regularization strength (alpha).\n",
    "lasso_cv = LassoCV(cv=5) \n",
    "lasso_cv.fit(X_train, y_train) \n",
    "\n",
    "# Feature selection. This selects the most significant features from the training and testing sets using the pre-trained lasso_cv model. \n",
    "# Only the features determined to be relevant by the L1 regularization are included in the final selected feature sets\n",
    "# These final selected feature sets are stored in X_train_selected and X_test_selected\n",
    "sfm = SelectFromModel(lasso_cv, prefit=True) \n",
    "X_train_selected = sfm.transform(X_train) \n",
    "X_test_selected = sfm.transform(X_test) \n",
    "\n",
    "# Train a Random Forest Classifier using the selected features \n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42) \n",
    "model.fit(X_train_selected, y_train) \n",
    "\n",
    "\n",
    "# Evaluate the model \n",
    "y_pred = model.predict(X_test_selected) \n",
    "print(classification_report(y_test, y_pred)) \n",
    "\n",
    "# Analyze selected features and their importance \n",
    "selected_feature_indices = np.where(sfm.get_support())[0] \n",
    "selected_features = train.columns[selected_feature_indices] \n",
    "coefficients = lasso_cv.coef_ \n",
    "print(\"Selected Features:\", selected_features) \n",
    "print(\"Feature Coefficients:\", coefficients) \n",
    "\n",
    "# Extract the selected features from the original dataset \n",
    "X_selected_features = X_train[:, selected_feature_indices] \n",
    "\n",
    "# Create a DataFrame for better visualization \n",
    "selected_features_df = pd.DataFrame(X_selected_features, columns=selected_features) \n",
    "\n",
    "# Add the target variable for coloring \n",
    "selected_features_df['target'] = y_train \n",
    "\n",
    "# Plot the two most important features \n",
    "sns.scatterplot(x='mean area', y='worst area', hue='target', data=selected_features_df, palette='viridis') \n",
    "plt.xlabel('Mean Area') \n",
    "plt.ylabel('Worst Area') \n",
    "plt.title('Scatter Plot of Two Most Important Features') \n",
    "plt.show() \n",
    "\n",
    "\n",
    "\n",
    "## This code will allow us to demonstrate the effect of \n",
    "## increasing alpha\n",
    "\n",
    "## set values for alpha\n",
    "alpha = [0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000]\n",
    "\n",
    "## The degree of the polynomial we will fit\n",
    "n=10\n",
    "\n",
    "#$ These will hold our coefficient estimates\n",
    "ridge_coefs = np.empty((len(alpha),n))\n",
    "lasso_coefs = np.empty((len(alpha),n))\n",
    "\n",
    "## for each alpha value\n",
    "for i in range(len(alpha)):\n",
    "    ## set up the lasso pipeline\n",
    "    ## first scale\n",
    "    ## then make polynomial features\n",
    "    ## then fit the lasso regression model\n",
    "    lasso_pipe = Pipeline([('scale',StandardScaler()),\n",
    "                              ('poly',PolynomialFeatures(n, interaction_only=False, include_bias=False)),\n",
    "                              ('lasso', Lasso(alpha=alpha[i], max_iter=5000000))\n",
    "                          ])\n",
    "    \n",
    "    ## fit the lasso\n",
    "    lasso_pipe.fit(x.reshape(-1,1), y)\n",
    "\n",
    "    # record the coefficients\n",
    "    lasso_coefs[i,:] = lasso_pipe['lasso'].coef_\n",
    "\n",
    "\n",
    "# A data frame to store the optimal alpha values\n",
    "bestalphas = pd.DataFrame(index=range(0,len(listofdatasets)))\n",
    "bestalphas['dfname'] = ''\n",
    "bestalphas['best_alpha_manual'] = np.nan\n",
    "bestalphas['best_alpha_automatic'] = np.nan\n",
    "\n",
    "\n",
    "for df in listofdatasets:\n",
    "    X_train = df.drop(columns=['PCIAT-PCIAT_Total'])\n",
    "    y_train = df['PCIAT-PCIAT_Total']\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_std = scaler.transform(X_train)\n",
    "    lassocv = LassoCV(alphas = alphas, scoring = 'neg_root_mean_squared_error')\n",
    "    lassocv.fit(X_std, y_train)\n",
    "    bestalphas.loc[bestalphas['dfname']==df.name,'best_alpha_automatic']=lassocv.alpha_.astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a Pipeline with the Custom Imputer and Transformer**\n",
    "\n",
    "Below is some code that is based on the 2_More_Advanced_Pipelines notebook from optional_extra_practice in Week 3\n",
    "\n",
    "In that code, their desired pipeline was:\n",
    "1 Impute the missing values of `body_mass_g` with the `median` value,\n",
    "2 Impute the missing values of `sex` with the most common value,\n",
    "3 One hot encode `island` and `sex` and\n",
    "4 Fit a random forest model to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "\n",
    "predictors = train_cleaned.columns.tolist()\n",
    "if 'id' in predictors:\n",
    "    predictors.remove('id')\n",
    "if 'sii' in predictors:\n",
    "    predictors.remove('sii')\n",
    "predictors = [x for x in predictors if 'PCIAT' not in x]\n",
    "predictors = [x for x in predictors if 'Season' not in x]\n",
    "\n",
    "\n",
    "pipe_knn = Pipeline([('knn_impute', Custom_KNN_Imputer()),\n",
    "                    ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                    ('rf', RandomForestRegressor(n_estimators = 300, max_features = 'sqrt', max_depth = 5, random_state = 216))])\n",
    "\n",
    "pipe_knn.fit(train_cleaned[predictors],train_cleaned['PCIAT-PCIAT_Total'])\n",
    "\n",
    "train_pred_knn = pipe_knn.predict(train_cleaned[predictors])\n",
    "\n",
    "\n",
    "\n",
    "pipe_mice = Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "                    ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                    ('rf', RandomForestRegressor(n_estimators = 300, max_features = 'sqrt', max_depth = 5, random_state = 216))])\n",
    "\n",
    "pipe_mice.fit(train_cleaned[predictors],train_cleaned['PCIAT-PCIAT_Total'])\n",
    "\n",
    "train_pred_mice = pipe_mice.predict(train_cleaned[predictors])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Physical-Height</td>\n",
       "      <td>0.138206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Basic_Demos-Age</td>\n",
       "      <td>0.128269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>PreInt_EduHx-computerinternet_hoursday</td>\n",
       "      <td>0.122673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Physical-Weight</td>\n",
       "      <td>0.088061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>SDS-SDS_Total_Raw</td>\n",
       "      <td>0.072821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>BIA-BIA_FFM</td>\n",
       "      <td>0.070766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>FGC-FGC_CU</td>\n",
       "      <td>0.062064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Physical-Waist_Circumference</td>\n",
       "      <td>0.036682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>BIA-BIA_Fat</td>\n",
       "      <td>0.029994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>BIA-BIA_FFMI</td>\n",
       "      <td>0.026096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Physical-BMI</td>\n",
       "      <td>0.021777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ENMO_Avg_Active_Days_MVPA192</td>\n",
       "      <td>0.021058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>BIA-BIA_FMI</td>\n",
       "      <td>0.017745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ENMO_Avg_Active_Days_MVPA110</td>\n",
       "      <td>0.016798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>FGC-FGC_SR</td>\n",
       "      <td>0.016445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>FGC-FGC_PU</td>\n",
       "      <td>0.015782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>PAQ_Total</td>\n",
       "      <td>0.013633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CGAS-CGAS_Score</td>\n",
       "      <td>0.012918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>FGC-FGC_TL</td>\n",
       "      <td>0.012826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Fitness_Endurance_Total_Time_Sec</td>\n",
       "      <td>0.012197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Fitness_Endurance-Max_Stage</td>\n",
       "      <td>0.010215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Physical-Systolic_BP</td>\n",
       "      <td>0.010049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Physical-Diastolic_BP</td>\n",
       "      <td>0.009184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Positive_Anglez_Active_Days</td>\n",
       "      <td>0.008011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Physical-HeartRate</td>\n",
       "      <td>0.007954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>BIA-BIA_Activity_Level_num</td>\n",
       "      <td>0.006205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>BIA-BIA_Frame_num</td>\n",
       "      <td>0.005064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Basic_Demos-Sex</td>\n",
       "      <td>0.004090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>PAQ_Zone</td>\n",
       "      <td>0.001439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>FGC-FGC_SR_Zone</td>\n",
       "      <td>0.000617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>FGC-FGC_TL_Zone</td>\n",
       "      <td>0.000360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>FGC-FGC_PU_Zone</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>FGC-FGC_CU_Zone</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   feature  importance_score\n",
       "4                          Physical-Height          0.138206\n",
       "0                          Basic_Demos-Age          0.128269\n",
       "24  PreInt_EduHx-computerinternet_hoursday          0.122673\n",
       "5                          Physical-Weight          0.088061\n",
       "23                       SDS-SDS_Total_Raw          0.072821\n",
       "18                             BIA-BIA_FFM          0.070766\n",
       "11                              FGC-FGC_CU          0.062064\n",
       "6             Physical-Waist_Circumference          0.036682\n",
       "21                             BIA-BIA_Fat          0.029994\n",
       "19                            BIA-BIA_FFMI          0.026096\n",
       "3                             Physical-BMI          0.021777\n",
       "25            ENMO_Avg_Active_Days_MVPA192          0.021058\n",
       "20                             BIA-BIA_FMI          0.017745\n",
       "26            ENMO_Avg_Active_Days_MVPA110          0.016798\n",
       "28                              FGC-FGC_SR          0.016445\n",
       "13                              FGC-FGC_PU          0.015782\n",
       "30                               PAQ_Total          0.013633\n",
       "2                          CGAS-CGAS_Score          0.012918\n",
       "15                              FGC-FGC_TL          0.012826\n",
       "32        Fitness_Endurance_Total_Time_Sec          0.012197\n",
       "10             Fitness_Endurance-Max_Stage          0.010215\n",
       "9                     Physical-Systolic_BP          0.010049\n",
       "7                    Physical-Diastolic_BP          0.009184\n",
       "27             Positive_Anglez_Active_Days          0.008011\n",
       "8                       Physical-HeartRate          0.007954\n",
       "17              BIA-BIA_Activity_Level_num          0.006205\n",
       "22                       BIA-BIA_Frame_num          0.005064\n",
       "1                          Basic_Demos-Sex          0.004090\n",
       "31                                PAQ_Zone          0.001439\n",
       "29                         FGC-FGC_SR_Zone          0.000617\n",
       "16                         FGC-FGC_TL_Zone          0.000360\n",
       "14                         FGC-FGC_PU_Zone          0.000000\n",
       "12                         FGC-FGC_CU_Zone          0.000000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get feature importance from the rf inside pipe\n",
    "score_knn_df = pd.DataFrame({'feature':train_cleaned[predictors].columns,\n",
    "                            'importance_score': pipe_knn.named_steps['rf'].feature_importances_})\n",
    "\n",
    "score_knn_df.sort_values('importance_score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Physical-Height</td>\n",
       "      <td>0.137001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Basic_Demos-Age</td>\n",
       "      <td>0.124362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>PreInt_EduHx-computerinternet_hoursday</td>\n",
       "      <td>0.123129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>BIA-BIA_FFM</td>\n",
       "      <td>0.076854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Physical-Weight</td>\n",
       "      <td>0.073994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>SDS-SDS_Total_Raw</td>\n",
       "      <td>0.073693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>FGC-FGC_CU</td>\n",
       "      <td>0.062461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ENMO_Avg_Active_Days_MVPA110</td>\n",
       "      <td>0.058113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>BIA-BIA_Fat</td>\n",
       "      <td>0.026684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>BIA-BIA_FFMI</td>\n",
       "      <td>0.022597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>FGC-FGC_PU</td>\n",
       "      <td>0.020965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Physical-BMI</td>\n",
       "      <td>0.018271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>PAQ_Total</td>\n",
       "      <td>0.017228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>FGC-FGC_TL</td>\n",
       "      <td>0.015490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CGAS-CGAS_Score</td>\n",
       "      <td>0.014505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>FGC-FGC_SR</td>\n",
       "      <td>0.014021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Physical-Waist_Circumference</td>\n",
       "      <td>0.013713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Fitness_Endurance-Max_Stage</td>\n",
       "      <td>0.012978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Physical-Systolic_BP</td>\n",
       "      <td>0.011299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ENMO_Avg_Active_Days_MVPA192</td>\n",
       "      <td>0.011215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Positive_Anglez_Active_Days</td>\n",
       "      <td>0.011157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Fitness_Endurance_Total_Time_Sec</td>\n",
       "      <td>0.011046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>BIA-BIA_FMI</td>\n",
       "      <td>0.010474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>BIA-BIA_Frame_num</td>\n",
       "      <td>0.008504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Physical-HeartRate</td>\n",
       "      <td>0.007888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Physical-Diastolic_BP</td>\n",
       "      <td>0.007296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>BIA-BIA_Activity_Level_num</td>\n",
       "      <td>0.006925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Basic_Demos-Sex</td>\n",
       "      <td>0.005431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>FGC-FGC_TL_Zone</td>\n",
       "      <td>0.001259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>PAQ_Zone</td>\n",
       "      <td>0.000866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>FGC-FGC_SR_Zone</td>\n",
       "      <td>0.000581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>FGC-FGC_PU_Zone</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>FGC-FGC_CU_Zone</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   feature  importance_score\n",
       "4                          Physical-Height          0.137001\n",
       "0                          Basic_Demos-Age          0.124362\n",
       "24  PreInt_EduHx-computerinternet_hoursday          0.123129\n",
       "18                             BIA-BIA_FFM          0.076854\n",
       "5                          Physical-Weight          0.073994\n",
       "23                       SDS-SDS_Total_Raw          0.073693\n",
       "11                              FGC-FGC_CU          0.062461\n",
       "26            ENMO_Avg_Active_Days_MVPA110          0.058113\n",
       "21                             BIA-BIA_Fat          0.026684\n",
       "19                            BIA-BIA_FFMI          0.022597\n",
       "13                              FGC-FGC_PU          0.020965\n",
       "3                             Physical-BMI          0.018271\n",
       "30                               PAQ_Total          0.017228\n",
       "15                              FGC-FGC_TL          0.015490\n",
       "2                          CGAS-CGAS_Score          0.014505\n",
       "28                              FGC-FGC_SR          0.014021\n",
       "6             Physical-Waist_Circumference          0.013713\n",
       "10             Fitness_Endurance-Max_Stage          0.012978\n",
       "9                     Physical-Systolic_BP          0.011299\n",
       "25            ENMO_Avg_Active_Days_MVPA192          0.011215\n",
       "27             Positive_Anglez_Active_Days          0.011157\n",
       "32        Fitness_Endurance_Total_Time_Sec          0.011046\n",
       "20                             BIA-BIA_FMI          0.010474\n",
       "22                       BIA-BIA_Frame_num          0.008504\n",
       "8                       Physical-HeartRate          0.007888\n",
       "7                    Physical-Diastolic_BP          0.007296\n",
       "17              BIA-BIA_Activity_Level_num          0.006925\n",
       "1                          Basic_Demos-Sex          0.005431\n",
       "16                         FGC-FGC_TL_Zone          0.001259\n",
       "31                                PAQ_Zone          0.000866\n",
       "29                         FGC-FGC_SR_Zone          0.000581\n",
       "14                         FGC-FGC_PU_Zone          0.000000\n",
       "12                         FGC-FGC_CU_Zone          0.000000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get feature importance from the rf inside pipe\n",
    "score_mice_df = pd.DataFrame({'feature':train_cleaned[predictors].columns,\n",
    "                            'importance_score': pipe_mice.named_steps['rf'].feature_importances_})\n",
    "\n",
    "score_mice_df.sort_values('importance_score',ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_fall_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
