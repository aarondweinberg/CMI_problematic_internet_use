{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goals**\n",
    "\n",
    "The goal of this notebook is to explore feature selection, continuing from the AW EDA Exploration goals:\n",
    "\n",
    "9. Create a linear regression model using a greedy algorithm from the \"bottom up\"\n",
    "    1. Make a list of all numerical predictors and also a new empty data frame with 100(?) rows and the predictors as variables\n",
    "    2. Randomly select a predictor from the list and create a linear model\n",
    "    3. Randomly select a second predictor from the list and add it to the model\n",
    "    4. Perform an F test to see if the new model is significantly better than the old\n",
    "    5. Repeat until the F test is no longer significant\n",
    "    6. Record the predictors that are in the model in the newly-created data frame\n",
    "    7. Repeat the above steps 100 (??) times\n",
    "    8. Compute the mean for each predictor in the data frame. This should give some sense of the \"importance\" of each predictor\n",
    "10. Repeat the previous method but using a \"top down\" algorithm, starting with a full model and removing predictors one-by-one\n",
    "11. *Maybe* Trying to use PCA and either linear or KNN regression to see if it appears to improve prediction\n",
    "    * PCA on the entire set of predictors\n",
    "    * On each set of grouped predictors\n",
    "12. Using RandomForest Regression on the entire set of predictors and examining the importance matrix to try to find a potential list of predictors\n",
    "13. *Maybe* using XGBoost to do stuff. (Need to learn what this is)\n",
    "14. Removing highly-correlated predictors and using LASSO and using LASSO regression (with hyperparameter tuning) to identify important predictors\n",
    "15. Comparing the apparent predictive power of all the previous methods. If none stand out, then stick with linear regression(?)\n",
    "16. Start to engage more formally with the modeling process, using Kfold splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the Data**\n",
    "\n",
    "For the purpose of developing our model(s), we'll work with data that include the imputed outcome (PCIAT_Total and/or sii) scores AND have cleaned predictors.\n",
    "\n",
    "In the final version of our code, we'll work with data with cleaned predictors but won't have any access to the outcome scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the cleaned & predictor-imputed data\n",
    "train_cleaned=pd.read_csv('train_cleaned_outcome_imputed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using KNN to Impute Values of Predictor Variables**\n",
    "\n",
    "Our first code chunk will use a KNN algorithm with all available predictor columns, excluding the Zone and Season columns\n",
    "\n",
    "We'll start by making a list of quantitative predictor variables. Note that:\n",
    "* The Zone variables are computed from others; we'll re-compute their values after doing imputation\n",
    "* The list includes Basic_Demos-Sex. Although this is categorical, all participants have data for this variable, and it's useful for imputing other variables\n",
    "* We *could* convert the Season variables into dummy variables, but this seems like it would over-weight them for KNN imputation. So we're leaving them out.\n",
    "\n",
    "Then, we'll construct and use a KNN imputer with 5 neighbors to impute missing values.\n",
    "\n",
    "We'll wrapp all of this inside a custom imputer that can be called inside a pipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We'll need these\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "## Define our custom imputer\n",
    "class Custom_KNN_Imputer(BaseEstimator, TransformerMixin):\n",
    "    # Class Constructor \n",
    "    # This allows you to initiate the class when you call Custom_KNN_Imputer\n",
    "    def __init__(self):\n",
    "        # I want to initiate each object with both a KNNImputer and StandardScaler object/method\n",
    "        self.KNNImputer = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')\n",
    "        self.StandardScaler = StandardScaler()\n",
    "\n",
    "    \n",
    "    # For my fit method I'm just going to \"steal\" KNNImputers's fit method using a curated collection of predictors\n",
    "    def fit(self, X, y = None ):\n",
    "        feature_list = X.columns.tolist()\n",
    "        if 'id' in feature_list:\n",
    "            feature_list.remove('id')\n",
    "        if 'sii' in feature_list:\n",
    "            feature_list.remove('sii')\n",
    "        feature_list = [x for x in feature_list if 'PCIAT' not in x]\n",
    "        feature_list = [x for x in feature_list if 'Zone' not in x]\n",
    "        feature_list = [x for x in feature_list if 'Season' not in x]\n",
    "        self.StandardScaler.fit(X[feature_list])\n",
    "        # I'm never sure if we need the .values and/or .reshape(-1,1)\n",
    "        #self.KNNImputer.fit(X[feature_list].values.reshape(-1,1))\n",
    "        self.KNNImputer.fit(X[feature_list])\n",
    "        return self\n",
    "    \n",
    "    # Now I want to transform the columns in feature list and return it with imputed values that have been un-transformed\n",
    "    def transform(self, X, y = None):\n",
    "        feature_list = X.columns.tolist()\n",
    "        if 'id' in feature_list:\n",
    "            feature_list.remove('id')\n",
    "        if 'sii' in feature_list:\n",
    "            feature_list.remove('sii')\n",
    "        feature_list = [x for x in feature_list if 'PCIAT' not in x]\n",
    "        feature_list = [x for x in feature_list if 'Zone' not in x]\n",
    "        feature_list = [x for x in feature_list if 'Season' not in x]\n",
    "        copy_X = X.copy()\n",
    "        copy_X[feature_list] = self.KNNImputer.transform(copy_X[feature_list])\n",
    "        copy_X2 = self.StandardScaler.inverse_transform(copy_X[feature_list])\n",
    "        df2 = pd.DataFrame(copy_X2, columns=feature_list)\n",
    "        copy_X[feature_list]=copy_X[feature_list].fillna(df2[feature_list])\n",
    "        return copy_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A Custom MICE Imputer**\n",
    "\n",
    "Next, we'll try to take the above code and turn it into a custom imputer that can be used inside a pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We'll need these\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "## Define our custom imputer\n",
    "class Custom_MICE_Imputer(BaseEstimator, TransformerMixin):\n",
    "    # Class Constructor \n",
    "    # This allows you to initiate the class when you call Custom_KNN_Imputer\n",
    "    def __init__(self):\n",
    "        # I want to initiate each object with both a KNNImputer and StandardScaler object/method\n",
    "        self.MICEImputer = IterativeImputer(max_iter=10, random_state=497)\n",
    "\n",
    "    \n",
    "    # For my fit method I'm just going to \"steal\" IterativeImputers's fit method using a curated collection of predictors\n",
    "    def fit(self, X, y = None ):\n",
    "        feature_list = X.columns.tolist()\n",
    "        if 'id' in feature_list:\n",
    "            feature_list.remove('id')\n",
    "        if 'sii' in feature_list:\n",
    "            feature_list.remove('sii')\n",
    "        feature_list = [x for x in feature_list if 'PCIAT' not in x]\n",
    "        feature_list = [x for x in feature_list if 'Zone' not in x]\n",
    "        feature_list = [x for x in feature_list if 'Season' not in x]\n",
    "        self.MICEImputer.fit(X[feature_list])\n",
    "        return self\n",
    "    \n",
    "    # Now I want to transform the columns in feature list and return it with imputed values that have been un-transformed\n",
    "    def transform(self, X, y = None):\n",
    "        feature_list = X.columns.tolist()\n",
    "        if 'id' in feature_list:\n",
    "            feature_list.remove('id')\n",
    "        if 'sii' in feature_list:\n",
    "            feature_list.remove('sii')\n",
    "        feature_list = [x for x in feature_list if 'PCIAT' not in x]\n",
    "        feature_list = [x for x in feature_list if 'Zone' not in x]\n",
    "        feature_list = [x for x in feature_list if 'Season' not in x]\n",
    "        copy_X = X.copy()\n",
    "        df2 = self.MICEImputer.transform(copy_X[feature_list])\n",
    "        df3 = pd.DataFrame(df2, columns=feature_list)\n",
    "        copy_X[feature_list]=copy_X[feature_list].fillna(df3[feature_list])\n",
    "        return copy_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computing Zone Values**\n",
    "\n",
    "In this section, we'll create functions that compute the FGC Zone and PAQ_Zone values from the corresponding FGC raw and PAQ_Total (imputed) scores\n",
    "\n",
    "FitnessGram Healthy Fitness Zones are documented at https://pftdata.org/files/hfz-standards.pdf for:\n",
    "* FGC-FGC_CU_Zone\n",
    "* FGC-FGC_PU_Zone\n",
    "* FGC-FGC_TL_Zone\n",
    "* FGC-FGC_SR_Zone\n",
    "\n",
    "FitnessGram Grip Strength Zones appear to be documented at https://www.topendsports.com/testing/norms/handgrip.htm. However, these zones are only defined for ages 10 and up. And it appears that no participants under the age of 10 had their grip strength measured. So maybe it doesn't make sense to include this predictor at all?\n",
    "\n",
    "For the PAQ numbers, some research (https://pubmed.ncbi.nlm.nih.gov/27759968/) has identified a cut-off score of 2.75 (ages 14-20) and 2.73 (ages 8-14) to discriminate >60 minutes of MVPA. However, the study suggests that, while the cutoff is significant for the older group, it isn't for for the younger.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute values for the 'FGC-FGC_SR_Zone' that is equal to 1 if any of the following are true:\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_SR >= 8\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_SR >= 9 and Basic_Demos-Age is between 5 and 10\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_SR >= 10 and Basic_Demos-Age is between 11 and 14\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_SR >= 12 and Basic_Demos-Age is at least 15\n",
    "# Note that Basic_Demos-Sex is coded as 0=Male and 1=Female\n",
    "\n",
    "def sitreachzone(sex, age, sr):\n",
    "    try:\n",
    "        if np.isnan(sr) or np.isnan(sex) or np.isnan(age):\n",
    "            return np.nan\n",
    "        elif sex == 0 and sr>=8:\n",
    "            return 1\n",
    "        elif sex == 1 and age >= 15 and sr >= 12:\n",
    "            return 1\n",
    "        elif sex == 1 and age >= 11 and sr >= 10:\n",
    "            return 1\n",
    "        elif sex == 1 and age >= 5 and sr >= 9:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute values for the 'FGC-FGC_CU_Zone' that is equal to 1 if any of the following are true:\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_CU >= 2 and Basic_Demos-Age is between 5 and 6\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_CU >= 4 and Basic_Demos-Age is 7\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_CU >= 6 and Basic_Demos-Age is 8\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_CU >= 9 and Basic_Demos-Age is 9\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_CU >= 12 and Basic_Demos-Age is 10\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_CU >= 15 and Basic_Demos-Age is 11\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_CU >= 18 and Basic_Demos-Age is 12\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_CU >= 21 and Basic_Demos-Age is 13\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_CU >= 24 and Basic_Demos-Age is at least 14\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_CU >= 2 and Basic_Demos-Age is between 5 and 6\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_CU >= 4 and Basic_Demos-Age is 7\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_CU >= 6 and Basic_Demos-Age is 8\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_CU >= 9 and Basic_Demos-Age is 9\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_CU >= 12 and Basic_Demos-Age is 10\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_CU >= 15 and Basic_Demos-Age is 11\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_CU >= 18 and Basic_Demos-Age is at least 12\n",
    "\n",
    "def curlupzone(sex, age, cu):\n",
    "    try:\n",
    "        if np.isnan(sex) or np.isnan(age) or np.isnan(cu):\n",
    "            return np.nan\n",
    "        elif sex == 0:\n",
    "            if (age >= 14 and cu >= 24) or (age == 13 and cu >= 21) or (age == 12 and cu >= 18) or (age == 11 and cu >= 15) or (age == 10 and cu >= 12) or (age == 9 and cu >= 9) or (age == 8 and cu >= 6) or (age == 7 and cu >= 4) or (age <= 6 and cu >= 2):\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        elif sex == 1:\n",
    "            if (age >= 12 and cu >= 18) or (age == 11 and cu >= 15) or (age == 10 and cu >= 12) or (age == 9 and cu >= 9) or (age == 8 and cu >= 6) or (age == 7 and cu >= 4) or (age <= 6 and cu >= 2):\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute values for the 'FGC-FGC_PU_Zone' that is equal to 1 if any of the following are true:\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_PU >= 3 and Basic_Demos-Age is between 5 and 6\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_PU >= 4 and Basic_Demos-Age is 7\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_PU >= 5 and Basic_Demos-Age is 8\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_PU >= 6 and Basic_Demos-Age is 9\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_PU >= 7 and Basic_Demos-Age is 10\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_PU >= 8 and Basic_Demos-Age is 11\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_PU >= 10 and Basic_Demos-Age is 12\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_PU >= 12 and Basic_Demos-Age is 13\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_PU >= 14 and Basic_Demos-Age is 14\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_PU >= 16 and Basic_Demos-Age is 15\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_PU >= 18 and Basic_Demos-Age is at least 16\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_PU >= 3 and Basic_Demos-Age is between 5 and 6\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_PU >= 4 and Basic_Demos-Age is 7\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_PU >= 5 and Basic_Demos-Age is 8\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_PU >= 6 and Basic_Demos-Age is 9\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_PU >= 7 and Basic_Demos-Age is at least 10\n",
    "\n",
    "def pullupzone(sex, age, pu):\n",
    "    try:\n",
    "        if np.isnan(sex) or np.isnan(age) or np.isnan(pu):\n",
    "            return np.nan\n",
    "        elif sex == 0:\n",
    "            if (age >= 16 and pu >= 18) or (age == 15 and pu >= 16) or (age == 14 and pu >= 14) or (age == 13 and pu >= 12) or (age == 12 and pu >= 10) or (age == 11 and pu >= 8) or (age == 10 and pu >= 7) or (age == 9 and pu >= 6) or (age == 8 and pu >= 5) or (age == 7 and pu >= 4) or (age <= 6 and pu >= 2):\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        elif sex == 1:\n",
    "            if (age >= 10 and pu >= 7) or (age == 9 and pu >= 6) or (age == 8 and pu >= 5) or (age == 7 and pu >= 4) or (age <= 6 and pu >= 3):\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comtlte values for the 'FGC-FGC_TL_Zone' that is equal to 1 if any of the following are true:\n",
    "# FGC-FGC_TL >= 6 and Basic_Demos-Age is between 5 and 9\n",
    "# FGC-FGC_TL >= 9 and Basic_Demos-Age is at least 10\n",
    "\n",
    "def tlzone(age, tl):\n",
    "    try:\n",
    "        if np.isnan(tl) or np.isnan(age):\n",
    "            return np.nan\n",
    "        elif (age >= 10 and tl >= 9) or (age <= 9 and tl >= 6):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comtlte values for the 'PAQ_MVPA' that is equal to 1 if any of the following are true:\n",
    "# PAQ_Total >= 2.73 and Basic_Demos-Age is between 5 and 13\n",
    "# PAQ_Total >= 2.75 and Basic_Demos-Age is at least 14\n",
    "\n",
    "def paqzone(age, paq):\n",
    "    try:\n",
    "        if np.isnan(paq) or np.isnan(age):\n",
    "            return np.nan\n",
    "        elif (age >= 14 and paq >= 2.75) or (age <= 13 and paq >= 2.73):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A Custom Encoder for Zone Variables**\n",
    "\n",
    "The goal of this next section is to define a function that will take in a dataframe and return one with the codes for the Zone variables based on the functions defined above\n",
    "\n",
    "It's possible that the dataframe might lack and age, sex, or one of the raw \"score\" variables that we'd use to do this encoding, so the encoder will need to check for the presence of these variables.\n",
    "\n",
    "If any of the variables are missing, the function imputes the mean of the already-present Zone values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zone_encoder(df):\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    if 'FGC-FGC_SR_Zone' in df_copy.columns:\n",
    "        if 'Basic_Demos-Age' in df_copy.columns and 'Basic_Demos-Sex' in df_copy.columns and 'FGC-FGC_SR' in df_copy.columns:\n",
    "            df_copy['FGC-FGC_SR_Zone'] = df_copy.apply(lambda x: sitreachzone(x['Basic_Demos-Sex'], x['Basic_Demos-Age'], x['FGC-FGC_SR']), axis=1)\n",
    "        else:\n",
    "            df_copy['FGC-FGC_SR_Zone'] = df_copy['FGC-FGC_SR_Zone'].fillna(df_copy['FGC-FGC_SR_Zone'].mean())\n",
    "    if 'FGC-FGC_CU_Zone' in df_copy.columns:\n",
    "        if 'Basic_Demos-Age' in df_copy.columns and 'Basic_Demos-Sex' in df_copy.columns and 'FGC-FGC_CU' in df_copy.columns:\n",
    "            df_copy['FGC-FGC_CU_Zone'] = df_copy.apply(lambda x: curlupzone(x['Basic_Demos-Sex'], x['Basic_Demos-Age'], x['FGC-FGC_CU']), axis=1)\n",
    "        else:\n",
    "            df_copy['FGC-FGC_CU_Zone'] = df_copy['FGC-FGC_CU_Zone'].fillna(df_copy['FGC-FGC_CU_Zone'].mean())\n",
    "    if 'FGC-FGC_PU_Zone' in df_copy.columns:\n",
    "        if 'Basic_Demos-Age' in df_copy.columns and 'Basic_Demos-Sex' in df_copy.columns and 'FGC-FGC_PU' in df_copy.columns:\n",
    "            df_copy['FGC-FGC_PU_Zone'] = df_copy.apply(lambda x: pullupzone(x['Basic_Demos-Sex'], x['Basic_Demos-Age'], x['FGC-FGC_PU']), axis=1)\n",
    "        else:\n",
    "            df_copy['FGC-FGC_PU_Zone'] = df_copy['FGC-FGC_PU_Zone'].fillna(df_copy['FGC-FGC_PU_Zone'].mean())\n",
    "    if 'FGC-FGC_TL_Zone' in df_copy.columns:\n",
    "        if 'Basic_Demos-Age' in df_copy.columns and 'FGC-FGC_TL' in df_copy.columns:\n",
    "            df_copy['FGC-FGC_TL_Zone'] = df_copy.apply(lambda x: tlzone(x['Basic_Demos-Age'], x['FGC-FGC_TL']), axis=1)\n",
    "        else:\n",
    "            df_copy['FGC-FGC_TL_Zone'] = df_copy['FGC-FGC_TL_Zone'].fillna(df_copy['FGC-FGC_TL_Zone'].mean())\n",
    "    if 'PAQ_Zone' in df_copy.columns:\n",
    "        if 'Basic_Demos-Age' in df_copy.columns and 'PAQ_Total' in df_copy.columns:\n",
    "            df_copy['PAQ_Zone'] = df_copy.apply(lambda x: tlzone(x['Basic_Demos-Age'], x['PAQ_Total']), axis=1)\n",
    "        else:\n",
    "            df_copy['PAQ_Zone'] = df_copy.apply(lambda x: paqzone(x['Basic_Demos-Age'], x['PAQ_Total']), axis=1)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking for NaN Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values in train_cleaned[predictors]: 22290\n",
      "Number of NaN values in train_cleaned['PCIAT-PCIAT_Total]: 0\n",
      "Number of NaN values in train_cleaned_imputed[predictors]: 3468\n",
      "Number of NaN values in train_cleaned_imputed_encoded[predictors]: 0\n"
     ]
    }
   ],
   "source": [
    "# Count the number of NaN values in train_cleaned[predictors]\n",
    "print(\"Number of NaN values in train_cleaned[predictors]:\", train_cleaned[predictors].isnull().sum().sum())\n",
    "\n",
    "# Count the number of NaN values in train_cleaned['PCIAT-PCIAT_Total]\n",
    "print(\"Number of NaN values in train_cleaned['PCIAT-PCIAT_Total]:\", train_cleaned['PCIAT-PCIAT_Total'].isnull().sum().sum())\n",
    "\n",
    "# Apply Custom_MICE_Imputer to train_cleaned\n",
    "mice_imputer = Custom_MICE_Imputer()\n",
    "train_cleaned_imputed = mice_imputer.fit_transform(train_cleaned)\n",
    "print(\"Number of NaN values in train_cleaned_imputed[predictors]:\", train_cleaned_imputed[predictors].isnull().sum().sum())\n",
    "\n",
    "# Apply zone_encoder to train_cleaned_imputed\n",
    "train_cleaned_imputed_encoded = zone_encoder(train_cleaned_imputed)\n",
    "print(\"Number of NaN values in train_cleaned_imputed_encoded[predictors]:\", train_cleaned_imputed_encoded[predictors].isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Constructing a Random Forest for Feature Identification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "\n",
    "predictors = train_cleaned.columns.tolist()\n",
    "if 'id' in predictors:\n",
    "    predictors.remove('id')\n",
    "if 'sii' in predictors:\n",
    "    predictors.remove('sii')\n",
    "predictors = [x for x in predictors if 'PCIAT' not in x]\n",
    "predictors = [x for x in predictors if 'Season' not in x]\n",
    "\n",
    "\n",
    "pipe_knn = Pipeline([('knn_impute', Custom_KNN_Imputer()),\n",
    "                    ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                    ('rf', RandomForestRegressor(n_estimators = 300, max_features = 'sqrt', max_depth = 5, random_state = 216))])\n",
    "\n",
    "pipe_knn.fit(train_cleaned[predictors],train_cleaned['PCIAT-PCIAT_Total'])\n",
    "\n",
    "train_pred_knn = pipe_knn.predict(train_cleaned[predictors])\n",
    "\n",
    "\n",
    "\n",
    "pipe_mice = Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "                    ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                    ('rf', RandomForestRegressor(n_estimators = 300, max_features = 'sqrt', max_depth = 5, random_state = 216))])\n",
    "\n",
    "pipe_mice.fit(train_cleaned[predictors],train_cleaned['PCIAT-PCIAT_Total'])\n",
    "\n",
    "train_pred_mice = pipe_mice.predict(train_cleaned[predictors])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get feature importance from the rf inside pipe\n",
    "score_knn_df = pd.DataFrame({'feature':train_cleaned[predictors].columns,\n",
    "                            'importance_score': pipe_knn.named_steps['rf'].feature_importances_})\n",
    "\n",
    "score_knn_df.sort_values('importance_score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get feature importance from the rf inside pipe\n",
    "score_mice_df = pd.DataFrame({'feature':train_cleaned[predictors].columns,\n",
    "                            'importance_score': pipe_mice.named_steps['rf'].feature_importances_})\n",
    "\n",
    "score_mice_df.sort_values('importance_score',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Constructing some Linear Models**\n",
    "\n",
    "In this section, I'll make linear models with:\n",
    "* A single predictor (hours spent on the internet)\n",
    "* A small number of predictors (taken from the importance scores generated above)\n",
    "* All the predictors\n",
    "\n",
    "Each of these will be run through a KFold split with a 20% validation set; for each model we'll compute several stats to compare the predictions with PCIAT scores and also with sii scores:\n",
    "* MSE\n",
    "* r2\n",
    "* kappa\n",
    "\n",
    "Note: custom loss functions for linear models are documented here: https://alexmiller.phd/posts/linear-model-custom-loss-function-regularization-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(models, data, iterations = 100):\n",
    "    results = {}\n",
    "    for i in models:\n",
    "        r2_train = []\n",
    "        r2_test = []\n",
    "        for j in range(iterations):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(data[X], \n",
    "                                                                data[Y], \n",
    "                                                                test_size= 0.2)\n",
    "            r2_test.append(metrics.r2_score(y_test,\n",
    "                                            models[i].fit(X_train, \n",
    "                                                         y_train).predict(X_test)))\n",
    "            r2_train.append(metrics.r2_score(y_train, \n",
    "                                             models[i].fit(X_train, \n",
    "                                                          y_train).predict(X_train)))\n",
    "        results[i] = [np.mean(r2_train), np.mean(r2_test)]\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "models = {'OLS': linear_model.LinearRegression(),\n",
    "           'Lasso': GridSearchCV(linear_model.Lasso(), \n",
    "                               param_grid=lasso_params).fit(df[X], df[Y]).best_estimator_,\n",
    "           'Ridge': GridSearchCV(linear_model.Ridge(), \n",
    "                               param_grid=ridge_params).fit(df[X], df[Y]).best_estimator_,}\n",
    "\n",
    "test(models, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sequential Binary Classification**\n",
    "\n",
    "It looks like our attempts so far have under-predicted sii values of 2 and 3. I'm going to try to implement a method that first predicts whether or not the sii value is 3, then on the remaining values predict whether or not they are 2, etc.\n",
    "\n",
    "I came up with this idea myself, but I wasn't the first one to do it. It was described on Medium: https://towardsdatascience.com/simple-trick-to-train-an-ordinal-regression-with-any-classifier-6911183d2a3c from an article by Frank and Hal\n",
    "\n",
    "Also described on stackoverflow: https://stackoverflow.com/questions/57561189/multi-class-multi-label-ordinal-classification-with-sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class OrdinalClassifier(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, clf):\n",
    "        self.clf = clf\n",
    "        self.clfs = {}\n",
    "        self.unique_class = np.NaN\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.unique_class = np.sort(np.unique(y))\n",
    "        if self.unique_class.shape[0] > 2:\n",
    "            for i in range(self.unique_class.shape[0]-1):\n",
    "                # for each k - 1 ordinal value we fit a binary classification problem\n",
    "                binary_y = (y > self.unique_class[i]).astype(np.uint8)\n",
    "                clf = clone(self.clf)\n",
    "                clf.fit(X, binary_y)\n",
    "                self.clfs[i] = clf\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        clfs_predict = {i: self.clfs[i].predict_proba(X) for i in self.clfs}\n",
    "        predicted = []\n",
    "        k = len(self.unique_class) - 1\n",
    "        for i, y in enumerate(self.unique_class):\n",
    "            if i == 0:\n",
    "                # V1 = 1 - Pr(y > V1)\n",
    "                predicted.append(1 - clfs_predict[0][:,1])\n",
    "            elif i < k:\n",
    "                # Vi = Pr(y <= Vi) * Pr(y > Vi-1)\n",
    "                 predicted.append((1 - clfs_predict[i][:,1]) * clfs_predict[i-1][:,1])\n",
    "            else:\n",
    "                # Vk = Pr(y > Vk-1)\n",
    "                predicted.append(clfs_predict[k-1][:,1])\n",
    "        return np.vstack(predicted).T\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.unique_class[np.argmax(self.predict_proba(X), axis=1)]\n",
    "\n",
    "    def score(self, X, y, sample_weight=None):\n",
    "        return accuracy_score(y, self.predict(X), sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using LASSO for Feature Selection**\n",
    "\n",
    "First, we'll try using LASSO to identify important features.\n",
    "\n",
    "Note that it isn't possible to use LASSO with pipelines (see https://stackoverflow.com/questions/39466671/use-of-scaler-with-lassocv-ridgecv). So we'll need to do the hyperparameter tuning manually.\n",
    "\n",
    "Some of the code below was suggested by Ali Furkan Kalay: https://alfurka.github.io/2018-11-18-grid-search/\n",
    "\n",
    "Some of the code below was suggested on Medium: https://medium.com/geekculture/regularization-using-pipeline-gridsearchcv-f377946e39d1\n",
    "\n",
    "Some of the code below was suggested on geeksforgeeks (https://www.geeksforgeeks.org/feature-selection-using-selectfrommodel-and-lassocv-in-scikit-learn/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tuning Lasso inside a Pipe with GridSearchCV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
      "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 455, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1004, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 848, in score\n",
      "    y_pred = self.predict(X)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 306, in predict\n",
      "    return self._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py\", line 1147, in _decision_function\n",
      "    return super()._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 285, in _decision_function\n",
      "    X = self._validate_data(X, accept_sparse=[\"csr\", \"csc\", \"coo\"], reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 633, in _validate_data\n",
      "    out = check_array(X, input_name=\"X\", **check_params)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1064, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains NaN.\n",
      "Lasso does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
      "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 455, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1004, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 848, in score\n",
      "    y_pred = self.predict(X)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 306, in predict\n",
      "    return self._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py\", line 1147, in _decision_function\n",
      "    return super()._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 285, in _decision_function\n",
      "    X = self._validate_data(X, accept_sparse=[\"csr\", \"csc\", \"coo\"], reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 633, in _validate_data\n",
      "    out = check_array(X, input_name=\"X\", **check_params)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1064, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains NaN.\n",
      "Lasso does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
      "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 455, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1004, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 848, in score\n",
      "    y_pred = self.predict(X)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 306, in predict\n",
      "    return self._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py\", line 1147, in _decision_function\n",
      "    return super()._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 285, in _decision_function\n",
      "    X = self._validate_data(X, accept_sparse=[\"csr\", \"csc\", \"coo\"], reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 633, in _validate_data\n",
      "    out = check_array(X, input_name=\"X\", **check_params)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1064, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains NaN.\n",
      "Lasso does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
      "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 455, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1004, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 848, in score\n",
      "    y_pred = self.predict(X)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 306, in predict\n",
      "    return self._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py\", line 1147, in _decision_function\n",
      "    return super()._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 285, in _decision_function\n",
      "    X = self._validate_data(X, accept_sparse=[\"csr\", \"csc\", \"coo\"], reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 633, in _validate_data\n",
      "    out = check_array(X, input_name=\"X\", **check_params)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1064, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains NaN.\n",
      "Lasso does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
      "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 455, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1004, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 848, in score\n",
      "    y_pred = self.predict(X)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 306, in predict\n",
      "    return self._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py\", line 1147, in _decision_function\n",
      "    return super()._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 285, in _decision_function\n",
      "    X = self._validate_data(X, accept_sparse=[\"csr\", \"csc\", \"coo\"], reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 633, in _validate_data\n",
      "    out = check_array(X, input_name=\"X\", **check_params)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1064, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains NaN.\n",
      "Lasso does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
      "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 455, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1004, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 848, in score\n",
      "    y_pred = self.predict(X)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 306, in predict\n",
      "    return self._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py\", line 1147, in _decision_function\n",
      "    return super()._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 285, in _decision_function\n",
      "    X = self._validate_data(X, accept_sparse=[\"csr\", \"csc\", \"coo\"], reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 633, in _validate_data\n",
      "    out = check_array(X, input_name=\"X\", **check_params)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1064, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains NaN.\n",
      "Lasso does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
      "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 455, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1004, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 848, in score\n",
      "    y_pred = self.predict(X)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 306, in predict\n",
      "    return self._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py\", line 1147, in _decision_function\n",
      "    return super()._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 285, in _decision_function\n",
      "    X = self._validate_data(X, accept_sparse=[\"csr\", \"csc\", \"coo\"], reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 633, in _validate_data\n",
      "    out = check_array(X, input_name=\"X\", **check_params)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1064, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains NaN.\n",
      "Lasso does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
      "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 455, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1004, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 848, in score\n",
      "    y_pred = self.predict(X)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 306, in predict\n",
      "    return self._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py\", line 1147, in _decision_function\n",
      "    return super()._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 285, in _decision_function\n",
      "    X = self._validate_data(X, accept_sparse=[\"csr\", \"csc\", \"coo\"], reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 633, in _validate_data\n",
      "    out = check_array(X, input_name=\"X\", **check_params)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1064, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains NaN.\n",
      "Lasso does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.441e+04, tolerance: 4.500e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
      "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 455, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1004, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 848, in score\n",
      "    y_pred = self.predict(X)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 306, in predict\n",
      "    return self._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py\", line 1147, in _decision_function\n",
      "    return super()._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 285, in _decision_function\n",
      "    X = self._validate_data(X, accept_sparse=[\"csr\", \"csc\", \"coo\"], reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 633, in _validate_data\n",
      "    out = check_array(X, input_name=\"X\", **check_params)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1064, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains NaN.\n",
      "Lasso does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.493e+05, tolerance: 4.500e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
      "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 455, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1004, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 848, in score\n",
      "    y_pred = self.predict(X)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 306, in predict\n",
      "    return self._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py\", line 1147, in _decision_function\n",
      "    return super()._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 285, in _decision_function\n",
      "    X = self._validate_data(X, accept_sparse=[\"csr\", \"csc\", \"coo\"], reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 633, in _validate_data\n",
      "    out = check_array(X, input_name=\"X\", **check_params)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1064, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains NaN.\n",
      "Lasso does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
      "10 fits failed out of a total of 20.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py\", line 980, in fit\n",
      "    X, y = self._validate_data(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 650, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1301, in check_X_y\n",
      "    X = check_array(\n",
      "        ^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1064, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains NaN.\n",
      "Lasso does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1103: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lasso__alpha': np.float64(5000000000.0)}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "  \n",
    "# Create a list of predictor variables; this eliminates id, sii, PCIAT, and Season variables\n",
    "predictors = train_cleaned.columns.tolist()\n",
    "if 'id' in predictors:\n",
    "    predictors.remove('id')\n",
    "if 'sii' in predictors:\n",
    "    predictors.remove('sii')\n",
    "predictors = [x for x in predictors if 'PCIAT' not in x]\n",
    "predictors = [x for x in predictors if 'Season' not in x]\n",
    "\n",
    "# A list of alpha (lambda) values to try in the hyperparameter tuning\n",
    "# create an array of 10**np.linspace(10,-2,100)*0.5\n",
    "#alphas = {'lasso__alpha': 10**np.linspace(10,-2,100)*0.5}\n",
    "alphas = {'lasso__alpha': 10**np.linspace(10,-2,10)*0.5}\n",
    "\n",
    "# Set up a lasso pipeline\n",
    "lasso_pipe = Pipeline([('impute', Custom_MICE_Imputer()),('fillzones', FunctionTransformer(zone_encoder)), ('lasso', Lasso())])\n",
    "\n",
    "gs_lasso_pipe = GridSearchCV(lasso_pipe, param_grid=alphas, cv=2).fit(train_cleaned[predictors], train_cleaned['PCIAT-PCIAT_Total'])\n",
    "\n",
    "gs_lasso_pipe.best_estimator_\n",
    "gs_lasso_pipe.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.feature_selection import SelectFromModel \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "  \n",
    "# Create a list of predictor variables; this eliminates id, sii, PCIAT, and Season variables\n",
    "predictors = train_cleaned.columns.tolist()\n",
    "if 'id' in predictors:\n",
    "    predictors.remove('id')\n",
    "if 'sii' in predictors:\n",
    "    predictors.remove('sii')\n",
    "predictors = [x for x in predictors if 'PCIAT' not in x]\n",
    "predictors = [x for x in predictors if 'Season' not in x]\n",
    "\n",
    "# Split the data into 80% Train/20% Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_cleaned[predictors], train_cleaned['PCIAT-PCIAT_Total'], test_size=0.2, random_state=216)\n",
    "\n",
    "# A list of alpha (lambda) values to try in the hyperparameter tuning\n",
    "alphas = 10**np.linspace(10,-2,100)*0.5\n",
    "\n",
    "# These will hold our coefficient estimates\n",
    "lasso_coefs = np.empty((len(alpha),n))\n",
    "\n",
    "# Set up a lasso pipeline\n",
    "lasso_pipe = Pipeline([('impute', Custom_MICE_Imputer()),('fillzones', FunctionTransformer(zone_encoder)), ('lasso', Lasso())])\n",
    "\n",
    "GridSearchCV(lasso_pipe, param_grid=alphas).fit(train_cleaned[predictors], train_cleaned['PCIAT-PCIAT_Total']).best_estimator_,\n",
    "\n",
    "def test(models, data, iterations = 100):\n",
    "    results = {}\n",
    "    for i in models:\n",
    "        r2_train = []\n",
    "        r2_test = []\n",
    "        for j in range(iterations):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(data[X], \n",
    "                                                                data[Y], \n",
    "                                                                test_size= 0.2)\n",
    "            r2_test.append(metrics.r2_score(y_test,\n",
    "                                            models[i].fit(X_train, \n",
    "                                                         y_train).predict(X_test)))\n",
    "            r2_train.append(metrics.r2_score(y_train, \n",
    "                                             models[i].fit(X_train, \n",
    "                                                          y_train).predict(X_train)))\n",
    "        results[i] = [np.mean(r2_train), np.mean(r2_test)]\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "models = {'OLS': linear_model.LinearRegression(),\n",
    "           'Lasso': GridSearchCV(linear_model.Lasso(), \n",
    "                               param_grid=lasso_params).fit(df[X], df[Y]).best_estimator_,\n",
    "           'Ridge': GridSearchCV(linear_model.Ridge(), \n",
    "                               param_grid=ridge_params).fit(df[X], df[Y]).best_estimator_,}\n",
    "\n",
    "test(models, df)\n",
    "\n",
    "## for each alpha value\n",
    "for i in range(len(alpha)):\n",
    "    ## set up the lasso pipeline\n",
    "    ## first scale\n",
    "    ## then make polynomial features\n",
    "    ## then fit the lasso regression model\n",
    "    lasso_pipe = Pipeline([('scale',StandardScaler()),\n",
    "                              ('poly',PolynomialFeatures(n, interaction_only=False, include_bias=False)),\n",
    "                              ('lasso', Lasso(alpha=alpha[i], max_iter=5000000))\n",
    "                          ])\n",
    "    \n",
    "    ## fit the lasso\n",
    "    lasso_pipe.fit(x.reshape(-1,1), y)\n",
    "\n",
    "    # record the coefficients\n",
    "    lasso_coefs[i,:] = lasso_pipe['lasso'].coef_\n",
    "\n",
    "\n",
    "# Fit LassoCV model with 5-fold cross-validation. It automatically evaluates performance over several folds in order to get the ideal regularization strength (alpha).\n",
    "lasso_cv = LassoCV(cv=5) \n",
    "lasso_cv.fit(X_train, y_train) \n",
    "\n",
    "# Feature selection. This selects the most significant features from the training and testing sets using the pre-trained lasso_cv model. \n",
    "# Only the features determined to be relevant by the L1 regularization are included in the final selected feature sets\n",
    "# These final selected feature sets are stored in X_train_selected and X_test_selected\n",
    "sfm = SelectFromModel(lasso_cv, prefit=True) \n",
    "X_train_selected = sfm.transform(X_train) \n",
    "X_test_selected = sfm.transform(X_test) \n",
    "\n",
    "# Train a Random Forest Classifier using the selected features \n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42) \n",
    "model.fit(X_train_selected, y_train) \n",
    "\n",
    "\n",
    "# Evaluate the model \n",
    "y_pred = model.predict(X_test_selected) \n",
    "print(classification_report(y_test, y_pred)) \n",
    "\n",
    "# Analyze selected features and their importance \n",
    "selected_feature_indices = np.where(sfm.get_support())[0] \n",
    "selected_features = train.columns[selected_feature_indices] \n",
    "coefficients = lasso_cv.coef_ \n",
    "print(\"Selected Features:\", selected_features) \n",
    "print(\"Feature Coefficients:\", coefficients) \n",
    "\n",
    "# Extract the selected features from the original dataset \n",
    "X_selected_features = X_train[:, selected_feature_indices] \n",
    "\n",
    "# Create a DataFrame for better visualization \n",
    "selected_features_df = pd.DataFrame(X_selected_features, columns=selected_features) \n",
    "\n",
    "# Add the target variable for coloring \n",
    "selected_features_df['target'] = y_train \n",
    "\n",
    "# Plot the two most important features \n",
    "sns.scatterplot(x='mean area', y='worst area', hue='target', data=selected_features_df, palette='viridis') \n",
    "plt.xlabel('Mean Area') \n",
    "plt.ylabel('Worst Area') \n",
    "plt.title('Scatter Plot of Two Most Important Features') \n",
    "plt.show() \n",
    "\n",
    "\n",
    "\n",
    "## This code will allow us to demonstrate the effect of \n",
    "## increasing alpha\n",
    "\n",
    "## set values for alpha\n",
    "alpha = [0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000]\n",
    "\n",
    "## The degree of the polynomial we will fit\n",
    "n=10\n",
    "\n",
    "#$ These will hold our coefficient estimates\n",
    "ridge_coefs = np.empty((len(alpha),n))\n",
    "lasso_coefs = np.empty((len(alpha),n))\n",
    "\n",
    "## for each alpha value\n",
    "for i in range(len(alpha)):\n",
    "    ## set up the lasso pipeline\n",
    "    ## first scale\n",
    "    ## then make polynomial features\n",
    "    ## then fit the lasso regression model\n",
    "    lasso_pipe = Pipeline([('scale',StandardScaler()),\n",
    "                              ('poly',PolynomialFeatures(n, interaction_only=False, include_bias=False)),\n",
    "                              ('lasso', Lasso(alpha=alpha[i], max_iter=5000000))\n",
    "                          ])\n",
    "    \n",
    "    ## fit the lasso\n",
    "    lasso_pipe.fit(x.reshape(-1,1), y)\n",
    "\n",
    "    # record the coefficients\n",
    "    lasso_coefs[i,:] = lasso_pipe['lasso'].coef_\n",
    "\n",
    "\n",
    "# A data frame to store the optimal alpha values\n",
    "bestalphas = pd.DataFrame(index=range(0,len(listofdatasets)))\n",
    "bestalphas['dfname'] = ''\n",
    "bestalphas['best_alpha_manual'] = np.nan\n",
    "bestalphas['best_alpha_automatic'] = np.nan\n",
    "\n",
    "\n",
    "for df in listofdatasets:\n",
    "    X_train = df.drop(columns=['PCIAT-PCIAT_Total'])\n",
    "    y_train = df['PCIAT-PCIAT_Total']\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_std = scaler.transform(X_train)\n",
    "    lassocv = LassoCV(alphas = alphas, scoring = 'neg_root_mean_squared_error')\n",
    "    lassocv.fit(X_std, y_train)\n",
    "    bestalphas.loc[bestalphas['dfname']==df.name,'best_alpha_automatic']=lassocv.alpha_.astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a Pipeline with the Custom Imputer and Transformer**\n",
    "\n",
    "Below is some code that is based on the 2_More_Advanced_Pipelines notebook from optional_extra_practice in Week 3\n",
    "\n",
    "In that code, their desired pipeline was:\n",
    "1 Impute the missing values of `body_mass_g` with the `median` value,\n",
    "2 Impute the missing values of `sex` with the most common value,\n",
    "3 One hot encode `island` and `sex` and\n",
    "4 Fit a random forest model to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "\n",
    "predictors = train_cleaned.columns.tolist()\n",
    "if 'id' in predictors:\n",
    "    predictors.remove('id')\n",
    "if 'sii' in predictors:\n",
    "    predictors.remove('sii')\n",
    "predictors = [x for x in predictors if 'PCIAT' not in x]\n",
    "predictors = [x for x in predictors if 'Season' not in x]\n",
    "\n",
    "\n",
    "pipe_knn = Pipeline([('knn_impute', Custom_KNN_Imputer()),\n",
    "                    ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                    ('rf', RandomForestRegressor(n_estimators = 300, max_features = 'sqrt', max_depth = 5, random_state = 216))])\n",
    "\n",
    "pipe_knn.fit(train_cleaned[predictors],train_cleaned['PCIAT-PCIAT_Total'])\n",
    "\n",
    "train_pred_knn = pipe_knn.predict(train_cleaned[predictors])\n",
    "\n",
    "\n",
    "\n",
    "pipe_mice = Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "                    ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                    ('rf', RandomForestRegressor(n_estimators = 300, max_features = 'sqrt', max_depth = 5, random_state = 216))])\n",
    "\n",
    "pipe_mice.fit(train_cleaned[predictors],train_cleaned['PCIAT-PCIAT_Total'])\n",
    "\n",
    "train_pred_mice = pipe_mice.predict(train_cleaned[predictors])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get feature importance from the rf inside pipe\n",
    "score_knn_df = pd.DataFrame({'feature':train_cleaned[predictors].columns,\n",
    "                            'importance_score': pipe_knn.named_steps['rf'].feature_importances_})\n",
    "\n",
    "score_knn_df.sort_values('importance_score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get feature importance from the rf inside pipe\n",
    "score_mice_df = pd.DataFrame({'feature':train_cleaned[predictors].columns,\n",
    "                            'importance_score': pipe_mice.named_steps['rf'].feature_importances_})\n",
    "\n",
    "score_mice_df.sort_values('importance_score',ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_fall_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
