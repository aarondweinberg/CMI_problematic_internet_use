{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goals**\n",
    "\n",
    "The goal of this notebook is to explore feature selection, continuing from the AW EDA Exploration goals:\n",
    "\n",
    "9. Create a linear regression model using a greedy algorithm from the \"bottom up\"\n",
    "    1. Make a list of all numerical predictors and also a new empty data frame with 100(?) rows and the predictors as variables\n",
    "    2. Randomly select a predictor from the list and create a linear model\n",
    "    3. Randomly select a second predictor from the list and add it to the model\n",
    "    4. Perform an F test to see if the new model is significantly better than the old\n",
    "    5. Repeat until the F test is no longer significant\n",
    "    6. Record the predictors that are in the model in the newly-created data frame\n",
    "    7. Repeat the above steps 100 (??) times\n",
    "    8. Compute the mean for each predictor in the data frame. This should give some sense of the \"importance\" of each predictor\n",
    "10. Repeat the previous method but using a \"top down\" algorithm, starting with a full model and removing predictors one-by-one\n",
    "11. *Maybe* Trying to use PCA and either linear or KNN regression to see if it appears to improve prediction\n",
    "    * PCA on the entire set of predictors\n",
    "    * On each set of grouped predictors\n",
    "12. Using RandomForest Regression on the entire set of predictors and examining the importance matrix to try to find a potential list of predictors\n",
    "13. *Maybe* using XGBoost to do stuff. (Need to learn what this is)\n",
    "14. Removing highly-correlated predictors and using LASSO and using LASSO regression (with hyperparameter tuning) to identify important predictors\n",
    "15. Comparing the apparent predictive power of all the previous methods. If none stand out, then stick with linear regression(?)\n",
    "16. Start to engage more formally with the modeling process, using Kfold splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the Data**\n",
    "\n",
    "For the purpose of developing our model(s), we'll work with data that include the imputed outcome (PCIAT_Total and/or sii) scores AND have cleaned predictors.\n",
    "\n",
    "In the final version of our code, we'll work with data with cleaned predictors but won't have any access to the outcome scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the cleaned & predictor-imputed data\n",
    "train_cleaned=pd.read_csv('train_cleaned_outcome_imputed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using KNN to Impute Values of Predictor Variables**\n",
    "\n",
    "Our first code chunk will use a KNN algorithm with all available predictor columns, excluding the Zone and Season columns\n",
    "\n",
    "We'll start by making a list of quantitative predictor variables. Note that:\n",
    "* The Zone variables are computed from others; we'll re-compute their values after doing imputation\n",
    "* The list includes Basic_Demos-Sex. Although this is categorical, all participants have data for this variable, and it's useful for imputing other variables\n",
    "* We *could* convert the Season variables into dummy variables, but this seems like it would over-weight them for KNN imputation. So we're leaving them out.\n",
    "\n",
    "Then, we'll construct and use a KNN imputer with 5 neighbors to impute missing values.\n",
    "\n",
    "We'll wrapp all of this inside a custom imputer that can be called inside a pipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We'll need these\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "## Define our custom imputer\n",
    "class Custom_KNN_Imputer(BaseEstimator, TransformerMixin):\n",
    "    # Class Constructor \n",
    "    # This allows you to initiate the class when you call Custom_KNN_Imputer\n",
    "    def __init__(self):\n",
    "        # I want to initiate each object with both a KNNImputer and StandardScaler object/method\n",
    "        self.KNNImputer = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')\n",
    "        self.StandardScaler = StandardScaler()\n",
    "\n",
    "    \n",
    "    # For my fit method I'm just going to \"steal\" KNNImputers's fit method using a curated collection of predictors\n",
    "    def fit(self, X, y = None ):\n",
    "        feature_list = X.columns.tolist()\n",
    "        if 'id' in feature_list:\n",
    "            feature_list.remove('id')\n",
    "        if 'sii' in feature_list:\n",
    "            feature_list.remove('sii')\n",
    "        feature_list = [x for x in feature_list if 'PCIAT' not in x]\n",
    "        feature_list = [x for x in feature_list if 'Zone' not in x]\n",
    "        feature_list = [x for x in feature_list if 'Season' not in x]\n",
    "        self.StandardScaler.fit(X[feature_list])\n",
    "        # I'm never sure if we need the .values and/or .reshape(-1,1)\n",
    "        #self.KNNImputer.fit(X[feature_list].values.reshape(-1,1))\n",
    "        self.KNNImputer.fit(X[feature_list])\n",
    "        return self\n",
    "    \n",
    "    # Now I want to transform the columns in feature list and return it with imputed values that have been un-transformed\n",
    "    def transform(self, X, y = None):\n",
    "        feature_list = X.columns.tolist()\n",
    "        if 'id' in feature_list:\n",
    "            feature_list.remove('id')\n",
    "        if 'sii' in feature_list:\n",
    "            feature_list.remove('sii')\n",
    "        feature_list = [x for x in feature_list if 'PCIAT' not in x]\n",
    "        feature_list = [x for x in feature_list if 'Zone' not in x]\n",
    "        feature_list = [x for x in feature_list if 'Season' not in x]\n",
    "        copy_X = X.copy()\n",
    "        copy_X[feature_list] = self.KNNImputer.transform(copy_X[feature_list])\n",
    "        copy_X2 = self.StandardScaler.inverse_transform(copy_X[feature_list])\n",
    "        df2 = pd.DataFrame(copy_X2, columns=feature_list)\n",
    "        copy_X[feature_list]=copy_X[feature_list].fillna(df2[feature_list])\n",
    "        return copy_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A Custom MICE Imputer**\n",
    "\n",
    "Next, we'll try to take the above code and turn it into a custom imputer that can be used inside a pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We'll need these\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "## Define our custom imputer\n",
    "class Custom_MICE_Imputer(BaseEstimator, TransformerMixin):\n",
    "    # Class Constructor \n",
    "    # This allows you to initiate the class when you call Custom_KNN_Imputer\n",
    "    def __init__(self):\n",
    "        # I want to initiate each object with both a KNNImputer and StandardScaler object/method\n",
    "        self.MICEImputer = IterativeImputer(max_iter=10, random_state=497)\n",
    "\n",
    "    \n",
    "    # For my fit method I'm just going to \"steal\" IterativeImputers's fit method using a curated collection of predictors\n",
    "    def fit(self, X, y = None ):\n",
    "        feature_list = X.columns.tolist()\n",
    "        if 'id' in feature_list:\n",
    "            feature_list.remove('id')\n",
    "        if 'sii' in feature_list:\n",
    "            feature_list.remove('sii')\n",
    "        feature_list = [x for x in feature_list if 'PCIAT' not in x]\n",
    "        feature_list = [x for x in feature_list if 'Zone' not in x]\n",
    "        feature_list = [x for x in feature_list if 'Season' not in x]\n",
    "        self.MICEImputer.fit(X[feature_list])\n",
    "        return self\n",
    "    \n",
    "    # Now I want to transform the columns in feature list and return it with imputed values that have been un-transformed\n",
    "    def transform(self, X, y = None):\n",
    "        feature_list = X.columns.tolist()\n",
    "        if 'id' in feature_list:\n",
    "            feature_list.remove('id')\n",
    "        if 'sii' in feature_list:\n",
    "            feature_list.remove('sii')\n",
    "        feature_list = [x for x in feature_list if 'PCIAT' not in x]\n",
    "        feature_list = [x for x in feature_list if 'Zone' not in x]\n",
    "        feature_list = [x for x in feature_list if 'Season' not in x]\n",
    "        copy_X = X.copy()\n",
    "        df2 = self.MICEImputer.transform(copy_X[feature_list])\n",
    "        df3 = pd.DataFrame(df2, columns=feature_list)\n",
    "        copy_X[feature_list]=copy_X[feature_list].fillna(df3[feature_list])\n",
    "        return copy_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computing Zone Values**\n",
    "\n",
    "In this section, we'll create functions that compute the FGC Zone and PAQ_Zone values from the corresponding FGC raw and PAQ_Total (imputed) scores\n",
    "\n",
    "FitnessGram Healthy Fitness Zones are documented at https://pftdata.org/files/hfz-standards.pdf for:\n",
    "* FGC-FGC_CU_Zone\n",
    "* FGC-FGC_PU_Zone\n",
    "* FGC-FGC_TL_Zone\n",
    "* FGC-FGC_SR_Zone\n",
    "\n",
    "FitnessGram Grip Strength Zones appear to be documented at https://www.topendsports.com/testing/norms/handgrip.htm. However, these zones are only defined for ages 10 and up. And it appears that no participants under the age of 10 had their grip strength measured. So maybe it doesn't make sense to include this predictor at all?\n",
    "\n",
    "For the PAQ numbers, some research (https://pubmed.ncbi.nlm.nih.gov/27759968/) has identified a cut-off score of 2.75 (ages 14-20) and 2.73 (ages 8-14) to discriminate >60 minutes of MVPA. However, the study suggests that, while the cutoff is significant for the older group, it isn't for for the younger.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute values for the 'FGC-FGC_SR_Zone' that is equal to 1 if any of the following are true:\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_SR >= 8\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_SR >= 9 and Basic_Demos-Age is between 5 and 10\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_SR >= 10 and Basic_Demos-Age is between 11 and 14\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_SR >= 12 and Basic_Demos-Age is at least 15\n",
    "# Note that Basic_Demos-Sex is coded as 0=Male and 1=Female\n",
    "\n",
    "def sitreachzone(sex, age, sr):\n",
    "    try:\n",
    "        if np.isnan(sr) or np.isnan(sex) or np.isnan(age):\n",
    "            return np.nan\n",
    "        elif sex == 0 and sr>=8:\n",
    "            return 1\n",
    "        elif sex == 1 and age >= 15 and sr >= 12:\n",
    "            return 1\n",
    "        elif sex == 1 and age >= 11 and sr >= 10:\n",
    "            return 1\n",
    "        elif sex == 1 and age >= 5 and sr >= 9:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute values for the 'FGC-FGC_CU_Zone' that is equal to 1 if any of the following are true:\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_CU >= 2 and Basic_Demos-Age is between 5 and 6\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_CU >= 4 and Basic_Demos-Age is 7\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_CU >= 6 and Basic_Demos-Age is 8\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_CU >= 9 and Basic_Demos-Age is 9\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_CU >= 12 and Basic_Demos-Age is 10\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_CU >= 15 and Basic_Demos-Age is 11\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_CU >= 18 and Basic_Demos-Age is 12\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_CU >= 21 and Basic_Demos-Age is 13\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_CU >= 24 and Basic_Demos-Age is at least 14\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_CU >= 2 and Basic_Demos-Age is between 5 and 6\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_CU >= 4 and Basic_Demos-Age is 7\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_CU >= 6 and Basic_Demos-Age is 8\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_CU >= 9 and Basic_Demos-Age is 9\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_CU >= 12 and Basic_Demos-Age is 10\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_CU >= 15 and Basic_Demos-Age is 11\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_CU >= 18 and Basic_Demos-Age is at least 12\n",
    "\n",
    "def curlupzone(sex, age, cu):\n",
    "    try:\n",
    "        if np.isnan(sex) or np.isnan(age) or np.isnan(cu):\n",
    "            return np.nan\n",
    "        elif sex == 0:\n",
    "            if (age >= 14 and cu >= 24) or (age == 13 and cu >= 21) or (age == 12 and cu >= 18) or (age == 11 and cu >= 15) or (age == 10 and cu >= 12) or (age == 9 and cu >= 9) or (age == 8 and cu >= 6) or (age == 7 and cu >= 4) or (age <= 6 and cu >= 2):\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        elif sex == 1:\n",
    "            if (age >= 12 and cu >= 18) or (age == 11 and cu >= 15) or (age == 10 and cu >= 12) or (age == 9 and cu >= 9) or (age == 8 and cu >= 6) or (age == 7 and cu >= 4) or (age <= 6 and cu >= 2):\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute values for the 'FGC-FGC_PU_Zone' that is equal to 1 if any of the following are true:\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_PU >= 3 and Basic_Demos-Age is between 5 and 6\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_PU >= 4 and Basic_Demos-Age is 7\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_PU >= 5 and Basic_Demos-Age is 8\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_PU >= 6 and Basic_Demos-Age is 9\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_PU >= 7 and Basic_Demos-Age is 10\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_PU >= 8 and Basic_Demos-Age is 11\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_PU >= 10 and Basic_Demos-Age is 12\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_PU >= 12 and Basic_Demos-Age is 13\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_PU >= 14 and Basic_Demos-Age is 14\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_PU >= 16 and Basic_Demos-Age is 15\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_PU >= 18 and Basic_Demos-Age is at least 16\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_PU >= 3 and Basic_Demos-Age is between 5 and 6\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_PU >= 4 and Basic_Demos-Age is 7\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_PU >= 5 and Basic_Demos-Age is 8\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_PU >= 6 and Basic_Demos-Age is 9\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_PU >= 7 and Basic_Demos-Age is at least 10\n",
    "\n",
    "def pullupzone(sex, age, pu):\n",
    "    try:\n",
    "        if np.isnan(sex) or np.isnan(age) or np.isnan(pu):\n",
    "            return np.nan\n",
    "        elif sex == 0:\n",
    "            if (age >= 16 and pu >= 18) or (age == 15 and pu >= 16) or (age == 14 and pu >= 14) or (age == 13 and pu >= 12) or (age == 12 and pu >= 10) or (age == 11 and pu >= 8) or (age == 10 and pu >= 7) or (age == 9 and pu >= 6) or (age == 8 and pu >= 5) or (age == 7 and pu >= 4) or (age <= 6 and pu >= 2):\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        elif sex == 1:\n",
    "            if (age >= 10 and pu >= 7) or (age == 9 and pu >= 6) or (age == 8 and pu >= 5) or (age == 7 and pu >= 4) or (age <= 6 and pu >= 3):\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comtlte values for the 'FGC-FGC_TL_Zone' that is equal to 1 if any of the following are true:\n",
    "# FGC-FGC_TL >= 6 and Basic_Demos-Age is between 5 and 9\n",
    "# FGC-FGC_TL >= 9 and Basic_Demos-Age is at least 10\n",
    "\n",
    "def tlzone(age, tl):\n",
    "    try:\n",
    "        if np.isnan(tl) or np.isnan(age):\n",
    "            return np.nan\n",
    "        elif (age >= 10 and tl >= 9) or (age <= 9 and tl >= 6):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comtlte values for the 'PAQ_MVPA' that is equal to 1 if any of the following are true:\n",
    "# PAQ_Total >= 2.73 and Basic_Demos-Age is between 5 and 13\n",
    "# PAQ_Total >= 2.75 and Basic_Demos-Age is at least 14\n",
    "\n",
    "def paqzone(age, paq):\n",
    "    try:\n",
    "        if np.isnan(paq) or np.isnan(age):\n",
    "            return np.nan\n",
    "        elif (age >= 14 and paq >= 2.75) or (age <= 13 and paq >= 2.73):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A Custom Encoder for Zone Variables**\n",
    "\n",
    "The goal of this next section is to define a function that will take in a dataframe and return one with the codes for the Zone variables based on the functions defined above\n",
    "\n",
    "It's possible that the dataframe might lack and age, sex, or one of the raw \"score\" variables that we'd use to do this encoding, so the encoder will need to check for the presence of these variables.\n",
    "\n",
    "If any of the variables are missing, the function imputes the mean of the already-present Zone values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zone_encoder(df):\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    if 'FGC-FGC_SR_Zone' in df_copy.columns:\n",
    "        if 'Basic_Demos-Age' in df_copy.columns and 'Basic_Demos-Sex' in df_copy.columns and 'FGC-FGC_SR' in df_copy.columns:\n",
    "            df_copy['FGC-FGC_SR_Zone'] = df_copy.apply(lambda x: sitreachzone(x['Basic_Demos-Sex'], x['Basic_Demos-Age'], x['FGC-FGC_SR']), axis=1)\n",
    "        else:\n",
    "            df_copy['FGC-FGC_SR_Zone'] = df_copy['FGC-FGC_SR_Zone'].fillna(df_copy['FGC-FGC_SR_Zone'].mean())\n",
    "    if 'FGC-FGC_CU_Zone' in df_copy.columns:\n",
    "        if 'Basic_Demos-Age' in df_copy.columns and 'Basic_Demos-Sex' in df_copy.columns and 'FGC-FGC_CU' in df_copy.columns:\n",
    "            df_copy['FGC-FGC_CU_Zone'] = df_copy.apply(lambda x: curlupzone(x['Basic_Demos-Sex'], x['Basic_Demos-Age'], x['FGC-FGC_CU']), axis=1)\n",
    "        else:\n",
    "            df_copy['FGC-FGC_CU_Zone'] = df_copy['FGC-FGC_CU_Zone'].fillna(df_copy['FGC-FGC_CU_Zone'].mean())\n",
    "    if 'FGC-FGC_PU_Zone' in df_copy.columns:\n",
    "        if 'Basic_Demos-Age' in df_copy.columns and 'Basic_Demos-Sex' in df_copy.columns and 'FGC-FGC_PU' in df_copy.columns:\n",
    "            df_copy['FGC-FGC_PU_Zone'] = df_copy.apply(lambda x: pullupzone(x['Basic_Demos-Sex'], x['Basic_Demos-Age'], x['FGC-FGC_PU']), axis=1)\n",
    "        else:\n",
    "            df_copy['FGC-FGC_PU_Zone'] = df_copy['FGC-FGC_PU_Zone'].fillna(df_copy['FGC-FGC_PU_Zone'].mean())\n",
    "    if 'FGC-FGC_TL_Zone' in df_copy.columns:\n",
    "        if 'Basic_Demos-Age' in df_copy.columns and 'FGC-FGC_TL' in df_copy.columns:\n",
    "            df_copy['FGC-FGC_TL_Zone'] = df_copy.apply(lambda x: tlzone(x['Basic_Demos-Age'], x['FGC-FGC_TL']), axis=1)\n",
    "        else:\n",
    "            df_copy['FGC-FGC_TL_Zone'] = df_copy['FGC-FGC_TL_Zone'].fillna(df_copy['FGC-FGC_TL_Zone'].mean())\n",
    "    if 'PAQ_Zone' in df_copy.columns:\n",
    "        if 'Basic_Demos-Age' in df_copy.columns and 'PAQ_Total' in df_copy.columns:\n",
    "            df_copy['PAQ_Zone'] = df_copy.apply(lambda x: tlzone(x['Basic_Demos-Age'], x['PAQ_Total']), axis=1)\n",
    "        else:\n",
    "            df_copy['PAQ_Zone'] = df_copy.apply(lambda x: paqzone(x['Basic_Demos-Age'], x['PAQ_Total']), axis=1)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking for NaN Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values in train_cleaned[predictors]: 22290\n",
      "Number of NaN values in train_cleaned['PCIAT-PCIAT_Total]: 0\n",
      "Number of NaN values in train_cleaned_imputed[predictors]: 3468\n",
      "Number of NaN values in train_cleaned_imputed_encoded[predictors]: 0\n"
     ]
    }
   ],
   "source": [
    "predictors = train_cleaned.columns.tolist()\n",
    "if 'id' in predictors:\n",
    "    predictors.remove('id')\n",
    "if 'sii' in predictors:\n",
    "    predictors.remove('sii')\n",
    "predictors = [x for x in predictors if 'PCIAT' not in x]\n",
    "predictors = [x for x in predictors if 'Season' not in x]\n",
    "\n",
    "# Count the number of NaN values in train_cleaned[predictors]\n",
    "print(\"Number of NaN values in train_cleaned[predictors]:\", train_cleaned[predictors].isnull().sum().sum())\n",
    "\n",
    "# Count the number of NaN values in train_cleaned['PCIAT-PCIAT_Total]\n",
    "print(\"Number of NaN values in train_cleaned['PCIAT-PCIAT_Total]:\", train_cleaned['PCIAT-PCIAT_Total'].isnull().sum().sum())\n",
    "\n",
    "# Apply Custom_MICE_Imputer to train_cleaned\n",
    "mice_imputer = Custom_MICE_Imputer()\n",
    "train_cleaned_imputed = mice_imputer.fit_transform(train_cleaned)\n",
    "print(\"Number of NaN values in train_cleaned_imputed[predictors]:\", train_cleaned_imputed[predictors].isnull().sum().sum())\n",
    "\n",
    "# Apply zone_encoder to train_cleaned_imputed\n",
    "train_cleaned_imputed_encoded = zone_encoder(train_cleaned_imputed)\n",
    "print(\"Number of NaN values in train_cleaned_imputed_encoded[predictors]:\", train_cleaned_imputed_encoded[predictors].isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "355.84224243935904"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "slr = LinearRegression()\n",
    "slr.fit(train_cleaned_imputed_encoded[['PreInt_EduHx-computerinternet_hoursday']],train_cleaned_imputed_encoded['PCIAT-PCIAT_Total'])\n",
    "    \n",
    "mean_squared_error(train_cleaned_imputed_encoded['PCIAT-PCIAT_Total'], slr.predict(train_cleaned_imputed_encoded[['PreInt_EduHx-computerinternet_hoursday']]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Constructing a Random Forest for Feature Identification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "\n",
    "predictors = train_cleaned.columns.tolist()\n",
    "if 'id' in predictors:\n",
    "    predictors.remove('id')\n",
    "if 'sii' in predictors:\n",
    "    predictors.remove('sii')\n",
    "predictors = [x for x in predictors if 'PCIAT' not in x]\n",
    "predictors = [x for x in predictors if 'Season' not in x]\n",
    "\n",
    "\n",
    "pipe_knn = Pipeline([('knn_impute', Custom_KNN_Imputer()),\n",
    "                    ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                    ('rf', RandomForestRegressor(n_estimators = 300, max_features = 'sqrt', max_depth = 5, random_state = 216))])\n",
    "\n",
    "pipe_knn.fit(train_cleaned[predictors],train_cleaned['PCIAT-PCIAT_Total'])\n",
    "\n",
    "train_pred_knn = pipe_knn.predict(train_cleaned[predictors])\n",
    "\n",
    "\n",
    "\n",
    "pipe_mice = Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "                    ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                    ('rf', RandomForestRegressor(n_estimators = 300, max_features = 'sqrt', max_depth = 5, random_state = 216))])\n",
    "\n",
    "pipe_mice.fit(train_cleaned[predictors],train_cleaned['PCIAT-PCIAT_Total'])\n",
    "\n",
    "train_pred_mice = pipe_mice.predict(train_cleaned[predictors])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Basic_Demos-Age</td>\n",
       "      <td>0.142554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Physical-Height</td>\n",
       "      <td>0.135766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>PreInt_EduHx-computerinternet_hoursday</td>\n",
       "      <td>0.118846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Physical-Weight</td>\n",
       "      <td>0.081496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>BIA-BIA_FFM</td>\n",
       "      <td>0.072900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>SDS-SDS_Total_Raw</td>\n",
       "      <td>0.069851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>FGC-FGC_CU</td>\n",
       "      <td>0.057256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Physical-Waist_Circumference</td>\n",
       "      <td>0.036904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>BIA-BIA_FFMI</td>\n",
       "      <td>0.029994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>BIA-BIA_Fat</td>\n",
       "      <td>0.028353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Physical-BMI</td>\n",
       "      <td>0.022395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ENMO_Avg_Active_Days_MVPA110</td>\n",
       "      <td>0.019681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>FGC-FGC_PU</td>\n",
       "      <td>0.017542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ENMO_Avg_Active_Days_MVPA192</td>\n",
       "      <td>0.017389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>FGC-FGC_SR</td>\n",
       "      <td>0.016435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>BIA-BIA_FMI</td>\n",
       "      <td>0.015657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>PAQ_Total</td>\n",
       "      <td>0.015427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CGAS-CGAS_Score</td>\n",
       "      <td>0.012510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Fitness_Endurance_Total_Time_Sec</td>\n",
       "      <td>0.012299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>FGC-FGC_TL</td>\n",
       "      <td>0.011939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Physical-Systolic_BP</td>\n",
       "      <td>0.011459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Physical-HeartRate</td>\n",
       "      <td>0.008799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Positive_Anglez_Active_Days</td>\n",
       "      <td>0.008101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Fitness_Endurance-Max_Stage</td>\n",
       "      <td>0.007865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Physical-Diastolic_BP</td>\n",
       "      <td>0.007832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>BIA-BIA_Activity_Level_num</td>\n",
       "      <td>0.006887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>BIA-BIA_Frame_num</td>\n",
       "      <td>0.004677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Basic_Demos-Sex</td>\n",
       "      <td>0.004533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>FGC-FGC_CU_Zone</td>\n",
       "      <td>0.001520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>FGC-FGC_PU_Zone</td>\n",
       "      <td>0.001207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>FGC-FGC_TL_Zone</td>\n",
       "      <td>0.001190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>FGC-FGC_SR_Zone</td>\n",
       "      <td>0.000736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>PAQ_Zone</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   feature  importance_score\n",
       "0                          Basic_Demos-Age          0.142554\n",
       "4                          Physical-Height          0.135766\n",
       "24  PreInt_EduHx-computerinternet_hoursday          0.118846\n",
       "5                          Physical-Weight          0.081496\n",
       "18                             BIA-BIA_FFM          0.072900\n",
       "23                       SDS-SDS_Total_Raw          0.069851\n",
       "11                              FGC-FGC_CU          0.057256\n",
       "6             Physical-Waist_Circumference          0.036904\n",
       "19                            BIA-BIA_FFMI          0.029994\n",
       "21                             BIA-BIA_Fat          0.028353\n",
       "3                             Physical-BMI          0.022395\n",
       "26            ENMO_Avg_Active_Days_MVPA110          0.019681\n",
       "13                              FGC-FGC_PU          0.017542\n",
       "25            ENMO_Avg_Active_Days_MVPA192          0.017389\n",
       "28                              FGC-FGC_SR          0.016435\n",
       "20                             BIA-BIA_FMI          0.015657\n",
       "30                               PAQ_Total          0.015427\n",
       "2                          CGAS-CGAS_Score          0.012510\n",
       "32        Fitness_Endurance_Total_Time_Sec          0.012299\n",
       "15                              FGC-FGC_TL          0.011939\n",
       "9                     Physical-Systolic_BP          0.011459\n",
       "8                       Physical-HeartRate          0.008799\n",
       "27             Positive_Anglez_Active_Days          0.008101\n",
       "10             Fitness_Endurance-Max_Stage          0.007865\n",
       "7                    Physical-Diastolic_BP          0.007832\n",
       "17              BIA-BIA_Activity_Level_num          0.006887\n",
       "22                       BIA-BIA_Frame_num          0.004677\n",
       "1                          Basic_Demos-Sex          0.004533\n",
       "12                         FGC-FGC_CU_Zone          0.001520\n",
       "14                         FGC-FGC_PU_Zone          0.001207\n",
       "16                         FGC-FGC_TL_Zone          0.001190\n",
       "29                         FGC-FGC_SR_Zone          0.000736\n",
       "31                                PAQ_Zone          0.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get feature importance from the rf inside pipe\n",
    "score_knn_df = pd.DataFrame({'feature':train_cleaned[predictors].columns,\n",
    "                            'importance_score': pipe_knn.named_steps['rf'].feature_importances_})\n",
    "\n",
    "score_knn_df.sort_values('importance_score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Basic_Demos-Age</td>\n",
       "      <td>0.137804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Physical-Height</td>\n",
       "      <td>0.126698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>PreInt_EduHx-computerinternet_hoursday</td>\n",
       "      <td>0.118666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>BIA-BIA_FFM</td>\n",
       "      <td>0.077628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>SDS-SDS_Total_Raw</td>\n",
       "      <td>0.074039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Physical-Weight</td>\n",
       "      <td>0.072494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ENMO_Avg_Active_Days_MVPA110</td>\n",
       "      <td>0.065296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>FGC-FGC_CU</td>\n",
       "      <td>0.055829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>BIA-BIA_FFMI</td>\n",
       "      <td>0.023911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>FGC-FGC_PU</td>\n",
       "      <td>0.023766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>BIA-BIA_Fat</td>\n",
       "      <td>0.023610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Physical-BMI</td>\n",
       "      <td>0.018582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>PAQ_Total</td>\n",
       "      <td>0.018188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>FGC-FGC_TL</td>\n",
       "      <td>0.015412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Fitness_Endurance-Max_Stage</td>\n",
       "      <td>0.014949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Physical-Waist_Circumference</td>\n",
       "      <td>0.013865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>FGC-FGC_SR</td>\n",
       "      <td>0.013734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CGAS-CGAS_Score</td>\n",
       "      <td>0.012663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>BIA-BIA_FMI</td>\n",
       "      <td>0.011455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ENMO_Avg_Active_Days_MVPA192</td>\n",
       "      <td>0.011191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Physical-Systolic_BP</td>\n",
       "      <td>0.011164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Positive_Anglez_Active_Days</td>\n",
       "      <td>0.010808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Fitness_Endurance_Total_Time_Sec</td>\n",
       "      <td>0.009825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Physical-HeartRate</td>\n",
       "      <td>0.008186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>BIA-BIA_Activity_Level_num</td>\n",
       "      <td>0.007720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Physical-Diastolic_BP</td>\n",
       "      <td>0.007447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>BIA-BIA_Frame_num</td>\n",
       "      <td>0.006813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Basic_Demos-Sex</td>\n",
       "      <td>0.004352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>FGC-FGC_TL_Zone</td>\n",
       "      <td>0.001435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>FGC-FGC_CU_Zone</td>\n",
       "      <td>0.001110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>FGC-FGC_PU_Zone</td>\n",
       "      <td>0.000719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>FGC-FGC_SR_Zone</td>\n",
       "      <td>0.000643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>PAQ_Zone</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   feature  importance_score\n",
       "0                          Basic_Demos-Age          0.137804\n",
       "4                          Physical-Height          0.126698\n",
       "24  PreInt_EduHx-computerinternet_hoursday          0.118666\n",
       "18                             BIA-BIA_FFM          0.077628\n",
       "23                       SDS-SDS_Total_Raw          0.074039\n",
       "5                          Physical-Weight          0.072494\n",
       "26            ENMO_Avg_Active_Days_MVPA110          0.065296\n",
       "11                              FGC-FGC_CU          0.055829\n",
       "19                            BIA-BIA_FFMI          0.023911\n",
       "13                              FGC-FGC_PU          0.023766\n",
       "21                             BIA-BIA_Fat          0.023610\n",
       "3                             Physical-BMI          0.018582\n",
       "30                               PAQ_Total          0.018188\n",
       "15                              FGC-FGC_TL          0.015412\n",
       "10             Fitness_Endurance-Max_Stage          0.014949\n",
       "6             Physical-Waist_Circumference          0.013865\n",
       "28                              FGC-FGC_SR          0.013734\n",
       "2                          CGAS-CGAS_Score          0.012663\n",
       "20                             BIA-BIA_FMI          0.011455\n",
       "25            ENMO_Avg_Active_Days_MVPA192          0.011191\n",
       "9                     Physical-Systolic_BP          0.011164\n",
       "27             Positive_Anglez_Active_Days          0.010808\n",
       "32        Fitness_Endurance_Total_Time_Sec          0.009825\n",
       "8                       Physical-HeartRate          0.008186\n",
       "17              BIA-BIA_Activity_Level_num          0.007720\n",
       "7                    Physical-Diastolic_BP          0.007447\n",
       "22                       BIA-BIA_Frame_num          0.006813\n",
       "1                          Basic_Demos-Sex          0.004352\n",
       "16                         FGC-FGC_TL_Zone          0.001435\n",
       "12                         FGC-FGC_CU_Zone          0.001110\n",
       "14                         FGC-FGC_PU_Zone          0.000719\n",
       "29                         FGC-FGC_SR_Zone          0.000643\n",
       "31                                PAQ_Zone          0.000000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get feature importance from the rf inside pipe\n",
    "score_mice_df = pd.DataFrame({'feature':train_cleaned[predictors].columns,\n",
    "                            'importance_score': pipe_mice.named_steps['rf'].feature_importances_})\n",
    "\n",
    "score_mice_df.sort_values('importance_score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the feature variable from score_mice_df sorted by importance_score\n",
    "ordered_feature_list = score_mice_df.sort_values('importance_score',ascending=False)['feature'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Basic_Demos-Age',\n",
       " 'Physical-Height',\n",
       " 'PreInt_EduHx-computerinternet_hoursday',\n",
       " 'BIA-BIA_FFM',\n",
       " 'SDS-SDS_Total_Raw',\n",
       " 'Physical-Weight',\n",
       " 'ENMO_Avg_Active_Days_MVPA110',\n",
       " 'FGC-FGC_CU']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_feature_list[0:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Constructing some Linear Models**\n",
    "\n",
    "In this section, I'll make linear models with:\n",
    "* A single predictor (hours spent on the internet)\n",
    "* A small number of predictors (taken from the importance scores generated above)\n",
    "* All the predictors\n",
    "\n",
    "Each of these will be run through a KFold split with a 20% validation set; for each model we'll compute several stats to compare the predictions with PCIAT scores and also with sii scores:\n",
    "* MSE\n",
    "* kappa\n",
    "\n",
    "Note: custom loss functions for linear models are documented here: https://alexmiller.phd/posts/linear-model-custom-loss-function-regularization-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 18\u001b[0m\n\u001b[1;32m     13\u001b[0m num_models \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m     14\u001b[0m kfold \u001b[38;5;241m=\u001b[39m KFold(num_splits,\n\u001b[1;32m     15\u001b[0m               random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m216\u001b[39m,\n\u001b[1;32m     16\u001b[0m               shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 18\u001b[0m a, b \u001b[38;5;241m=\u001b[39m kfold\u001b[38;5;241m.\u001b[39msplit(train_cleaned)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Import statements\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#partial_feature_list = ordered_feature_list[0:8]\n",
    "full_feature_list = predictors\n",
    "\n",
    "## Make a KFold object\n",
    "## remember to set a random_state and set shuffle = True\n",
    "num_splits = 5\n",
    "num_models = 4\n",
    "kfold = KFold(num_splits,\n",
    "              random_state = 216,\n",
    "              shuffle=True)\n",
    "\n",
    "a, b = kfold.split(train_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Basic_Demos-Age', 'Basic_Demos-Sex', 'CGAS-CGAS_Score', 'Physical-BMI',\n",
       "       'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
       "       'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
       "       'Fitness_Endurance-Max_Stage', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone',\n",
       "       'FGC-FGC_PU', 'FGC-FGC_PU_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone',\n",
       "       'BIA-BIA_Activity_Level_num', 'BIA-BIA_FFM', 'BIA-BIA_FFMI',\n",
       "       'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num', 'SDS-SDS_Total_Raw',\n",
       "       'PreInt_EduHx-computerinternet_hoursday',\n",
       "       'ENMO_Avg_Active_Days_MVPA192', 'ENMO_Avg_Active_Days_MVPA110',\n",
       "       'Positive_Anglez_Active_Days', 'FGC-FGC_SR', 'FGC-FGC_SR_Zone',\n",
       "       'PAQ_Total', 'PAQ_Zone', 'Fitness_Endurance_Total_Time_Sec'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tt[predictors].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values in train_tt[predictors]: 16706\n",
      "Number of NaN values in train_ho[predictors]: 5584\n",
      "Number of NaN values in train_cleaned[predictors]: 22290\n",
      "Number of NaN values in train_tt_imputed[predictors]: 6062\n",
      "Number of NaN values in train_ho_imputed[predictors]: 4339\n",
      "Number of NaN values in train_cleaned_imputed[predictors]: 3468\n",
      "Number of NaN values in train_tt_imputed_zoned[predictors]: 4079\n",
      "Number of NaN values in train_ho_imputed_zoned[predictors]: 4085\n",
      "Number of NaN values in train_cleaned_imputed_zoned[predictors]: 0\n",
      "Number of NaN values in train_tt_imputed_zoned[['PreInt_EduHx-computerinternet_hoursday']]: 15\n",
      "Number of NaN values in train_cleaned_imputed_zoned[['PreInt_EduHx-computerinternet_hoursday']]: 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nLinearRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of NaN values in train_cleaned_imputed_zoned[[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPreInt_EduHx-computerinternet_hoursday\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]]:\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_cleaned_imputed_zoned[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPreInt_EduHx-computerinternet_hoursday\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39msum())\n\u001b[1;32m     36\u001b[0m slr \u001b[38;5;241m=\u001b[39m LinearRegression()\n\u001b[0;32m---> 37\u001b[0m \u001b[43mslr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_tt_imputed_zoned\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPreInt_EduHx-computerinternet_hoursday\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_tt_imputed_zoned\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPCIAT-PCIAT_Total\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m mean_squared_error(train_ho_imputed_zoned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPCIAT-PCIAT_Total\u001b[39m\u001b[38;5;124m'\u001b[39m], slr\u001b[38;5;241m.\u001b[39mpredict(train_ho_imputed_zoned[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPreInt_EduHx-computerinternet_hoursday\u001b[39m\u001b[38;5;124m'\u001b[39m]]))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py:609\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    605\u001b[0m n_jobs_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs\n\u001b[1;32m    607\u001b[0m accept_sparse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositive \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoo\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 609\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    618\u001b[0m has_sw \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_sw:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py:1301\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1296\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1298\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1299\u001b[0m     )\n\u001b[0;32m-> 1301\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1303\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1306\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1318\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1320\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py:1064\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1059\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1060\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1061\u001b[0m     )\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m-> 1064\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m   1073\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py:123\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py:172\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    171\u001b[0m     )\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nLinearRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "# generate a test train split with test=20% from train_cleaned\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_tt, train_ho = train_test_split(train_cleaned)\n",
    "\n",
    "print(\"Number of NaN values in train_tt[predictors]:\", train_tt[predictors].isnull().sum().sum())\n",
    "print(\"Number of NaN values in train_ho[predictors]:\", train_ho[predictors].isnull().sum().sum())\n",
    "print(\"Number of NaN values in train_cleaned[predictors]:\", train_cleaned[predictors].isnull().sum().sum())\n",
    "\n",
    "cusmouse = Custom_MICE_Imputer()\n",
    "train_tt_imputed = cusmouse.fit_transform(train_tt)\n",
    "train_ho_imputed = cusmouse.fit_transform(train_ho)\n",
    "train_cleaned_imputed = cusmouse.fit_transform(train_cleaned)\n",
    "\n",
    "print(\"Number of NaN values in train_tt_imputed[predictors]:\", train_tt_imputed[predictors].isnull().sum().sum())\n",
    "print(\"Number of NaN values in train_ho_imputed[predictors]:\", train_ho_imputed[predictors].isnull().sum().sum())\n",
    "print(\"Number of NaN values in train_cleaned_imputed[predictors]:\", train_cleaned_imputed[predictors].isnull().sum().sum())\n",
    "\n",
    "train_tt_imputed_zoned = zone_encoder(train_tt_imputed)\n",
    "train_ho_imputed_zoned = zone_encoder(train_ho_imputed)\n",
    "train_cleaned_imputed_zoned = zone_encoder(train_cleaned_imputed)\n",
    "\n",
    "#Count the total number of NaN values in train_tt_imputed_zoned\n",
    "print(\"Number of NaN values in train_tt_imputed_zoned[predictors]:\", train_tt_imputed_zoned[predictors].isnull().sum().sum())\n",
    "print(\"Number of NaN values in train_ho_imputed_zoned[predictors]:\", train_ho_imputed_zoned[predictors].isnull().sum().sum())\n",
    "print(\"Number of NaN values in train_cleaned_imputed_zoned[predictors]:\", train_cleaned_imputed_zoned[predictors].isnull().sum().sum())\n",
    "\n",
    "## Fit and get ho mse for slr model with one predictor\n",
    "#slr = Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "#            ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "#            ('linear', LinearRegression())])\n",
    "\n",
    "#Compute the number of NaN values in train_tt_imputed_zoned[['PreInt_EduHx-computerinternet_hoursday']]\n",
    "print(\"Number of NaN values in train_tt_imputed_zoned[['PreInt_EduHx-computerinternet_hoursday']]:\", train_tt_imputed_zoned[['PreInt_EduHx-computerinternet_hoursday']].isnull().sum().sum())\n",
    "print(\"Number of NaN values in train_cleaned_imputed_zoned[['PreInt_EduHx-computerinternet_hoursday']]:\", train_cleaned_imputed_zoned[['PreInt_EduHx-computerinternet_hoursday']].isnull().sum().sum())\n",
    "\n",
    "slr = LinearRegression()\n",
    "slr.fit(train_tt_imputed_zoned[['PreInt_EduHx-computerinternet_hoursday']],train_tt_imputed_zoned['PCIAT-PCIAT_Total'])\n",
    "\n",
    "mean_squared_error(train_ho_imputed_zoned['PCIAT-PCIAT_Total'], slr.predict(train_ho_imputed_zoned[['PreInt_EduHx-computerinternet_hoursday']]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split number: 0\n",
      "Number of NaN values in train_tt[predictors]: 17854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/impute/_iterative.py:825: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values in train_tt_imputed[predictors]: 5794\n",
      "Number of NaN values in train_tt_imputed_zoned[predictors]: Basic_Demos-Age                             0\n",
      "Basic_Demos-Sex                             0\n",
      "CGAS-CGAS_Score                            38\n",
      "Physical-BMI                               20\n",
      "Physical-Height                            20\n",
      "Physical-Weight                            20\n",
      "Physical-Waist_Circumference              311\n",
      "Physical-Diastolic_BP                      26\n",
      "Physical-HeartRate                         24\n",
      "Physical-Systolic_BP                       26\n",
      "Fitness_Endurance-Max_Stage               245\n",
      "FGC-FGC_CU                                112\n",
      "FGC-FGC_CU_Zone                           112\n",
      "FGC-FGC_PU                                113\n",
      "FGC-FGC_PU_Zone                           113\n",
      "FGC-FGC_TL                                112\n",
      "FGC-FGC_TL_Zone                           112\n",
      "BIA-BIA_Activity_Level_num                119\n",
      "BIA-BIA_FFM                               120\n",
      "BIA-BIA_FFMI                              121\n",
      "BIA-BIA_FMI                               125\n",
      "BIA-BIA_Fat                               125\n",
      "BIA-BIA_Frame_num                         119\n",
      "SDS-SDS_Total_Raw                          19\n",
      "PreInt_EduHx-computerinternet_hoursday     11\n",
      "ENMO_Avg_Active_Days_MVPA192              229\n",
      "ENMO_Avg_Active_Days_MVPA110              229\n",
      "Positive_Anglez_Active_Days               229\n",
      "FGC-FGC_SR                                112\n",
      "FGC-FGC_SR_Zone                           112\n",
      "PAQ_Total                                 122\n",
      "PAQ_Zone                                  122\n",
      "Fitness_Endurance_Total_Time_Sec          245\n",
      "dtype: int64\n",
      "Number of NaN values in train_tt_imputed_zoned[['PreInt_EduHx-computerinternet_hoursday']]: 11\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nLinearRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of NaN values in train_tt_imputed_zoned[[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPreInt_EduHx-computerinternet_hoursday\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]]:\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_tt_imputed_zoned[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPreInt_EduHx-computerinternet_hoursday\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39msum())\n\u001b[1;32m     58\u001b[0m slr \u001b[38;5;241m=\u001b[39m LinearRegression()\n\u001b[0;32m---> 59\u001b[0m \u001b[43mslr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_tt_imputed_zoned\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPreInt_EduHx-computerinternet_hoursday\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_tt_imputed_zoned\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPCIAT-PCIAT_Total\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m rmses[\u001b[38;5;241m0\u001b[39m, i] \u001b[38;5;241m=\u001b[39m mean_squared_error(train_ho_imputed_zoned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPCIAT-PCIAT_Total\u001b[39m\u001b[38;5;124m'\u001b[39m], slr\u001b[38;5;241m.\u001b[39mpredict(train_ho_imputed_zoned[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPreInt_EduHx-computerinternet_hoursday\u001b[39m\u001b[38;5;124m'\u001b[39m]]))\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m## Fit and get ho mse for mlr model with the partial_feature_list as predictors\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m#mlr_partial = Pipeline([('mice_impute', Custom_MICE_Imputer()),\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m#            ('add_zones', FunctionTransformer(zone_encoder)),\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m#            ('linear', LinearRegression())])\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py:609\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    605\u001b[0m n_jobs_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs\n\u001b[1;32m    607\u001b[0m accept_sparse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositive \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoo\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 609\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    618\u001b[0m has_sw \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_sw:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py:1301\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1296\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1298\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1299\u001b[0m     )\n\u001b[0;32m-> 1301\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1303\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1306\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1318\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1320\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py:1064\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1059\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1060\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1061\u001b[0m     )\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m-> 1064\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m   1073\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py:123\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py:172\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    171\u001b[0m     )\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nLinearRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "# Import statements\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "partial_feature_list = ordered_feature_list[0:8]\n",
    "full_feature_list = predictors\n",
    "\n",
    "## Make a KFold object\n",
    "## remember to set a random_state and set shuffle = True\n",
    "num_splits = 5\n",
    "num_models = 4\n",
    "kfold = KFold(num_splits,\n",
    "              random_state = 216,\n",
    "              shuffle=True)\n",
    "\n",
    "## This array will hold the mse for each model and split\n",
    "mses = np.zeros((num_models, num_splits))\n",
    "\n",
    "## sets a split counter\n",
    "i = 0\n",
    "\n",
    "## loop through the kfold here\n",
    "for train_index, test_index in kfold.split(train_cleaned):\n",
    "    print('split number:', i)\n",
    "    ## cv training set\n",
    "    train_tt = train_cleaned.iloc[train_index]\n",
    "    \n",
    "    ## cv holdout set\n",
    "    train_ho = train_cleaned.iloc[test_index]\n",
    "    \n",
    "    print(\"Number of NaN values in train_tt[predictors]:\", train_tt[predictors].isnull().sum().sum())\n",
    "    #print(\"Number of NaN values in train_ho[predictors]:\", train_ho[predictors].isnull().sum().sum())\n",
    "\n",
    "    cusmouse = Custom_MICE_Imputer()\n",
    "    train_tt_imputed = cusmouse.fit_transform(train_tt)\n",
    "    train_ho_imputed = cusmouse.fit_transform(train_ho)\n",
    "\n",
    "    print(\"Number of NaN values in train_tt_imputed[predictors]:\", train_tt_imputed[predictors].isnull().sum().sum())\n",
    "    #print(\"Number of NaN values in train_ho_imputed[predictors]:\", train_ho_imputed[predictors].isnull().sum().sum())\n",
    "\n",
    "    train_tt_imputed_zoned = zone_encoder(train_tt_imputed)\n",
    "    train_ho_imputed_zoned = zone_encoder(train_ho_imputed)\n",
    "\n",
    "    #Count the total number of NaN values in train_tt_imputed_zoned\n",
    "    print(\"Number of NaN values in train_tt_imputed_zoned[predictors]:\", train_tt_imputed_zoned[predictors].isnull().sum())\n",
    "    #print(\"Number of NaN values in train_ho_imputed_zoned[predictors]:\", train_ho_imputed_zoned[predictors].isnull().sum().sum())\n",
    "\n",
    "    ## Fit and get ho mse for slr model with one predictor\n",
    "    #slr = Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "    #            ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "    #            ('linear', LinearRegression())])\n",
    "    \n",
    "    #Compute the number of NaN values in train_tt_imputed_zoned[['PreInt_EduHx-computerinternet_hoursday']]\n",
    "    print(\"Number of NaN values in train_tt_imputed_zoned[['PreInt_EduHx-computerinternet_hoursday']]:\", train_tt_imputed_zoned[['PreInt_EduHx-computerinternet_hoursday']].isnull().sum().sum())\n",
    "\n",
    "    slr = LinearRegression()\n",
    "    slr.fit(train_tt_imputed_zoned[['PreInt_EduHx-computerinternet_hoursday']],train_tt_imputed_zoned['PCIAT-PCIAT_Total'])\n",
    "    \n",
    "    rmses[0, i] = mean_squared_error(train_ho_imputed_zoned['PCIAT-PCIAT_Total'], slr.predict(train_ho_imputed_zoned[['PreInt_EduHx-computerinternet_hoursday']]))\n",
    "    \n",
    "    ## Fit and get ho mse for mlr model with the partial_feature_list as predictors\n",
    "    #mlr_partial = Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "    #            ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "    #            ('linear', LinearRegression())])\n",
    "    \n",
    "    mlr_partial = LinearRegression()\n",
    "    mlr_partial.fit(train_tt_imputed_zoned[partial_feature_list],train_tt_imputed_zoned['PCIAT-PCIAT_Total'])\n",
    "    \n",
    "    rmses[1, i] = mean_squared_error(train_ho_imputed_zoned['PCIAT-PCIAT_Total'], mlr_partial.predict(train_ho_imputed_zoned[partial_feature_list]))\n",
    "    \n",
    "    ## Fit and get ho mse for mlr model with the partial_feature_list as predictors\n",
    "    mlr_full = Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "                ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                ('linear', LinearRegression())])\n",
    "    \n",
    "    mlr_full.fit(train_tt[full_feature_list],train_tt['PCIAT-PCIAT_Total'])\n",
    "    \n",
    "    rmses[2, i] = mean_squared_error(train_ho['PCIAT-PCIAT_Total'], mlr_full.predict(train_ho[full_feature_list]))\n",
    "\n",
    "    ## Fit and get ho mse for the knn model\n",
    "    knn = Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "            ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "            ('scale', StandardScaler()),\n",
    "            ('knn', KNeighborsRegressor(10))])\n",
    "        \n",
    "    knn.fit(train_tt[full_feature_list],train_tt['PCIAT-PCIAT_Total'])\n",
    "    \n",
    "    rmses[3, i] = mean_squared_error(train_ho['PCIAT-PCIAT_Total'], knn.predict(train_ho[full_feature_list]))\n",
    "    \n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(models, pred_data, out_data, iterations = 100):\n",
    "    results = {}\n",
    "    for i in models:\n",
    "        mse_train = []\n",
    "        mse_test = []\n",
    "        for j in range(iterations):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(pred_data, \n",
    "                                                                out_data, \n",
    "                                                                test_size= 0.2)\n",
    "            mse_test.append(metrics.mean_squared_error(y_test,\n",
    "                                            models[i].fit(X_train, \n",
    "                                                         y_train).predict(X_test)))\n",
    "            mse_train.append(metrics.mean_squared_error(y_train, \n",
    "                                             models[i].fit(X_train, \n",
    "                                                          y_train).predict(X_train)))\n",
    "        results[i] = [np.mean(mse_train), np.mean(mse_test)]\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Construct the pipes\n",
    "pipe_linear = Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "                    ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                    ('linear', LinearRegression())])\n",
    "\n",
    "\n",
    "#Iterate through models?\n",
    "\n",
    "models = {'OLS': linear_model.LinearRegression(),\n",
    "           'Lasso': GridSearchCV(linear_model.Lasso(), \n",
    "                               param_grid=lasso_params).fit(df[X], df[Y]).best_estimator_,\n",
    "           'Ridge': GridSearchCV(linear_model.Ridge(), \n",
    "                               param_grid=ridge_params).fit(df[X], df[Y]).best_estimator_,}\n",
    "\n",
    "test(models, train_cleaned[predictors], train_cleaned['PCIAT-PCIAT_Total'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sequential Binary Classification**\n",
    "\n",
    "It looks like our attempts so far have under-predicted sii values of 2 and 3. I'm going to try to implement a method that first predicts whether or not the sii value is 3, then on the remaining values predict whether or not they are 2, etc.\n",
    "\n",
    "I came up with this idea myself, but I wasn't the first one to do it. It was described on Medium: https://towardsdatascience.com/simple-trick-to-train-an-ordinal-regression-with-any-classifier-6911183d2a3c from an article by Frank and Hal\n",
    "\n",
    "Also described on stackoverflow: https://stackoverflow.com/questions/57561189/multi-class-multi-label-ordinal-classification-with-sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class OrdinalClassifier(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, clf):\n",
    "        self.clf = clf\n",
    "        self.clfs = {}\n",
    "        self.unique_class = np.NaN\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.unique_class = np.sort(np.unique(y))\n",
    "        if self.unique_class.shape[0] > 2:\n",
    "            for i in range(self.unique_class.shape[0]-1):\n",
    "                # for each k - 1 ordinal value we fit a binary classification problem\n",
    "                binary_y = (y > self.unique_class[i]).astype(np.uint8)\n",
    "                clf = clone(self.clf)\n",
    "                clf.fit(X, binary_y)\n",
    "                self.clfs[i] = clf\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        clfs_predict = {i: self.clfs[i].predict_proba(X) for i in self.clfs}\n",
    "        predicted = []\n",
    "        k = len(self.unique_class) - 1\n",
    "        for i, y in enumerate(self.unique_class):\n",
    "            if i == 0:\n",
    "                # V1 = 1 - Pr(y > V1)\n",
    "                predicted.append(1 - clfs_predict[0][:,1])\n",
    "            elif i < k:\n",
    "                # Vi = Pr(y <= Vi) * Pr(y > Vi-1)\n",
    "                 predicted.append((1 - clfs_predict[i][:,1]) * clfs_predict[i-1][:,1])\n",
    "            else:\n",
    "                # Vk = Pr(y > Vk-1)\n",
    "                predicted.append(clfs_predict[k-1][:,1])\n",
    "        return np.vstack(predicted).T\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.unique_class[np.argmax(self.predict_proba(X), axis=1)]\n",
    "\n",
    "    def score(self, X, y, sample_weight=None):\n",
    "        return accuracy_score(y, self.predict(X), sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using LASSO for Feature Selection**\n",
    "\n",
    "First, we'll try using LASSO to identify important features.\n",
    "\n",
    "Note that it isn't possible to use LASSO with pipelines (see https://stackoverflow.com/questions/39466671/use-of-scaler-with-lassocv-ridgecv). So we'll need to do the hyperparameter tuning manually.\n",
    "\n",
    "Some of the code below was suggested by Ali Furkan Kalay: https://alfurka.github.io/2018-11-18-grid-search/\n",
    "\n",
    "Some of the code below was suggested on Medium: https://medium.com/geekculture/regularization-using-pipeline-gridsearchcv-f377946e39d1\n",
    "\n",
    "Some of the code below was suggested on geeksforgeeks (https://www.geeksforgeeks.org/feature-selection-using-selectfrommodel-and-lassocv-in-scikit-learn/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tuning Lasso inside a Pipe with GridSearchCV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
      "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 455, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1004, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 848, in score\n",
      "    y_pred = self.predict(X)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 306, in predict\n",
      "    return self._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py\", line 1147, in _decision_function\n",
      "    return super()._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 285, in _decision_function\n",
      "    X = self._validate_data(X, accept_sparse=[\"csr\", \"csc\", \"coo\"], reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 633, in _validate_data\n",
      "    out = check_array(X, input_name=\"X\", **check_params)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1064, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains NaN.\n",
      "Lasso does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
      "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 455, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1004, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 848, in score\n",
      "    y_pred = self.predict(X)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 306, in predict\n",
      "    return self._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py\", line 1147, in _decision_function\n",
      "    return super()._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 285, in _decision_function\n",
      "    X = self._validate_data(X, accept_sparse=[\"csr\", \"csc\", \"coo\"], reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 633, in _validate_data\n",
      "    out = check_array(X, input_name=\"X\", **check_params)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1064, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains NaN.\n",
      "Lasso does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
      "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 455, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1004, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 848, in score\n",
      "    y_pred = self.predict(X)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 306, in predict\n",
      "    return self._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py\", line 1147, in _decision_function\n",
      "    return super()._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 285, in _decision_function\n",
      "    X = self._validate_data(X, accept_sparse=[\"csr\", \"csc\", \"coo\"], reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 633, in _validate_data\n",
      "    out = check_array(X, input_name=\"X\", **check_params)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1064, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains NaN.\n",
      "Lasso does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
      "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 455, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1004, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 848, in score\n",
      "    y_pred = self.predict(X)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 306, in predict\n",
      "    return self._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py\", line 1147, in _decision_function\n",
      "    return super()._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 285, in _decision_function\n",
      "    X = self._validate_data(X, accept_sparse=[\"csr\", \"csc\", \"coo\"], reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 633, in _validate_data\n",
      "    out = check_array(X, input_name=\"X\", **check_params)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1064, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains NaN.\n",
      "Lasso does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
      "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 455, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1004, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 848, in score\n",
      "    y_pred = self.predict(X)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 306, in predict\n",
      "    return self._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py\", line 1147, in _decision_function\n",
      "    return super()._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 285, in _decision_function\n",
      "    X = self._validate_data(X, accept_sparse=[\"csr\", \"csc\", \"coo\"], reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 633, in _validate_data\n",
      "    out = check_array(X, input_name=\"X\", **check_params)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1064, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains NaN.\n",
      "Lasso does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
      "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 455, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1004, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 848, in score\n",
      "    y_pred = self.predict(X)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 306, in predict\n",
      "    return self._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py\", line 1147, in _decision_function\n",
      "    return super()._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 285, in _decision_function\n",
      "    X = self._validate_data(X, accept_sparse=[\"csr\", \"csc\", \"coo\"], reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 633, in _validate_data\n",
      "    out = check_array(X, input_name=\"X\", **check_params)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1064, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains NaN.\n",
      "Lasso does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
      "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 455, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1004, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 848, in score\n",
      "    y_pred = self.predict(X)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 306, in predict\n",
      "    return self._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py\", line 1147, in _decision_function\n",
      "    return super()._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 285, in _decision_function\n",
      "    X = self._validate_data(X, accept_sparse=[\"csr\", \"csc\", \"coo\"], reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 633, in _validate_data\n",
      "    out = check_array(X, input_name=\"X\", **check_params)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1064, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains NaN.\n",
      "Lasso does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
      "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 455, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1004, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 848, in score\n",
      "    y_pred = self.predict(X)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 306, in predict\n",
      "    return self._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py\", line 1147, in _decision_function\n",
      "    return super()._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 285, in _decision_function\n",
      "    X = self._validate_data(X, accept_sparse=[\"csr\", \"csc\", \"coo\"], reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 633, in _validate_data\n",
      "    out = check_array(X, input_name=\"X\", **check_params)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1064, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains NaN.\n",
      "Lasso does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.441e+04, tolerance: 4.500e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
      "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 455, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1004, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 848, in score\n",
      "    y_pred = self.predict(X)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 306, in predict\n",
      "    return self._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py\", line 1147, in _decision_function\n",
      "    return super()._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 285, in _decision_function\n",
      "    X = self._validate_data(X, accept_sparse=[\"csr\", \"csc\", \"coo\"], reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 633, in _validate_data\n",
      "    out = check_array(X, input_name=\"X\", **check_params)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1064, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains NaN.\n",
      "Lasso does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.493e+05, tolerance: 4.500e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
      "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 455, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1004, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 848, in score\n",
      "    y_pred = self.predict(X)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 306, in predict\n",
      "    return self._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py\", line 1147, in _decision_function\n",
      "    return super()._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 285, in _decision_function\n",
      "    X = self._validate_data(X, accept_sparse=[\"csr\", \"csc\", \"coo\"], reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 633, in _validate_data\n",
      "    out = check_array(X, input_name=\"X\", **check_params)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1064, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains NaN.\n",
      "Lasso does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
      "10 fits failed out of a total of 20.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py\", line 980, in fit\n",
      "    X, y = self._validate_data(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 650, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1301, in check_X_y\n",
      "    X = check_array(\n",
      "        ^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1064, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains NaN.\n",
      "Lasso does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1103: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lasso__alpha': np.float64(5000000000.0)}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "  \n",
    "# Create a list of predictor variables; this eliminates id, sii, PCIAT, and Season variables\n",
    "predictors = train_cleaned.columns.tolist()\n",
    "if 'id' in predictors:\n",
    "    predictors.remove('id')\n",
    "if 'sii' in predictors:\n",
    "    predictors.remove('sii')\n",
    "predictors = [x for x in predictors if 'PCIAT' not in x]\n",
    "predictors = [x for x in predictors if 'Season' not in x]\n",
    "\n",
    "# A list of alpha (lambda) values to try in the hyperparameter tuning\n",
    "# create an array of 10**np.linspace(10,-2,100)*0.5\n",
    "#alphas = {'lasso__alpha': 10**np.linspace(10,-2,100)*0.5}\n",
    "alphas = {'lasso__alpha': 10**np.linspace(10,-2,10)*0.5}\n",
    "\n",
    "# Set up a lasso pipeline\n",
    "lasso_pipe = Pipeline([('impute', Custom_MICE_Imputer()),('fillzones', FunctionTransformer(zone_encoder)), ('lasso', Lasso())])\n",
    "\n",
    "gs_lasso_pipe = GridSearchCV(lasso_pipe, param_grid=alphas, cv=2).fit(train_cleaned[predictors], train_cleaned['PCIAT-PCIAT_Total'])\n",
    "\n",
    "gs_lasso_pipe.best_estimator_\n",
    "gs_lasso_pipe.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.feature_selection import SelectFromModel \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "  \n",
    "# Create a list of predictor variables; this eliminates id, sii, PCIAT, and Season variables\n",
    "predictors = train_cleaned.columns.tolist()\n",
    "if 'id' in predictors:\n",
    "    predictors.remove('id')\n",
    "if 'sii' in predictors:\n",
    "    predictors.remove('sii')\n",
    "predictors = [x for x in predictors if 'PCIAT' not in x]\n",
    "predictors = [x for x in predictors if 'Season' not in x]\n",
    "\n",
    "# Split the data into 80% Train/20% Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_cleaned[predictors], train_cleaned['PCIAT-PCIAT_Total'], test_size=0.2, random_state=216)\n",
    "\n",
    "# A list of alpha (lambda) values to try in the hyperparameter tuning\n",
    "alphas = 10**np.linspace(10,-2,100)*0.5\n",
    "\n",
    "# These will hold our coefficient estimates\n",
    "lasso_coefs = np.empty((len(alpha),n))\n",
    "\n",
    "# Set up a lasso pipeline\n",
    "lasso_pipe = Pipeline([('impute', Custom_MICE_Imputer()),('fillzones', FunctionTransformer(zone_encoder)), ('lasso', Lasso())])\n",
    "\n",
    "GridSearchCV(lasso_pipe, param_grid=alphas).fit(train_cleaned[predictors], train_cleaned['PCIAT-PCIAT_Total']).best_estimator_,\n",
    "\n",
    "def test(models, data, iterations = 100):\n",
    "    results = {}\n",
    "    for i in models:\n",
    "        r2_train = []\n",
    "        r2_test = []\n",
    "        for j in range(iterations):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(data[X], \n",
    "                                                                data[Y], \n",
    "                                                                test_size= 0.2)\n",
    "            r2_test.append(metrics.r2_score(y_test,\n",
    "                                            models[i].fit(X_train, \n",
    "                                                         y_train).predict(X_test)))\n",
    "            r2_train.append(metrics.r2_score(y_train, \n",
    "                                             models[i].fit(X_train, \n",
    "                                                          y_train).predict(X_train)))\n",
    "        results[i] = [np.mean(r2_train), np.mean(r2_test)]\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "models = {'OLS': linear_model.LinearRegression(),\n",
    "           'Lasso': GridSearchCV(linear_model.Lasso(), \n",
    "                               param_grid=lasso_params).fit(df[X], df[Y]).best_estimator_,\n",
    "           'Ridge': GridSearchCV(linear_model.Ridge(), \n",
    "                               param_grid=ridge_params).fit(df[X], df[Y]).best_estimator_,}\n",
    "\n",
    "test(models, df)\n",
    "\n",
    "## for each alpha value\n",
    "for i in range(len(alpha)):\n",
    "    ## set up the lasso pipeline\n",
    "    ## first scale\n",
    "    ## then make polynomial features\n",
    "    ## then fit the lasso regression model\n",
    "    lasso_pipe = Pipeline([('scale',StandardScaler()),\n",
    "                              ('poly',PolynomialFeatures(n, interaction_only=False, include_bias=False)),\n",
    "                              ('lasso', Lasso(alpha=alpha[i], max_iter=5000000))\n",
    "                          ])\n",
    "    \n",
    "    ## fit the lasso\n",
    "    lasso_pipe.fit(x.reshape(-1,1), y)\n",
    "\n",
    "    # record the coefficients\n",
    "    lasso_coefs[i,:] = lasso_pipe['lasso'].coef_\n",
    "\n",
    "\n",
    "# Fit LassoCV model with 5-fold cross-validation. It automatically evaluates performance over several folds in order to get the ideal regularization strength (alpha).\n",
    "lasso_cv = LassoCV(cv=5) \n",
    "lasso_cv.fit(X_train, y_train) \n",
    "\n",
    "# Feature selection. This selects the most significant features from the training and testing sets using the pre-trained lasso_cv model. \n",
    "# Only the features determined to be relevant by the L1 regularization are included in the final selected feature sets\n",
    "# These final selected feature sets are stored in X_train_selected and X_test_selected\n",
    "sfm = SelectFromModel(lasso_cv, prefit=True) \n",
    "X_train_selected = sfm.transform(X_train) \n",
    "X_test_selected = sfm.transform(X_test) \n",
    "\n",
    "# Train a Random Forest Classifier using the selected features \n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42) \n",
    "model.fit(X_train_selected, y_train) \n",
    "\n",
    "\n",
    "# Evaluate the model \n",
    "y_pred = model.predict(X_test_selected) \n",
    "print(classification_report(y_test, y_pred)) \n",
    "\n",
    "# Analyze selected features and their importance \n",
    "selected_feature_indices = np.where(sfm.get_support())[0] \n",
    "selected_features = train.columns[selected_feature_indices] \n",
    "coefficients = lasso_cv.coef_ \n",
    "print(\"Selected Features:\", selected_features) \n",
    "print(\"Feature Coefficients:\", coefficients) \n",
    "\n",
    "# Extract the selected features from the original dataset \n",
    "X_selected_features = X_train[:, selected_feature_indices] \n",
    "\n",
    "# Create a DataFrame for better visualization \n",
    "selected_features_df = pd.DataFrame(X_selected_features, columns=selected_features) \n",
    "\n",
    "# Add the target variable for coloring \n",
    "selected_features_df['target'] = y_train \n",
    "\n",
    "# Plot the two most important features \n",
    "sns.scatterplot(x='mean area', y='worst area', hue='target', data=selected_features_df, palette='viridis') \n",
    "plt.xlabel('Mean Area') \n",
    "plt.ylabel('Worst Area') \n",
    "plt.title('Scatter Plot of Two Most Important Features') \n",
    "plt.show() \n",
    "\n",
    "\n",
    "\n",
    "## This code will allow us to demonstrate the effect of \n",
    "## increasing alpha\n",
    "\n",
    "## set values for alpha\n",
    "alpha = [0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000]\n",
    "\n",
    "## The degree of the polynomial we will fit\n",
    "n=10\n",
    "\n",
    "#$ These will hold our coefficient estimates\n",
    "ridge_coefs = np.empty((len(alpha),n))\n",
    "lasso_coefs = np.empty((len(alpha),n))\n",
    "\n",
    "## for each alpha value\n",
    "for i in range(len(alpha)):\n",
    "    ## set up the lasso pipeline\n",
    "    ## first scale\n",
    "    ## then make polynomial features\n",
    "    ## then fit the lasso regression model\n",
    "    lasso_pipe = Pipeline([('scale',StandardScaler()),\n",
    "                              ('poly',PolynomialFeatures(n, interaction_only=False, include_bias=False)),\n",
    "                              ('lasso', Lasso(alpha=alpha[i], max_iter=5000000))\n",
    "                          ])\n",
    "    \n",
    "    ## fit the lasso\n",
    "    lasso_pipe.fit(x.reshape(-1,1), y)\n",
    "\n",
    "    # record the coefficients\n",
    "    lasso_coefs[i,:] = lasso_pipe['lasso'].coef_\n",
    "\n",
    "\n",
    "# A data frame to store the optimal alpha values\n",
    "bestalphas = pd.DataFrame(index=range(0,len(listofdatasets)))\n",
    "bestalphas['dfname'] = ''\n",
    "bestalphas['best_alpha_manual'] = np.nan\n",
    "bestalphas['best_alpha_automatic'] = np.nan\n",
    "\n",
    "\n",
    "for df in listofdatasets:\n",
    "    X_train = df.drop(columns=['PCIAT-PCIAT_Total'])\n",
    "    y_train = df['PCIAT-PCIAT_Total']\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_std = scaler.transform(X_train)\n",
    "    lassocv = LassoCV(alphas = alphas, scoring = 'neg_root_mean_squared_error')\n",
    "    lassocv.fit(X_std, y_train)\n",
    "    bestalphas.loc[bestalphas['dfname']==df.name,'best_alpha_automatic']=lassocv.alpha_.astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a Pipeline with the Custom Imputer and Transformer**\n",
    "\n",
    "Below is some code that is based on the 2_More_Advanced_Pipelines notebook from optional_extra_practice in Week 3\n",
    "\n",
    "In that code, their desired pipeline was:\n",
    "1 Impute the missing values of `body_mass_g` with the `median` value,\n",
    "2 Impute the missing values of `sex` with the most common value,\n",
    "3 One hot encode `island` and `sex` and\n",
    "4 Fit a random forest model to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "\n",
    "predictors = train_cleaned.columns.tolist()\n",
    "if 'id' in predictors:\n",
    "    predictors.remove('id')\n",
    "if 'sii' in predictors:\n",
    "    predictors.remove('sii')\n",
    "predictors = [x for x in predictors if 'PCIAT' not in x]\n",
    "predictors = [x for x in predictors if 'Season' not in x]\n",
    "\n",
    "\n",
    "pipe_knn = Pipeline([('knn_impute', Custom_KNN_Imputer()),\n",
    "                    ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                    ('rf', RandomForestRegressor(n_estimators = 300, max_features = 'sqrt', max_depth = 5, random_state = 216))])\n",
    "\n",
    "pipe_knn.fit(train_cleaned[predictors],train_cleaned['PCIAT-PCIAT_Total'])\n",
    "\n",
    "train_pred_knn = pipe_knn.predict(train_cleaned[predictors])\n",
    "\n",
    "\n",
    "\n",
    "pipe_mice = Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "                    ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                    ('rf', RandomForestRegressor(n_estimators = 300, max_features = 'sqrt', max_depth = 5, random_state = 216))])\n",
    "\n",
    "pipe_mice.fit(train_cleaned[predictors],train_cleaned['PCIAT-PCIAT_Total'])\n",
    "\n",
    "train_pred_mice = pipe_mice.predict(train_cleaned[predictors])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get feature importance from the rf inside pipe\n",
    "score_knn_df = pd.DataFrame({'feature':train_cleaned[predictors].columns,\n",
    "                            'importance_score': pipe_knn.named_steps['rf'].feature_importances_})\n",
    "\n",
    "score_knn_df.sort_values('importance_score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get feature importance from the rf inside pipe\n",
    "score_mice_df = pd.DataFrame({'feature':train_cleaned[predictors].columns,\n",
    "                            'importance_score': pipe_mice.named_steps['rf'].feature_importances_})\n",
    "\n",
    "score_mice_df.sort_values('importance_score',ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_fall_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
