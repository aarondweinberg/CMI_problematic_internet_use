{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goals**\n",
    "\n",
    "1. Set up a pipeline to incorporate the imputation\n",
    "2. Do a random forest regressor to identify important features\n",
    "3. Do a test run with one model (linear, most likely) that computes:\n",
    "    - MSE for predicting PCIAT-Total\n",
    "    - MSE for predicting sii when computed from predicted PCIAT-Total\n",
    "    - MSE for predicting sii directly\n",
    "    - kappa for predicting sii when computed from predicted PCIAT-Total\n",
    "    - kappa for predicting sii directly\n",
    "4. After getting the model working, measure these things for out-of-the box:\n",
    "    - multiple linear regression\n",
    "    - knn regression\n",
    "    - random forest\n",
    "    - support vector\n",
    "    - gradient boost\n",
    "    - adaboost\n",
    "    - xgboost\n",
    "5. After identifying a promising out-of-the-box model, try tuning it\n",
    "6. Try implementing a sequential predictor (either logistic regression or random forest) that:\n",
    "    - Starts by predicting 3's vs. non-threes\n",
    "    - Predicts 2's vs. non-twos from the remaining cases\n",
    "    - etc.\n",
    "7. Try using different models for doing this sequential prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from CustomImputers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the Data**\n",
    "\n",
    "For the purpose of developing our model(s), we'll work with data that include the imputed outcome (PCIAT_Total and/or sii) scores AND have cleaned predictors.\n",
    "\n",
    "In the final version of our code, we'll work with data with cleaned predictors but won't have any access to the outcome scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the cleaned & outcome-imputed data\n",
    "train_cleaned=pd.read_csv('train_cleaned_outcome_imputed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an initial list of predictor and outcome columns\n",
    "\n",
    "predictors = train_cleaned.columns.tolist()\n",
    "if 'id' in predictors:\n",
    "    predictors.remove('id')\n",
    "if 'sii' in predictors:\n",
    "    predictors.remove('sii')\n",
    "predictors = [x for x in predictors if 'PCIAT' not in x]\n",
    "predictors = [x for x in predictors if 'Season' not in x]\n",
    "\n",
    "outcome_pciat = ['PCIAT-PCIAT_Total']\n",
    "outcome_sii = ['sii']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Constructing a Random Forest for Feature Identification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Basic_Demos-Age</td>\n",
       "      <td>0.137804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Physical-Height</td>\n",
       "      <td>0.126698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>PreInt_EduHx-computerinternet_hoursday</td>\n",
       "      <td>0.118666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>BIA-BIA_FFM</td>\n",
       "      <td>0.077628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>SDS-SDS_Total_Raw</td>\n",
       "      <td>0.074039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Physical-Weight</td>\n",
       "      <td>0.072494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ENMO_Avg_Active_Days_MVPA110</td>\n",
       "      <td>0.065296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>FGC-FGC_CU</td>\n",
       "      <td>0.055829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>BIA-BIA_FFMI</td>\n",
       "      <td>0.023911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>FGC-FGC_PU</td>\n",
       "      <td>0.023766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>BIA-BIA_Fat</td>\n",
       "      <td>0.023610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Physical-BMI</td>\n",
       "      <td>0.018582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>PAQ_Total</td>\n",
       "      <td>0.018188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>FGC-FGC_TL</td>\n",
       "      <td>0.015412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Fitness_Endurance-Max_Stage</td>\n",
       "      <td>0.014949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Physical-Waist_Circumference</td>\n",
       "      <td>0.013865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>FGC-FGC_SR</td>\n",
       "      <td>0.013734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CGAS-CGAS_Score</td>\n",
       "      <td>0.012663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>BIA-BIA_FMI</td>\n",
       "      <td>0.011455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ENMO_Avg_Active_Days_MVPA192</td>\n",
       "      <td>0.011191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Physical-Systolic_BP</td>\n",
       "      <td>0.011164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Positive_Anglez_Active_Days</td>\n",
       "      <td>0.010808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Fitness_Endurance_Total_Time_Sec</td>\n",
       "      <td>0.009825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Physical-HeartRate</td>\n",
       "      <td>0.008186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>BIA-BIA_Activity_Level_num</td>\n",
       "      <td>0.007720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Physical-Diastolic_BP</td>\n",
       "      <td>0.007447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>BIA-BIA_Frame_num</td>\n",
       "      <td>0.006813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Basic_Demos-Sex</td>\n",
       "      <td>0.004352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>FGC-FGC_TL_Zone</td>\n",
       "      <td>0.001435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>FGC-FGC_CU_Zone</td>\n",
       "      <td>0.001110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>FGC-FGC_PU_Zone</td>\n",
       "      <td>0.000719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>FGC-FGC_SR_Zone</td>\n",
       "      <td>0.000643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>PAQ_Zone</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   feature  importance_score\n",
       "0                          Basic_Demos-Age          0.137804\n",
       "4                          Physical-Height          0.126698\n",
       "24  PreInt_EduHx-computerinternet_hoursday          0.118666\n",
       "18                             BIA-BIA_FFM          0.077628\n",
       "23                       SDS-SDS_Total_Raw          0.074039\n",
       "5                          Physical-Weight          0.072494\n",
       "26            ENMO_Avg_Active_Days_MVPA110          0.065296\n",
       "11                              FGC-FGC_CU          0.055829\n",
       "19                            BIA-BIA_FFMI          0.023911\n",
       "13                              FGC-FGC_PU          0.023766\n",
       "21                             BIA-BIA_Fat          0.023610\n",
       "3                             Physical-BMI          0.018582\n",
       "30                               PAQ_Total          0.018188\n",
       "15                              FGC-FGC_TL          0.015412\n",
       "10             Fitness_Endurance-Max_Stage          0.014949\n",
       "6             Physical-Waist_Circumference          0.013865\n",
       "28                              FGC-FGC_SR          0.013734\n",
       "2                          CGAS-CGAS_Score          0.012663\n",
       "20                             BIA-BIA_FMI          0.011455\n",
       "25            ENMO_Avg_Active_Days_MVPA192          0.011191\n",
       "9                     Physical-Systolic_BP          0.011164\n",
       "27             Positive_Anglez_Active_Days          0.010808\n",
       "32        Fitness_Endurance_Total_Time_Sec          0.009825\n",
       "8                       Physical-HeartRate          0.008186\n",
       "17              BIA-BIA_Activity_Level_num          0.007720\n",
       "7                    Physical-Diastolic_BP          0.007447\n",
       "22                       BIA-BIA_Frame_num          0.006813\n",
       "1                          Basic_Demos-Sex          0.004352\n",
       "16                         FGC-FGC_TL_Zone          0.001435\n",
       "12                         FGC-FGC_CU_Zone          0.001110\n",
       "14                         FGC-FGC_PU_Zone          0.000719\n",
       "29                         FGC-FGC_SR_Zone          0.000643\n",
       "31                                PAQ_Zone          0.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "\n",
    "pipe_mice = Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "                    ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                    ('rf', RandomForestRegressor(n_estimators = 300, max_features = 'sqrt', max_depth = 5, random_state = 216))])\n",
    "\n",
    "pipe_mice.fit(train_cleaned[predictors],train_cleaned['PCIAT-PCIAT_Total'])\n",
    "\n",
    "train_pred_mice = pipe_mice.predict(train_cleaned[predictors])\n",
    "\n",
    "#Get feature importance from the rf inside pipe\n",
    "score_mice_df = pd.DataFrame({'feature':train_cleaned[predictors].columns,\n",
    "                            'importance_score': pipe_mice.named_steps['rf'].feature_importances_})\n",
    "\n",
    "score_mice_df.sort_values('importance_score',ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyfeatures = ['Basic_Demos-Age',\n",
    " 'Physical-Height',\n",
    " 'PreInt_EduHx-computerinternet_hoursday',\n",
    " 'BIA-BIA_FFM',\n",
    " 'SDS-SDS_Total_Raw',\n",
    " 'Physical-Weight',\n",
    " 'ENMO_Avg_Active_Days_MVPA110',\n",
    " 'FGC-FGC_CU']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Constructing some Linear Models**\n",
    "\n",
    "In this section, I'll make linear models with:\n",
    "* A single predictor (hours spent on the internet)\n",
    "* A small number of predictors (taken from the importance scores generated above)\n",
    "* All the predictors\n",
    "\n",
    "Each of these will be run through a KFold split with a 20% validation set; for each model we'll compute several stats to compare the predictions with PCIAT scores and also with sii scores:\n",
    "* MSE\n",
    "* kappa\n",
    "\n",
    "Note: Column selector documented here: https://stackoverflow.com/questions/62416223/how-to-select-only-few-columns-in-scikit-learn-column-selector-pipeline\n",
    "\n",
    "Note: custom loss functions for linear models are documented here: https://alexmiller.phd/posts/linear-model-custom-loss-function-regularization-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(334.7045879129308)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First I'll see if I can get a pipe set up to do prediction on a split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "train_tt, train_ho = train_test_split(train_cleaned, test_size=0.2)\n",
    "\n",
    "slr = Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "                ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                ('selector', ColumnTransformer([('selector', 'passthrough', ['PreInt_EduHx-computerinternet_hoursday'])], remainder=\"drop\")),\n",
    "                ('linear', LinearRegression())])\n",
    "\n",
    "slr.fit(train_tt[predictors], train_tt['PCIAT-PCIAT_Total'])\n",
    "mean_squared_error(train_ho['PCIAT-PCIAT_Total'], slr.predict(train_ho))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse for {'slr_pipe'}  for predicting PCIAT: 383.11066807240763\n",
      "mse for {'mlr_key_pipe'}  for predicting PCIAT: 325.23402248777586\n",
      "mse for {'mlr_all_pipe'}  for predicting PCIAT: 331.75078129397434\n",
      "mse for {'knn_pipe'}  for predicting PCIAT: 361.39609293849657\n",
      "mse for {'svr_pipe'}  for predicting PCIAT: 391.7747439954416\n",
      "mse for {'rf_pipe'}  for predicting PCIAT: 332.2653856218679\n",
      "mse for {'ada_pipe'}  for predicting PCIAT: 333.10566180398655\n",
      "mse for {'grad_pipe'}  for predicting PCIAT: 322.78059556576756\n",
      "mse for {'xgb_pipe'}  for predicting PCIAT: 401.81210256552765\n",
      "mse for {'slr_pipe'}  for predicting sii: 0.5851009007945649\n",
      "kappa for {'slr_pipe'}  for predicting sii with regular rounding: 0.23091724728168783\n",
      "kappa for {'slr_pipe'}  for predicting sii with regular rounding up: 0.12012579641186916\n",
      "mse for {'mlr_key_pipe'}  for predicting sii: 0.4983862534531409\n",
      "kappa for {'mlr_key_pipe'}  for predicting sii with regular rounding: 0.40109140518417463\n",
      "kappa for {'mlr_key_pipe'}  for predicting sii with regular rounding up: 0.20700031144438324\n",
      "mse for {'mlr_all_pipe'}  for predicting sii: 0.5059152479871977\n",
      "kappa for {'mlr_all_pipe'}  for predicting sii with regular rounding: 0.3652773071275274\n",
      "kappa for {'mlr_all_pipe'}  for predicting sii with regular rounding up: 0.2610233625582642\n",
      "mse for {'knn_pipe'}  for predicting sii: 0.5520501138952164\n",
      "kappa for {'knn_pipe'}  for predicting sii with regular rounding: 0.33405041412737246\n",
      "kappa for {'knn_pipe'}  for predicting sii with regular rounding up: 0.14560299894728734\n",
      "mse for {'svr_pipe'}  for predicting sii: 0.5781012803797273\n",
      "kappa for {'svr_pipe'}  for predicting sii with regular rounding: 0.25723923140754157\n",
      "kappa for {'svr_pipe'}  for predicting sii with regular rounding up: 0.19909295236570146\n",
      "mse for {'rf_pipe'}  for predicting sii: 0.4969785876993167\n",
      "kappa for {'rf_pipe'}  for predicting sii with regular rounding: 0.38208818426824076\n",
      "kappa for {'rf_pipe'}  for predicting sii with regular rounding up: 0.22030839345298214\n",
      "mse for {'ada_pipe'}  for predicting sii: 0.5209708483072961\n",
      "kappa for {'ada_pipe'}  for predicting sii with regular rounding: 0.3057486155121\n",
      "kappa for {'ada_pipe'}  for predicting sii with regular rounding up: 0.20765156670710416\n",
      "mse for {'grad_pipe'}  for predicting sii: 0.500599722740219\n",
      "kappa for {'grad_pipe'}  for predicting sii with regular rounding: 0.3886933543852724\n",
      "kappa for {'grad_pipe'}  for predicting sii with regular rounding up: 0.22049766071402654\n",
      "mse for {'xgb_pipe'}  for predicting sii: 0.6005931150779773\n",
      "kappa for {'xgb_pipe'}  for predicting sii with regular rounding: 0.3144776931723965\n",
      "kappa for {'xgb_pipe'}  for predicting sii with regular rounding up: 0.23444847391161006\n",
      "mse for {'slr_pipe'}  for predicting sii computed from PCIAT: 0.7266514806378133\n",
      "kappa for {'slr_pipe'}  for predicting sii computed from PCIAT: 0.2314687271909076\n",
      "mse for {'mlr_key_pipe'}  for predicting sii computed from PCIAT: 0.662870159453303\n",
      "kappa for {'mlr_key_pipe'}  for predicting sii computed from PCIAT: 0.32466893981444755\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 84\u001b[0m\n\u001b[1;32m     82\u001b[0m labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m     83\u001b[0m pred \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mcut(pred, bins\u001b[38;5;241m=\u001b[39mbins, labels\u001b[38;5;241m=\u001b[39mlabels, right\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 84\u001b[0m mse \u001b[38;5;241m=\u001b[39m \u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ho\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msii\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m kappa \u001b[38;5;241m=\u001b[39m cohen_kappa_score(train_ho[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msii\u001b[39m\u001b[38;5;124m'\u001b[39m], pred, weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquadratic\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse for\u001b[39m\u001b[38;5;124m'\u001b[39m, {pipeline_name},\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for predicting sii computed from PCIAT:\u001b[39m\u001b[38;5;124m'\u001b[39m,mse)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/metrics/_regression.py:506\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m squared:\n\u001b[1;32m    502\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m root_mean_squared_error(\n\u001b[1;32m    503\u001b[0m             y_true, y_pred, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, multioutput\u001b[38;5;241m=\u001b[39mmultioutput\n\u001b[1;32m    504\u001b[0m         )\n\u001b[0;32m--> 506\u001b[0m y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    510\u001b[0m output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage((y_true \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/metrics/_regression.py:113\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype, xp)\u001b[0m\n\u001b[1;32m    111\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[1;32m    112\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m--> 113\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_true\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    116\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mreshape(y_true, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py:1064\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1059\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1060\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1061\u001b[0m     )\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m-> 1064\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m   1073\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py:123\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py:172\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    171\u001b[0m     )\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Next either stick this in a kfold split or use cross_val_score\n",
    "\n",
    "train_tt, train_ho = train_test_split(train_cleaned, test_size=0.2)\n",
    "\n",
    "models = {\n",
    "'slr_pipe' : Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "                ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                ('selector', ColumnTransformer([('selector', 'passthrough', ['PreInt_EduHx-computerinternet_hoursday'])], remainder=\"drop\")),\n",
    "                ('linear', LinearRegression())]),\n",
    "\n",
    "'mlr_key_pipe' : Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "                ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                ('selector', ColumnTransformer([('selector', 'passthrough', keyfeatures)], remainder=\"drop\")),\n",
    "                ('linear', LinearRegression())]),\n",
    "\n",
    "'mlr_all_pipe' : Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "                ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                ('linear', LinearRegression())]),\n",
    "\n",
    "'knn_pipe' : Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "                ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                ('knn', KNeighborsRegressor(10))]),\n",
    "\n",
    "'svr_pipe' : Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "                ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                ('rf', SVR())]),\n",
    "\n",
    "'rf_pipe' : Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "                ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                ('rf', RandomForestRegressor())]),\n",
    "\n",
    "'ada_pipe' : Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "                ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                ('ada', AdaBoostRegressor())]),\n",
    "\n",
    "'grad_pipe' : Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "                ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                ('grad', GradientBoostingRegressor())]),\n",
    "\n",
    "'xgb_pipe' : Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "                ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                ('xgb', XGBRegressor())])\n",
    "}\n",
    "\n",
    "for pipeline_name, pipeline_obj in models.items():\n",
    "    # print(f\"Pipeline: {pipeline_name}\")\n",
    "    pipeline_obj.fit(train_tt[predictors], train_tt['PCIAT-PCIAT_Total'])\n",
    "    pred = pipeline_obj.predict(train_ho[predictors])\n",
    "    mse = mean_squared_error(train_ho['PCIAT-PCIAT_Total'], pred)\n",
    "    print('mse for', {pipeline_name},' for predicting PCIAT:',mse)\n",
    "    #print(f\"Pipeline {pipeline_name} predictions: {y_pred}\")\n",
    "\n",
    "for pipeline_name, pipeline_obj in models.items():\n",
    "    pipeline_obj.fit(train_tt[predictors], train_tt['sii'])\n",
    "    pred = pipeline_obj.predict(train_ho[predictors])\n",
    "    pred_round = np.round(pred)\n",
    "    pred_roundup = np.ceil(pred)\n",
    "    mse = mean_squared_error(train_ho['sii'], pred)\n",
    "    kappa_round = cohen_kappa_score(train_ho['sii'], pred_round, weights='quadratic')\n",
    "    kappa_roundup = cohen_kappa_score(train_ho['sii'], pred_roundup, weights='quadratic')\n",
    "    print('mse for', {pipeline_name},' for predicting sii:',mse)\n",
    "    print('kappa for', {pipeline_name},' for predicting sii with regular rounding:',kappa_round)\n",
    "    print('kappa for', {pipeline_name},' for predicting sii with regular rounding up:',kappa_roundup)\n",
    "\n",
    "for pipeline_name, pipeline_obj in models.items():\n",
    "    pipeline_obj.fit(train_tt[predictors], train_tt['PCIAT-PCIAT_Total'])\n",
    "    pred = pipeline_obj.predict(train_ho[predictors])\n",
    "    bins = [0, 30, 49,79,100]\n",
    "    labels = [0,1,2,3]\n",
    "    pred = pd.cut(pred, bins=bins, labels=labels, right=False)\n",
    "    mse = mean_squared_error(train_ho['sii'], pred)\n",
    "    kappa = cohen_kappa_score(train_ho['sii'], pred, weights='quadratic')\n",
    "    print('mse for', {pipeline_name},' for predicting sii computed from PCIAT:',mse)\n",
    "    print('kappa for', {pipeline_name},' for predicting sii computed from PCIAT:',kappa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sequential Binary Classification**\n",
    "\n",
    "It looks like our attempts so far have under-predicted sii values of 2 and 3. I'm going to try to implement a method that first predicts whether or not the sii value is 3, then on the remaining values predict whether or not they are 2, etc.\n",
    "\n",
    "I came up with this idea myself, but I wasn't the first one to do it. It was described on Medium: https://towardsdatascience.com/simple-trick-to-train-an-ordinal-regression-with-any-classifier-6911183d2a3c from an article by Frank and Hal\n",
    "\n",
    "Also described on stackoverflow: https://stackoverflow.com/questions/57561189/multi-class-multi-label-ordinal-classification-with-sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class OrdinalClassifier(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, clf):\n",
    "        self.clf = clf\n",
    "        self.clfs = {}\n",
    "        self.unique_class = np.NaN\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.unique_class = np.sort(np.unique(y))\n",
    "        if self.unique_class.shape[0] > 2:\n",
    "            for i in range(self.unique_class.shape[0]-1):\n",
    "                # for each k - 1 ordinal value we fit a binary classification problem\n",
    "                binary_y = (y > self.unique_class[i]).astype(np.uint8)\n",
    "                clf = clone(self.clf)\n",
    "                clf.fit(X, binary_y)\n",
    "                self.clfs[i] = clf\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        clfs_predict = {i: self.clfs[i].predict_proba(X) for i in self.clfs}\n",
    "        predicted = []\n",
    "        k = len(self.unique_class) - 1\n",
    "        for i, y in enumerate(self.unique_class):\n",
    "            if i == 0:\n",
    "                # V1 = 1 - Pr(y > V1)\n",
    "                predicted.append(1 - clfs_predict[0][:,1])\n",
    "            elif i < k:\n",
    "                # Vi = Pr(y <= Vi) * Pr(y > Vi-1)\n",
    "                 predicted.append((1 - clfs_predict[i][:,1]) * clfs_predict[i-1][:,1])\n",
    "            else:\n",
    "                # Vk = Pr(y > Vk-1)\n",
    "                predicted.append(clfs_predict[k-1][:,1])\n",
    "        return np.vstack(predicted).T\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.unique_class[np.argmax(self.predict_proba(X), axis=1)]\n",
    "\n",
    "    def score(self, X, y, sample_weight=None):\n",
    "        return accuracy_score(y, self.predict(X), sample_weight=sample_weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_fall_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
