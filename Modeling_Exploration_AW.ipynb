{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goals**\n",
    "\n",
    "1. Set up a pipeline to incorporate the imputation\n",
    "2. Do a random forest regressor to identify important features\n",
    "3. Do a test run with one model (linear, most likely) that computes:\n",
    "    - MSE for predicting PCIAT-Total\n",
    "    - MSE for predicting sii when computed from predicted PCIAT-Total\n",
    "    - MSE for predicting sii directly\n",
    "    - kappa for predicting sii when computed from predicted PCIAT-Total\n",
    "    - kappa for predicting sii directly\n",
    "4. After getting the model working, measure these things for out-of-the box:\n",
    "    - multiple linear regression\n",
    "    - knn regression\n",
    "    - random forest\n",
    "    - support vector\n",
    "    - gradient boost\n",
    "    - adaboost\n",
    "    - xgboost\n",
    "5. After identifying a promising out-of-the-box model, try tuning it\n",
    "6. Try implementing a sequential predictor (either logistic regression or random forest) that:\n",
    "    - Starts by predicting 3's vs. non-threes\n",
    "    - Predicts 2's vs. non-twos from the remaining cases\n",
    "    - etc.\n",
    "7. Try using different models for doing this sequential prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from CustomImputers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the Data**\n",
    "\n",
    "For the purpose of developing our model(s), we'll work with data that include the imputed outcome (PCIAT_Total and/or sii) scores AND have cleaned predictors.\n",
    "\n",
    "In the final version of our code, we'll work with data with cleaned predictors but won't have any access to the outcome scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the cleaned & outcome-imputed data\n",
    "train_cleaned=pd.read_csv('train_cleaned_outcome_imputed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an initial list of predictor and outcome columns\n",
    "\n",
    "predictors = train_cleaned.columns.tolist()\n",
    "if 'id' in predictors:\n",
    "    predictors.remove('id')\n",
    "if 'sii' in predictors:\n",
    "    predictors.remove('sii')\n",
    "predictors = [x for x in predictors if 'PCIAT' not in x]\n",
    "predictors = [x for x in predictors if 'Season' not in x]\n",
    "\n",
    "outcome_pciat = ['PCIAT-PCIAT_Total']\n",
    "outcome_sii = ['sii']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Constructing a Random Forest for Feature Identification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Basic_Demos-Age</td>\n",
       "      <td>0.137804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Physical-Height</td>\n",
       "      <td>0.126698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>PreInt_EduHx-computerinternet_hoursday</td>\n",
       "      <td>0.118666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>BIA-BIA_FFM</td>\n",
       "      <td>0.077628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>SDS-SDS_Total_Raw</td>\n",
       "      <td>0.074039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Physical-Weight</td>\n",
       "      <td>0.072494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ENMO_Avg_Active_Days_MVPA110</td>\n",
       "      <td>0.065296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>FGC-FGC_CU</td>\n",
       "      <td>0.055829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>BIA-BIA_FFMI</td>\n",
       "      <td>0.023911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>FGC-FGC_PU</td>\n",
       "      <td>0.023766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>BIA-BIA_Fat</td>\n",
       "      <td>0.023610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Physical-BMI</td>\n",
       "      <td>0.018582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>PAQ_Total</td>\n",
       "      <td>0.018188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>FGC-FGC_TL</td>\n",
       "      <td>0.015412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Fitness_Endurance-Max_Stage</td>\n",
       "      <td>0.014949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Physical-Waist_Circumference</td>\n",
       "      <td>0.013865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>FGC-FGC_SR</td>\n",
       "      <td>0.013734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CGAS-CGAS_Score</td>\n",
       "      <td>0.012663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>BIA-BIA_FMI</td>\n",
       "      <td>0.011455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ENMO_Avg_Active_Days_MVPA192</td>\n",
       "      <td>0.011191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Physical-Systolic_BP</td>\n",
       "      <td>0.011164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Positive_Anglez_Active_Days</td>\n",
       "      <td>0.010808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Fitness_Endurance_Total_Time_Sec</td>\n",
       "      <td>0.009825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Physical-HeartRate</td>\n",
       "      <td>0.008186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>BIA-BIA_Activity_Level_num</td>\n",
       "      <td>0.007720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Physical-Diastolic_BP</td>\n",
       "      <td>0.007447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>BIA-BIA_Frame_num</td>\n",
       "      <td>0.006813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Basic_Demos-Sex</td>\n",
       "      <td>0.004352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>FGC-FGC_TL_Zone</td>\n",
       "      <td>0.001435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>FGC-FGC_CU_Zone</td>\n",
       "      <td>0.001110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>FGC-FGC_PU_Zone</td>\n",
       "      <td>0.000719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>FGC-FGC_SR_Zone</td>\n",
       "      <td>0.000643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>PAQ_Zone</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   feature  importance_score\n",
       "0                          Basic_Demos-Age          0.137804\n",
       "4                          Physical-Height          0.126698\n",
       "24  PreInt_EduHx-computerinternet_hoursday          0.118666\n",
       "18                             BIA-BIA_FFM          0.077628\n",
       "23                       SDS-SDS_Total_Raw          0.074039\n",
       "5                          Physical-Weight          0.072494\n",
       "26            ENMO_Avg_Active_Days_MVPA110          0.065296\n",
       "11                              FGC-FGC_CU          0.055829\n",
       "19                            BIA-BIA_FFMI          0.023911\n",
       "13                              FGC-FGC_PU          0.023766\n",
       "21                             BIA-BIA_Fat          0.023610\n",
       "3                             Physical-BMI          0.018582\n",
       "30                               PAQ_Total          0.018188\n",
       "15                              FGC-FGC_TL          0.015412\n",
       "10             Fitness_Endurance-Max_Stage          0.014949\n",
       "6             Physical-Waist_Circumference          0.013865\n",
       "28                              FGC-FGC_SR          0.013734\n",
       "2                          CGAS-CGAS_Score          0.012663\n",
       "20                             BIA-BIA_FMI          0.011455\n",
       "25            ENMO_Avg_Active_Days_MVPA192          0.011191\n",
       "9                     Physical-Systolic_BP          0.011164\n",
       "27             Positive_Anglez_Active_Days          0.010808\n",
       "32        Fitness_Endurance_Total_Time_Sec          0.009825\n",
       "8                       Physical-HeartRate          0.008186\n",
       "17              BIA-BIA_Activity_Level_num          0.007720\n",
       "7                    Physical-Diastolic_BP          0.007447\n",
       "22                       BIA-BIA_Frame_num          0.006813\n",
       "1                          Basic_Demos-Sex          0.004352\n",
       "16                         FGC-FGC_TL_Zone          0.001435\n",
       "12                         FGC-FGC_CU_Zone          0.001110\n",
       "14                         FGC-FGC_PU_Zone          0.000719\n",
       "29                         FGC-FGC_SR_Zone          0.000643\n",
       "31                                PAQ_Zone          0.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "\n",
    "pipe_mice = Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "                    ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                    ('rf', RandomForestRegressor(n_estimators = 300, max_features = 'sqrt', max_depth = 5, random_state = 216))])\n",
    "\n",
    "pipe_mice.fit(train_cleaned[predictors],train_cleaned['PCIAT-PCIAT_Total'])\n",
    "\n",
    "train_pred_mice = pipe_mice.predict(train_cleaned[predictors])\n",
    "\n",
    "#Get feature importance from the rf inside pipe\n",
    "score_mice_df = pd.DataFrame({'feature':train_cleaned[predictors].columns,\n",
    "                            'importance_score': pipe_mice.named_steps['rf'].feature_importances_})\n",
    "\n",
    "score_mice_df.sort_values('importance_score',ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyfeatures = ['Basic_Demos-Age',\n",
    " 'Physical-Height',\n",
    " 'PreInt_EduHx-computerinternet_hoursday',\n",
    " 'BIA-BIA_FFM',\n",
    " 'SDS-SDS_Total_Raw',\n",
    " 'Physical-Weight',\n",
    " 'ENMO_Avg_Active_Days_MVPA110',\n",
    " 'FGC-FGC_CU']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Constructing some Linear Models**\n",
    "\n",
    "In this section, I'll make linear models with:\n",
    "* A single predictor (hours spent on the internet)\n",
    "* A small number of predictors (taken from the importance scores generated above)\n",
    "* All the predictors\n",
    "\n",
    "Each of these will be run through a KFold split with a 20% validation set; for each model we'll compute several stats to compare the predictions with PCIAT scores and also with sii scores:\n",
    "* MSE\n",
    "* kappa\n",
    "\n",
    "Note: Column selector documented here: https://stackoverflow.com/questions/62416223/how-to-select-only-few-columns-in-scikit-learn-column-selector-pipeline\n",
    "\n",
    "Note: custom loss functions for linear models are documented here: https://alexmiller.phd/posts/linear-model-custom-loss-function-regularization-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(334.7045879129308)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First I'll see if I can get a pipe set up to do prediction on a split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "train_tt, train_ho = train_test_split(train_cleaned, test_size=0.2)\n",
    "\n",
    "slr = Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "                ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                ('selector', ColumnTransformer([('selector', 'passthrough', ['PreInt_EduHx-computerinternet_hoursday'])], remainder=\"drop\")),\n",
    "                ('linear', LinearRegression())])\n",
    "\n",
    "slr.fit(train_tt[predictors], train_tt['PCIAT-PCIAT_Total'])\n",
    "mean_squared_error(train_ho['PCIAT-PCIAT_Total'], slr.predict(train_ho))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse for {'slr_pipe'}  for predicting PCIAT: 360.4679146833167\n",
      "mse for {'slr_pipe'}  for predicting sii computed from PCIAT: 0.6537585421412301\n",
      "kappa for {'slr_pipe'}  for predicting sii computed from PCIAT: 0.2982728756258807\n",
      "mse for {'mlr_key_pipe'}  for predicting PCIAT: 324.89280136779126\n",
      "mse for {'mlr_key_pipe'}  for predicting sii computed from PCIAT: 0.6036446469248291\n",
      "kappa for {'mlr_key_pipe'}  for predicting sii computed from PCIAT: 0.3883575796131461\n",
      "mse for {'mlr_all_pipe'}  for predicting PCIAT: 321.2262999260181\n",
      "mse for {'mlr_all_pipe'}  for predicting sii computed from PCIAT: 0.5466970387243736\n",
      "kappa for {'mlr_all_pipe'}  for predicting sii computed from PCIAT: 0.4378101488714583\n",
      "mse for {'knn_pipe'}  for predicting PCIAT: 370.3012419134396\n",
      "mse for {'knn_pipe'}  for predicting sii computed from PCIAT: 0.6742596810933941\n",
      "kappa for {'knn_pipe'}  for predicting sii computed from PCIAT: 0.31604821306384545\n",
      "mse for {'svr_pipe'}  for predicting PCIAT: 386.954916445968\n",
      "mse for {'svr_pipe'}  for predicting sii computed from PCIAT: 0.7813211845102506\n",
      "kappa for {'svr_pipe'}  for predicting sii computed from PCIAT: 0.19283734742778103\n",
      "mse for {'rf_pipe'}  for predicting PCIAT: 305.49211690761837\n",
      "mse for {'rf_pipe'}  for predicting sii computed from PCIAT: 0.5945330296127562\n",
      "kappa for {'rf_pipe'}  for predicting sii computed from PCIAT: 0.39347629306922305\n",
      "mse for {'ada_pipe'}  for predicting PCIAT: 343.0025661410732\n",
      "mse for {'ada_pipe'}  for predicting sii computed from PCIAT: 0.6036446469248291\n",
      "kappa for {'ada_pipe'}  for predicting sii computed from PCIAT: 0.38062685343427727\n",
      "mse for {'grad_pipe'}  for predicting PCIAT: 308.140356863923\n",
      "mse for {'grad_pipe'}  for predicting sii computed from PCIAT: 0.5945330296127562\n",
      "kappa for {'grad_pipe'}  for predicting sii computed from PCIAT: 0.3859021015001528\n",
      "mse for {'xgb_pipe'}  for predicting PCIAT: 360.86779811961725\n",
      "mse for {'xgb_pipe'}  for predicting sii computed from PCIAT: 0.6651480637813212\n",
      "kappa for {'xgb_pipe'}  for predicting sii computed from PCIAT: 0.3757219803446026\n",
      "mse for {'slr_pipe'}  for predicting sii: 0.534970778569514\n",
      "kappa for {'slr_pipe'}  for predicting sii with regular rounding: 0.33371706098978826\n",
      "kappa for {'slr_pipe'}  for predicting sii with rounding up: 0.13905625224714813\n",
      "mse for {'mlr_key_pipe'}  for predicting sii: 0.4876469847297859\n",
      "kappa for {'mlr_key_pipe'}  for predicting sii with regular rounding: 0.41533960911088097\n",
      "kappa for {'mlr_key_pipe'}  for predicting sii with rounding up: 0.21943228485039934\n",
      "mse for {'mlr_all_pipe'}  for predicting sii: 0.48082639035674074\n",
      "kappa for {'mlr_all_pipe'}  for predicting sii with regular rounding: 0.42199638637309267\n",
      "kappa for {'mlr_all_pipe'}  for predicting sii with rounding up: 0.2618000665425738\n",
      "mse for {'knn_pipe'}  for predicting sii: 0.5701366742596811\n",
      "kappa for {'knn_pipe'}  for predicting sii with regular rounding: 0.29840945412639575\n",
      "kappa for {'knn_pipe'}  for predicting sii with rounding up: 0.1525453041230609\n",
      "mse for {'svr_pipe'}  for predicting sii: 0.5535625808227705\n",
      "kappa for {'svr_pipe'}  for predicting sii with regular rounding: 0.3203038224635648\n",
      "kappa for {'svr_pipe'}  for predicting sii with rounding up: 0.18698875324633257\n",
      "mse for {'rf_pipe'}  for predicting sii: 0.48520651733738296\n",
      "kappa for {'rf_pipe'}  for predicting sii with regular rounding: 0.36371180102232736\n",
      "kappa for {'rf_pipe'}  for predicting sii with rounding up: 0.22196467144989562\n",
      "mse for {'ada_pipe'}  for predicting sii: 0.524199266279155\n",
      "kappa for {'ada_pipe'}  for predicting sii with regular rounding: 0.10142771781278204\n",
      "kappa for {'ada_pipe'}  for predicting sii with rounding up: 0.23607555464453467\n",
      "mse for {'grad_pipe'}  for predicting sii: 0.4769306498398787\n",
      "kappa for {'grad_pipe'}  for predicting sii with regular rounding: 0.39391864589413195\n",
      "kappa for {'grad_pipe'}  for predicting sii with rounding up: 0.2595990773234864\n",
      "mse for {'xgb_pipe'}  for predicting sii: 0.5457944978689928\n",
      "kappa for {'xgb_pipe'}  for predicting sii with regular rounding: 0.4352515626497697\n",
      "kappa for {'xgb_pipe'}  for predicting sii with rounding up: 0.24060270628590663\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Next either stick this in a kfold split or use cross_val_score\n",
    "\n",
    "train_tt, train_ho = train_test_split(train_cleaned, test_size=0.2)\n",
    "\n",
    "models = {\n",
    "'slr_pipe' : Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "                ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                ('selector', ColumnTransformer([('selector', 'passthrough', ['PreInt_EduHx-computerinternet_hoursday'])], remainder=\"drop\")),\n",
    "                ('linear', LinearRegression())]),\n",
    "\n",
    "'mlr_key_pipe' : Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "                ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                ('selector', ColumnTransformer([('selector', 'passthrough', keyfeatures)], remainder=\"drop\")),\n",
    "                ('linear', LinearRegression())]),\n",
    "\n",
    "'mlr_all_pipe' : Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "                ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                ('linear', LinearRegression())]),\n",
    "\n",
    "'knn_pipe' : Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "                ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                ('knn', KNeighborsRegressor(10))]),\n",
    "\n",
    "'svr_pipe' : Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "                ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                ('rf', SVR())]),\n",
    "\n",
    "'rf_pipe' : Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "                ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                ('rf', RandomForestRegressor())]),\n",
    "\n",
    "'ada_pipe' : Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "                ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                ('ada', AdaBoostRegressor())]),\n",
    "\n",
    "'grad_pipe' : Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "                ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                ('grad', GradientBoostingRegressor())]),\n",
    "\n",
    "'xgb_pipe' : Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "                ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                ('xgb', XGBRegressor())])\n",
    "}\n",
    "\n",
    "for pipeline_name, pipeline_obj in models.items():\n",
    "    # print(f\"Pipeline: {pipeline_name}\")\n",
    "    # Fit and make predictions of PCIAT\n",
    "    pipeline_obj.fit(train_tt[predictors], train_tt['PCIAT-PCIAT_Total'])\n",
    "    pred = pipeline_obj.predict(train_ho[predictors])\n",
    "    # Compute mse for PCIAT predictions\n",
    "    mse = mean_squared_error(train_ho['PCIAT-PCIAT_Total'], pred)\n",
    "    print('mse for', {pipeline_name},' for predicting PCIAT:',mse)\n",
    "    # Next compute sii based on PCIAT and compute mse and kappa\n",
    "    bins = [0, 30, 49,79,100]\n",
    "    pred_bin = np.digitize(pred, bins)-1    \n",
    "    mse2 = mean_squared_error(train_ho['sii'], pred_bin)\n",
    "    print('mse for', {pipeline_name},' for predicting sii computed from PCIAT:',mse2)\n",
    "    kappa = cohen_kappa_score(train_ho['sii'], pred_bin, weights='quadratic')\n",
    "    print('kappa for', {pipeline_name},' for predicting sii computed from PCIAT:',kappa)\n",
    "    #print(f\"Pipeline {pipeline_name} predictions: {y_pred}\")\n",
    "\n",
    "for pipeline_name, pipeline_obj in models.items():\n",
    "    # Fit and make predictions of sii\n",
    "    pipeline_obj.fit(train_tt[predictors], train_tt['sii'])\n",
    "    pred = pipeline_obj.predict(train_ho[predictors])\n",
    "    # Try two different ways of rounding the predictions\n",
    "    pred_round = np.round(pred)\n",
    "    pred_roundup = np.ceil(pred)\n",
    "    # Compute mse and kappas\n",
    "    mse = mean_squared_error(train_ho['sii'], pred)\n",
    "    kappa_round = cohen_kappa_score(train_ho['sii'], pred_round, weights='quadratic')\n",
    "    kappa_roundup = cohen_kappa_score(train_ho['sii'], pred_roundup, weights='quadratic')\n",
    "    print('mse for', {pipeline_name},' for predicting sii:',mse)\n",
    "    print('kappa for', {pipeline_name},' for predicting sii with regular rounding:',kappa_round)\n",
    "    print('kappa for', {pipeline_name},' for predicting sii with rounding up:',kappa_roundup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sequential Binary Classification**\n",
    "\n",
    "It looks like our attempts so far have under-predicted sii values of 2 and 3. I'm going to try to implement a method that first predicts whether or not the sii value is 3, then on the remaining values predict whether or not they are 2, etc.\n",
    "\n",
    "I came up with this idea myself, but I wasn't the first one to do it. It was described on Medium: https://towardsdatascience.com/simple-trick-to-train-an-ordinal-regression-with-any-classifier-6911183d2a3c from an article by Frank and Hal\n",
    "\n",
    "Also described on stackoverflow: https://stackoverflow.com/questions/57561189/multi-class-multi-label-ordinal-classification-with-sklearn\n",
    "\n",
    "Some discussion of the proposed code that highlights some of its issues is on stackoverflow: https://stackoverflow.com/questions/66486947/how-to-use-ordinal-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class OrdinalClassifier(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, clf):\n",
    "        self.clf = clf\n",
    "        self.clfs = {}\n",
    "        self.unique_class = np.NaN\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.unique_class = np.sort(np.unique(y))\n",
    "        if self.unique_class.shape[0] > 2:\n",
    "            for i in range(self.unique_class.shape[0]-1):\n",
    "                # for each k - 1 ordinal value we fit a binary classification problem\n",
    "                binary_y = (y > self.unique_class[i]).astype(np.uint8)\n",
    "                clf = clone(self.clf)\n",
    "                clf.fit(X, binary_y)\n",
    "                self.clfs[i] = clf\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        clfs_predict = {i: self.clfs[i].predict_proba(X) for i in self.clfs}\n",
    "        predicted = []\n",
    "        k = len(self.unique_class) - 1\n",
    "        for i, y in enumerate(self.unique_class):\n",
    "            if i == 0:\n",
    "                # V1 = 1 - Pr(y > V1)\n",
    "                predicted.append(1 - clfs_predict[0][:,1])\n",
    "            elif i < k:\n",
    "                # Vi = Pr(y <= Vi) * Pr(y > Vi-1)\n",
    "                 predicted.append((1 - clfs_predict[i][:,1]) * clfs_predict[i-1][:,1])\n",
    "            else:\n",
    "                # Vk = Pr(y > Vk-1)\n",
    "                predicted.append(clfs_predict[k-1][:,1])\n",
    "        return np.vstack(predicted).T\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.unique_class[np.argmax(self.predict_proba(X), axis=1)]\n",
    "\n",
    "    def score(self, X, y, sample_weight=None):\n",
    "        return accuracy_score(y, self.predict(X), sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Neural Regression to Predict Ordinal Outcomes**\n",
    "\n",
    "It looks like we could also use \"neural regression\" (PyTorch) to do the prediction. \n",
    "\n",
    "Here is a website that describes how to accomplish this: https://visualstudiomagazine.com/articles/2021/10/04/ordinal-classification-pytorch.aspx\n",
    "\n",
    "The code below is copied from the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# house_price_ord.py\n",
    "# predict ordinal price from AC, sq ft, style, nearest school\n",
    "# PyTorch 1.8.0-CPU Anaconda3-2020.02  Python 3.7.6\n",
    "# Windows 10 \n",
    "\n",
    "import numpy as np\n",
    "import torch as T\n",
    "device = T.device(\"cpu\")  # apply to Tensor or Module\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "class HouseDataset(T.utils.data.Dataset):\n",
    "  # AC  sq ft   style  price   school\n",
    "  # -1  0.2500  0 1 0    3     0 1 0\n",
    "  #  1  0.1275  1 0 0    2     0 0 1\n",
    "  # air condition: -1 = no, +1 = yes\n",
    "  # style: art_deco, bungalow, colonial\n",
    "  # price: k=4: 0 = low, 1 = medium, 2 = high, 3 = very high\n",
    "  # school: johnson, kennedy, lincoln\n",
    "\n",
    "  def __init__(self, src_file, k):\n",
    "    # k for programmtic approach\n",
    "    all_xy = np.loadtxt(src_file, \n",
    "      usecols=[0,1,2,3,4,5,6,7,8], delimiter=\"\\t\",\n",
    "      comments=\"#\", skiprows=0, dtype=np.float32)\n",
    "\n",
    "    tmp_x = all_xy[:,[0,1,2,3,4,6,7,8]]\n",
    "    tmp_y = all_xy[:,5]    # 1D -- 2D will be required\n",
    "\n",
    "    n = len(tmp_y)\n",
    "    for i in range(n):  # hard-coded is easy to understand\n",
    "      if int(tmp_y[i])   == 0: tmp_y[i] = 0.125\n",
    "      elif int(tmp_y[i]) == 1: tmp_y[i] = 0.375\n",
    "      elif int(tmp_y[i]) == 2: tmp_y[i] = 0.625\n",
    "      elif int(tmp_y[i]) == 3: tmp_y[i] = 0.875\n",
    "      else: print(\"Fatal logic error \")\n",
    "\n",
    "    tmp_y = np.reshape(tmp_y, (-1,1))  # 2D    \n",
    "\n",
    "    self.x_data = T.tensor(tmp_x, \\\n",
    "      dtype=T.float32).to(device)\n",
    "    self.y_data = T.tensor(tmp_y, \\\n",
    "      dtype=T.float32).to(device)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.x_data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    preds = self.x_data[idx,:]  # or just [idx]\n",
    "    price = self.y_data[idx,:] \n",
    "    return (preds, price)       # tuple of two matrices \n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "class Net(T.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Net, self).__init__()\n",
    "    self.hid1 = T.nn.Linear(8, 10)  # 8-(10-10)-1\n",
    "    self.hid2 = T.nn.Linear(10, 10)\n",
    "    self.oupt = T.nn.Linear(10, 1)  # [0.0 to 1.0]\n",
    "\n",
    "    T.nn.init.xavier_uniform_(self.hid1.weight)\n",
    "    T.nn.init.zeros_(self.hid1.bias)\n",
    "    T.nn.init.xavier_uniform_(self.hid2.weight)\n",
    "    T.nn.init.zeros_(self.hid2.bias)\n",
    "    T.nn.init.xavier_uniform_(self.oupt.weight)\n",
    "    T.nn.init.zeros_(self.oupt.bias)\n",
    "\n",
    "  def forward(self, x):\n",
    "    z = T.tanh(self.hid1(x))\n",
    "    z = T.tanh(self.hid2(z))\n",
    "    z = T.sigmoid(self.oupt(z))  # \n",
    "    return z\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def accuracy(model, ds, k):\n",
    "  n_correct = 0; n_wrong = 0\n",
    "  acc_delta = (1.0 / k) / 2   # if k=4 delta = 0.125\n",
    "  for i in range(len(ds)):    # each input\n",
    "    (X, y) = ds[i]            # (predictors, target)\n",
    "    with T.no_grad():         # y target is like 0.375\n",
    "      oupt = model(X)         # oupt is in [0.0, 1.0]\n",
    "\n",
    "    if T.abs(oupt - y) <= acc_delta:\n",
    "      n_correct += 1\n",
    "    else:\n",
    "      n_wrong += 1\n",
    "\n",
    "  acc = (n_correct * 1.0) / (n_correct + n_wrong)\n",
    "  return acc\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def accuracy_old(model, ds, k):\n",
    "  model.eval()\n",
    "  n_correct = 0; n_wrong = 0\n",
    "  for i in range(len(ds)):\n",
    "    (X, Y) = ds[i]            # (predictors, target)\n",
    "    with T.no_grad():\n",
    "      oupt = model(X)         # computed is in 0.0 to 1.0\n",
    "    if oupt >= 0.0 and oupt < 0.25 and Y == 0.125:  # ugly\n",
    "      n_correct += 1\n",
    "    elif oupt >= 0.25 and oupt < 0.50 and Y == 0.375:\n",
    "      n_correct += 1\n",
    "    elif oupt >= 0.50 and oupt < 0.75 and Y == 0.625:\n",
    "      n_correct += 1\n",
    "    elif oupt >= 0.75 and Y == 0.875:\n",
    "      n_correct += 1\n",
    "    else:\n",
    "      n_wrong += 1\n",
    "  acc = (n_correct * 1.0) / (n_correct + n_wrong)\n",
    "  return acc\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def train(net, ds, bs, lr, me, le):\n",
    "  # network, dataset, batch_size, learn_rate, \n",
    "  # max_epochs, log_every\n",
    "  train_ldr = T.utils.data.DataLoader(ds,\n",
    "    batch_size=bs, shuffle=True)\n",
    "  loss_func = T.nn.MSELoss()\n",
    "  opt = T.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "  for epoch in range(0, me):\n",
    "    # T.manual_seed(1+epoch)  # recovery reproducibility\n",
    "    epoch_loss = 0  # for one full epoch\n",
    "\n",
    "    for (b_idx, batch) in enumerate(train_ldr):\n",
    "      (X, y) = batch           # (predictors, targets)\n",
    "      opt.zero_grad()          # prepare gradients\n",
    "      oupt = net(X)            # predicted prices\n",
    "\n",
    "      loss_val = loss_func(oupt, y)  # a tensor\n",
    "      epoch_loss += loss_val.item()  # accumulate\n",
    "      loss_val.backward()  # compute gradients\n",
    "      opt.step()           # update weights\n",
    "\n",
    "    if epoch % le == 0:\n",
    "      print(\"epoch = %4d   loss = %0.4f\" % \\\n",
    "       (epoch, epoch_loss))\n",
    "      # TODO: save checkpoint\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def float_oupt_to_class(oupt, k):\n",
    "  end_pts = np.zeros(k+1, dtype=np.float32) \n",
    "  delta = 1.0 / k\n",
    "  for i in range(k):\n",
    "    end_pts[i] = i * delta\n",
    "  end_pts[k] = 1.0\n",
    "  # if k=4, [0.0, 0.25, 0.50, 0.75, 1.0] \n",
    "\n",
    "  for i in range(k):\n",
    "    if oupt >= end_pts[i] and oupt <= end_pts[i+1]:\n",
    "      return i\n",
    "  return -1  # fatal error \n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "  # 0. get started\n",
    "  print(\"\\nBegin predict House ordinal price \\n\")\n",
    "  T.manual_seed(1)  # representative results \n",
    "  np.random.seed(1)\n",
    "  \n",
    "  # 1. create Dataset objects\n",
    "  print(\"Creating Houses Dataset objects \")\n",
    "  print(\"Converting ordinal labels to float targets \")\n",
    "  train_file = \".\\\\Data\\\\houses_train_ord.txt\"\n",
    "  train_ds = HouseDataset(train_file, k=4)  # 200 rows\n",
    "\n",
    "  test_file = \".\\\\Data\\\\houses_test_ord.txt\"\n",
    "  test_ds = HouseDataset(test_file, k=4)  # 40 rows\n",
    "\n",
    "  # 2. create network\n",
    "  print(\"\\nCreating 8-10-10-1 neural network \")\n",
    "  net = Net().to(device)\n",
    "  net.train()   # set mode\n",
    "\n",
    "  # 3. train model\n",
    "  bat_size = 10\n",
    "  lrn_rate = 0.010\n",
    "  max_epochs = 500\n",
    "  log_every = 100\n",
    "\n",
    "  print(\"\\nbat_size = %3d \" % bat_size)\n",
    "  print(\"lrn_rate = %0.3f \" % lrn_rate)\n",
    "  print(\"loss = MSELoss \")\n",
    "  print(\"optimizer = Adam \")\n",
    "  print(\"max_epochs = %3d \" % max_epochs)\n",
    "\n",
    "  print(\"\\nStarting training \")\n",
    "  train(net, train_ds, bat_size, lrn_rate, \n",
    "    max_epochs, log_every)\n",
    "  print(\"Training complete \")\n",
    "\n",
    "  # 4. evaluate model accuracy\n",
    "  print(\"\\nComputing model accuracy\")\n",
    "  net.eval()  # set mode\n",
    "  acc_train = accuracy(net, train_ds, k=4) \n",
    "  print(\"Accuracy on train data = %0.4f\" % \\\n",
    "    acc_train)\n",
    "\n",
    "  acc_test = accuracy(net, test_ds, k=4) \n",
    "  print(\"Accuracy on test data  = %0.4f\" % \\\n",
    "    acc_test)\n",
    "\n",
    "  # 5. save trained model (TODO)\n",
    "  print(\"\\nSaving trained model as houses_model.h5 \")\n",
    "  # model.save_weights(\".\\\\Models\\\\houses_model_wts.h5\")\n",
    "  # model.save(\".\\\\Models\\\\houses_model.h5\")\n",
    "\n",
    "  # 6. make a prediction\n",
    "  print(\"\\nPredicting house price for AC=no, sqft=2300, \")\n",
    "  print(\" style=colonial, school=kennedy: \")\n",
    "  unk = np.array([[-1, 0.2300,  0,0,1,  0,1,0]],\n",
    "    dtype=np.float32)\n",
    "  unk = T.tensor(unk, dtype=T.float32).to(device) \n",
    "\n",
    "  with T.no_grad():\n",
    "    pred_price = net(unk)\n",
    "  pred_price = pred_price.item()  # scalar 0.0 to 1.0\n",
    "  print(\"\\nPredicted price raw output: %0.4f\" % \\\n",
    "    pred_price)\n",
    "\n",
    "  labels = [\"low\", \"medium\", \"high\", \"very high\"]\n",
    "  c = float_oupt_to_class(pred_price, k=4)\n",
    "  print(\"Predicted price ordinal label: %d \" % c)\n",
    "  print(\"Predicted price friendly class: %s \" % \\\n",
    "    labels[c])\n",
    "\n",
    "  print(\"\\nEnd House ordinal price demo\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()\n",
    "\n",
    "# ===========\n",
    "# houses_train_ord.txt\n",
    "# AC (-1 = no), sq_ft, style (one-hot)\n",
    "# price (0=low, 1=med, 2=high, 3=v. high), \n",
    "# school (one-hot)\n",
    "#   -1   0.1275   0   1   0   0   0   0   1\n",
    "#    1   0.1100   1   0   0   0   1   0   0\n",
    "#   -1   0.1375   0   0   1   0   0   1   0\n",
    "#    1   0.1975   0   1   0   2   0   0   1\n",
    "#   -1   0.1200   0   0   1   0   1   0   0\n",
    "#   -1   0.2500   0   1   0   2   0   1   0\n",
    "#    1   0.1275   1   0   0   1   0   0   1\n",
    "#   -1   0.1750   0   0   1   1   0   0   1\n",
    "#   -1   0.2500   0   1   0   2   0   0   1\n",
    "#    1   0.1800   0   1   0   1   1   0   0\n",
    "#    1   0.0975   1   0   0   0   0   0   1\n",
    "#   -1   0.1100   0   1   0   0   0   1   0\n",
    "#    1   0.1975   0   0   1   1   0   0   1\n",
    "#   -1   0.3175   1   0   0   3   0   1   0\n",
    "#   -1   0.1700   0   1   0   1   1   0   0\n",
    "#    1   0.1650   0   1   0   1   0   1   0\n",
    "#   -1   0.2250   0   1   0   2   0   1   0\n",
    "#   -1   0.2125   0   1   0   2   0   1   0\n",
    "#    1   0.1675   0   1   0   1   0   1   0\n",
    "#    1   0.1550   1   0   0   1   0   1   0\n",
    "#   -1   0.1375   0   0   1   0   1   0   0\n",
    "#   -1   0.2425   0   1   0   2   1   0   0\n",
    "#    1   0.3200   0   0   1   3   0   1   0\n",
    "#   -1   0.3075   1   0   0   3   0   1   0\n",
    "#   -1   0.2700   1   0   0   2   0   0   1\n",
    "#    1   0.1700   0   1   0   1   0   0   1\n",
    "#   -1   0.1475   1   0   0   1   1   0   0\n",
    "#   -1   0.2500   0   1   0   2   0   0   1\n",
    "#   -1   0.2750   1   0   0   2   0   0   1\n",
    "#   -1   0.2000   1   0   0   2   1   0   0\n",
    "#   -1   0.1100   0   0   1   0   1   0   0\n",
    "#   -1   0.3400   1   0   0   3   0   1   0\n",
    "#    1   0.3000   0   0   1   3   1   0   0\n",
    "#    1   0.1550   0   1   0   1   0   1   0\n",
    "#   -1   0.2150   0   1   0   1   0   0   1\n",
    "#   -1   0.2900   0   0   1   3   0   1   0\n",
    "#    1   0.2750   0   0   1   2   0   1   0\n",
    "#    1   0.2175   0   1   0   2   0   1   0\n",
    "#    1   0.2150   0   1   0   2   0   0   1\n",
    "#    1   0.1050   1   0   0   1   1   0   0\n",
    "#   -1   0.2775   1   0   0   2   0   0   1\n",
    "#   -1   0.3225   1   0   0   3   0   1   0\n",
    "#    1   0.2075   0   1   0   2   1   0   0\n",
    "#   -1   0.3225   1   0   0   3   0   0   1\n",
    "#    1   0.2800   0   0   1   3   0   0   1\n",
    "#   -1   0.1575   0   1   0   1   0   0   1\n",
    "#    1   0.3250   0   0   1   3   0   0   1\n",
    "#   -1   0.2750   1   0   0   2   0   0   1\n",
    "#    1   0.1250   1   0   0   1   1   0   0\n",
    "#   -1   0.2325   0   1   0   2   0   0   1\n",
    "#    1   0.1825   1   0   0   2   1   0   0\n",
    "#   -1   0.2600   0   1   0   2   0   1   0\n",
    "#   -1   0.3075   1   0   0   3   0   0   1\n",
    "#   -1   0.2875   1   0   0   3   0   0   1\n",
    "#    1   0.2300   0   1   0   2   0   1   0\n",
    "#    1   0.3100   0   0   1   3   1   0   0\n",
    "#   -1   0.2750   1   0   0   2   0   0   1\n",
    "#    1   0.1125   0   1   0   0   0   0   1\n",
    "#    1   0.2525   1   0   0   2   1   0   0\n",
    "#    1   0.1625   0   1   0   1   0   1   0\n",
    "#    1   0.1075   1   0   0   1   0   0   1\n",
    "#   -1   0.2200   0   1   0   2   0   1   0\n",
    "#   -1   0.2300   0   1   0   2   0   1   0\n",
    "#   -1   0.3100   1   0   0   3   0   1   0\n",
    "#   -1   0.2875   1   0   0   3   0   1   0\n",
    "#    1   0.3375   0   0   1   3   0   0   1\n",
    "#   -1   0.1450   0   0   1   0   1   0   0\n",
    "#   -1   0.2650   1   0   0   2   1   0   0\n",
    "#    1   0.2225   0   1   0   2   1   0   0\n",
    "#   -1   0.2300   0   1   0   2   0   1   0\n",
    "#    1   0.1025   0   1   0   0   0   1   0\n",
    "#    1   0.1925   0   1   0   2   1   0   0\n",
    "#   -1   0.2525   0   1   0   2   0   1   0\n",
    "#   -1   0.1650   0   1   0   1   0   1   0\n",
    "#    1   0.1650   0   1   0   1   0   1   0\n",
    "#   -1   0.1300   1   0   0   1   0   1   0\n",
    "#   -1   0.2900   1   0   0   3   1   0   0\n",
    "#   -1   0.2175   0   1   0   1   0   0   1\n",
    "#    1   0.2300   1   0   0   2   1   0   0\n",
    "#   -1   0.3000   1   0   0   3   1   0   0\n",
    "#    1   0.2125   0   1   0   1   1   0   0\n",
    "#    1   0.2825   0   0   1   2   0   0   1\n",
    "#    1   0.3125   0   0   1   3   0   1   0\n",
    "#    1   0.2500   0   1   0   2   1   0   0\n",
    "#   -1   0.2375   0   1   0   2   0   0   1\n",
    "#    1   0.3375   0   0   1   3   0   1   0\n",
    "#    1   0.2000   0   1   0   2   0   0   1\n",
    "#   -1   0.2100   0   1   0   1   0   1   0\n",
    "#   -1   0.3225   1   0   0   3   1   0   0\n",
    "#    1   0.2375   0   0   1   2   1   0   0\n",
    "#   -1   0.2250   0   1   0   2   0   1   0\n",
    "#    1   0.1250   1   0   0   1   0   0   1\n",
    "#   -1   0.1925   1   0   0   1   1   0   0\n",
    "#   -1   0.2750   0   1   0   2   0   0   1\n",
    "#    1   0.2200   0   1   0   2   1   0   0\n",
    "#   -1   0.1675   0   1   0   1   1   0   0\n",
    "#   -1   0.1700   0   1   0   1   0   0   1\n",
    "#   -1   0.1350   0   0   1   0   0   1   0\n",
    "#   -1   0.1600   0   1   0   1   0   1   0\n",
    "#   -1   0.2125   0   1   0   1   0   0   1\n",
    "#    1   0.1200   1   0   0   1   0   0   1\n",
    "#   -1   0.2100   0   1   0   2   0   1   0\n",
    "#   -1   0.1250   0   0   1   0   0   0   1\n",
    "#   -1   0.2550   0   1   0   2   0   1   0\n",
    "#    1   0.2750   0   0   1   2   0   1   0\n",
    "#   -1   0.2200   0   0   1   1   1   0   0\n",
    "#    1   0.0925   1   0   0   1   1   0   0\n",
    "#    1   0.3350   0   0   1   3   0   1   0\n",
    "#   -1   0.2250   0   1   0   2   0   0   1\n",
    "#   -1   0.2425   0   1   0   2   1   0   0\n",
    "#    1   0.1275   0   1   0   1   0   1   0\n",
    "#    1   0.3350   0   1   0   3   1   0   0\n",
    "#   -1   0.1850   0   1   0   1   0   0   1\n",
    "#    1   0.1600   0   1   0   1   1   0   0\n",
    "#   -1   0.2400   0   1   0   2   1   0   0\n",
    "#    1   0.3300   0   0   1   3   0   0   1\n",
    "#   -1   0.3075   1   0   0   3   1   0   0\n",
    "#    1   0.2900   0   1   0   3   0   0   1\n",
    "#   -1   0.0950   0   0   1   0   1   0   0\n",
    "#   -1   0.1900   0   1   0   1   0   0   1\n",
    "#    1   0.1375   0   1   0   1   1   0   0\n",
    "#   -1   0.2100   0   1   0   1   1   0   0\n",
    "#   -1   0.3025   1   0   0   3   1   0   0\n",
    "#    1   0.1375   1   0   0   0   0   0   1\n",
    "#   -1   0.1475   1   0   0   1   0   1   0\n",
    "#    1   0.2150   0   1   0   2   1   0   0\n",
    "#   -1   0.2400   0   1   0   2   1   0   0\n",
    "#   -1   0.1375   0   0   1   0   0   0   1\n",
    "#    1   0.2200   1   0   0   2   1   0   0\n",
    "#   -1   0.1150   0   0   1   0   0   1   0\n",
    "#    1   0.1825   0   0   1   2   0   1   0\n",
    "#   -1   0.3225   1   0   0   3   0   0   1\n",
    "#   -1   0.1450   0   0   1   0   0   0   1\n",
    "#    1   0.1675   0   1   0   1   1   0   0\n",
    "#    1   0.3325   0   0   1   3   0   1   0\n",
    "#    1   0.1075   1   0   0   0   0   0   1\n",
    "#   -1   0.1350   0   0   1   0   1   0   0\n",
    "#   -1   0.1450   0   0   1   0   1   0   0\n",
    "#    1   0.1575   0   1   0   1   1   0   0\n",
    "#   -1   0.1825   0   1   0   1   0   0   1\n",
    "#   -1   0.2450   0   1   0   2   0   1   0\n",
    "#    1   0.1425   1   0   0   1   1   0   0\n",
    "#    1   0.2175   0   1   0   2   0   0   1\n",
    "#    1   0.2325   0   1   0   2   0   1   0\n",
    "#   -1   0.2875   1   0   0   3   1   0   0\n",
    "#    1   0.2625   0   1   0   2   0   0   1\n",
    "#    1   0.1575   0   1   0   1   0   0   1\n",
    "#    1   0.2750   0   0   1   2   1   0   0\n",
    "#   -1   0.2500   0   1   0   2   1   0   0\n",
    "#   -1   0.2400   0   1   0   2   0   1   0\n",
    "#    1   0.1100   1   0   0   0   0   0   1\n",
    "#   -1   0.2975   1   0   0   3   0   0   1\n",
    "#   -1   0.1725   0   0   1   1   1   0   0\n",
    "#    1   0.3225   0   0   1   3   1   0   0\n",
    "#   -1   0.1450   0   0   1   0   0   0   1\n",
    "#    1   0.1725   0   1   0   1   0   1   0\n",
    "#    1   0.3050   0   0   1   3   1   0   0\n",
    "#   -1   0.3200   1   0   0   3   0   0   1\n",
    "#    1   0.1450   1   0   0   1   1   0   0\n",
    "#   -1   0.3175   1   0   0   3   0   1   0\n",
    "#    1   0.1475   1   0   0   1   0   1   0\n",
    "#    1   0.2575   0   1   0   2   1   0   0\n",
    "#    1   0.1200   1   0   0   1   0   0   1\n",
    "#   -1   0.2425   0   1   0   2   0   1   0\n",
    "#   -1   0.0900   1   0   0   0   1   0   0\n",
    "#   -1   0.0925   0   0   1   0   1   0   0\n",
    "#   -1   0.1650   0   0   1   1   0   1   0\n",
    "#    1   0.1025   1   0   0   0   0   0   1\n",
    "#   -1   0.1475   0   0   1   0   0   0   1\n",
    "#    1   0.2225   1   0   0   2   0   0   1\n",
    "#    1   0.3250   1   0   0   3   0   0   1\n",
    "#    1   0.2800   0   0   1   2   1   0   0\n",
    "#    1   0.2625   0   1   0   2   0   0   1\n",
    "#    1   0.1450   1   0   0   1   0   1   0\n",
    "#    1   0.2350   0   1   0   2   0   1   0\n",
    "#   -1   0.3425   0   0   1   3   1   0   0\n",
    "#   -1   0.1575   0   1   0   1   0   0   1\n",
    "#   -1   0.3075   0   0   1   2   0   1   0\n",
    "#   -1   0.0950   0   0   1   0   0   1   0\n",
    "#   -1   0.1925   0   1   0   1   0   0   1\n",
    "#    1   0.1300   1   0   0   1   1   0   0\n",
    "#   -1   0.3075   1   0   0   3   0   1   0\n",
    "#   -1   0.2000   0   1   0   1   1   0   0\n",
    "#    1   0.2475   0   1   0   3   1   0   0\n",
    "#   -1   0.2825   1   0   0   3   1   0   0\n",
    "#    1   0.2425   0   1   0   3   0   1   0\n",
    "#   -1   0.2625   0   0   1   2   1   0   0\n",
    "#    1   0.0900   1   0   0   0   1   0   0\n",
    "#    1   0.2800   0   0   1   2   0   0   1\n",
    "#    1   0.2600   0   1   0   2   0   1   0\n",
    "#    1   0.0900   0   1   0   0   0   1   0\n",
    "#    1   0.2900   0   0   1   3   1   0   0\n",
    "#    1   0.1950   0   1   0   2   0   1   0\n",
    "#    1   0.2325   0   1   0   2   1   0   0\n",
    "#    1   0.2025   0   1   0   1   0   1   0\n",
    "#    1   0.3025   0   0   1   3   1   0   0\n",
    "#   -1   0.1800   0   0   1   1   0   1   0\n",
    "#   -1   0.2225   0   1   0   2   1   0   0\n",
    "#   -1   0.1425   0   0   1   0   1   0   0\n",
    "#   -1   0.2725   1   0   0   2   0   0   1\n",
    "#  \n",
    "# houses_test_ord.txt                          \n",
    "#    1   0.2550   0   1   0   2   1   0   0\n",
    "#    1   0.1625   0   1   0   1   0   1   0\n",
    "#   -1   0.2750   1   0   0   2   1   0   0\n",
    "#   -1   0.1275   0   0   1   0   0   0   1\n",
    "#   -1   0.1650   0   0   1   1   0   0   1\n",
    "#    1   0.1450   1   0   0   1   0   1   0\n",
    "#   -1   0.3275   1   0   0   3   1   0   0\n",
    "#    1   0.2175   0   1   0   2   0   1   0\n",
    "#    1   0.2725   0   0   1   2   0   1   0\n",
    "#   -1   0.3075   1   0   0   3   0   1   0\n",
    "#   -1   0.2600   1   0   0   2   0   1   0\n",
    "#   -1   0.1525   0   0   1   0   0   1   0\n",
    "#   -1   0.1450   0   0   1   0   1   0   0\n",
    "#    1   0.2375   0   1   0   2   0   0   1\n",
    "#   -1   0.1950   0   1   0   1   0   1   0\n",
    "#   -1   0.2375   0   1   0   2   0   0   1\n",
    "#    1   0.2475   0   1   0   2   1   0   0\n",
    "#    1   0.3150   0   0   1   3   0   0   1\n",
    "#    1   0.1525   1   0   0   1   1   0   0\n",
    "#    1   0.3050   0   0   1   3   0   0   1\n",
    "#    1   0.2350   0   1   0   2   0   0   1\n",
    "#   -1   0.1525   0   0   1   0   0   0   1\n",
    "#    1   0.2550   0   1   0   2   0   0   1\n",
    "#    1   0.1200   0   1   0   1   1   0   0\n",
    "#    1   0.2450   0   1   0   2   1   0   0\n",
    "#   -1   0.3300   1   0   0   3   0   0   1\n",
    "#    1   0.3275   1   0   0   3   1   0   0\n",
    "#    1   0.2300   1   0   0   2   0   1   0\n",
    "#    1   0.2275   0   1   0   2   0   0   1\n",
    "#    1   0.2350   1   0   0   2   1   0   0\n",
    "#    1   0.1475   1   0   0   1   1   0   0\n",
    "#    1   0.2850   0   0   1   3   0   0   1\n",
    "#    1   0.1000   0   0   1   0   1   0   0\n",
    "#    1   0.1750   0   1   0   1   1   0   0\n",
    "#    1   0.3075   0   0   1   3   0   0   1\n",
    "#    1   0.1550   0   1   0   1   0   0   1\n",
    "#   -1   0.0925   0   0   1   0   1   0   0\n",
    "#   -1   0.1300   0   0   1   0   0   0   1\n",
    "#    1   0.1425   0   0   1   1   1   0   0\n",
    "#    1   0.2975   0   0   1   3   0   0   1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_fall_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
