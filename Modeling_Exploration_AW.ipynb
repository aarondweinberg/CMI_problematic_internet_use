{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goals**\n",
    "\n",
    "1. Set up a pipeline to incorporate the imputation\n",
    "2. Do a random forest regressor to identify important features\n",
    "3. Do a test run with one model (linear, most likely) that computes:\n",
    "    - MSE for predicting PCIAT-Total\n",
    "    - MSE for predicting sii when computed from predicted PCIAT-Total\n",
    "    - MSE for predicting sii directly\n",
    "    - kappa for predicting sii when computed from predicted PCIAT-Total\n",
    "    - kappa for predicting sii directly\n",
    "4. After getting the model working, measure these things for out-of-the box:\n",
    "    - multiple linear regression\n",
    "    - knn regression\n",
    "    - random forest\n",
    "    - support vector\n",
    "    - gradient boost\n",
    "    - adaboost\n",
    "    - xgboost\n",
    "5. After identifying a promising out-of-the-box model, try tuning it\n",
    "6. Try implementing a sequential predictor (either logistic regression or random forest) that:\n",
    "    - Starts by predicting 3's vs. non-threes\n",
    "    - Predicts 2's vs. non-twos from the remaining cases\n",
    "    - etc.\n",
    "7. Try using different models for doing this sequential prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from CustomImputers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the Data**\n",
    "\n",
    "For the purpose of developing our model(s), we'll work with data that include the imputed outcome (PCIAT_Total and/or sii) scores AND have cleaned predictors.\n",
    "\n",
    "In the final version of our code, we'll work with data with cleaned predictors but won't have any access to the outcome scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the cleaned & outcome-imputed data\n",
    "train_cleaned=pd.read_csv('train_cleaned_outcome_imputed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an initial list of predictor and outcome columns\n",
    "\n",
    "predictors = train_cleaned.columns.tolist()\n",
    "if 'id' in predictors:\n",
    "    predictors.remove('id')\n",
    "if 'sii' in predictors:\n",
    "    predictors.remove('sii')\n",
    "predictors = [x for x in predictors if 'PCIAT' not in x]\n",
    "predictors = [x for x in predictors if 'Season' not in x]\n",
    "\n",
    "outcome_pciat = ['PCIAT-PCIAT_Total']\n",
    "outcome_sii = ['sii']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Constructing a Random Forest for Feature Identification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Basic_Demos-Age</td>\n",
       "      <td>0.137804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Physical-Height</td>\n",
       "      <td>0.126698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>PreInt_EduHx-computerinternet_hoursday</td>\n",
       "      <td>0.118666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>BIA-BIA_FFM</td>\n",
       "      <td>0.077628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>SDS-SDS_Total_Raw</td>\n",
       "      <td>0.074039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Physical-Weight</td>\n",
       "      <td>0.072494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ENMO_Avg_Active_Days_MVPA110</td>\n",
       "      <td>0.065296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>FGC-FGC_CU</td>\n",
       "      <td>0.055829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>BIA-BIA_FFMI</td>\n",
       "      <td>0.023911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>FGC-FGC_PU</td>\n",
       "      <td>0.023766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>BIA-BIA_Fat</td>\n",
       "      <td>0.023610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Physical-BMI</td>\n",
       "      <td>0.018582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>PAQ_Total</td>\n",
       "      <td>0.018188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>FGC-FGC_TL</td>\n",
       "      <td>0.015412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Fitness_Endurance-Max_Stage</td>\n",
       "      <td>0.014949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Physical-Waist_Circumference</td>\n",
       "      <td>0.013865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>FGC-FGC_SR</td>\n",
       "      <td>0.013734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CGAS-CGAS_Score</td>\n",
       "      <td>0.012663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>BIA-BIA_FMI</td>\n",
       "      <td>0.011455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ENMO_Avg_Active_Days_MVPA192</td>\n",
       "      <td>0.011191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Physical-Systolic_BP</td>\n",
       "      <td>0.011164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Positive_Anglez_Active_Days</td>\n",
       "      <td>0.010808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Fitness_Endurance_Total_Time_Sec</td>\n",
       "      <td>0.009825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Physical-HeartRate</td>\n",
       "      <td>0.008186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>BIA-BIA_Activity_Level_num</td>\n",
       "      <td>0.007720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Physical-Diastolic_BP</td>\n",
       "      <td>0.007447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>BIA-BIA_Frame_num</td>\n",
       "      <td>0.006813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Basic_Demos-Sex</td>\n",
       "      <td>0.004352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>FGC-FGC_TL_Zone</td>\n",
       "      <td>0.001435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>FGC-FGC_CU_Zone</td>\n",
       "      <td>0.001110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>FGC-FGC_PU_Zone</td>\n",
       "      <td>0.000719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>FGC-FGC_SR_Zone</td>\n",
       "      <td>0.000643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>PAQ_Zone</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   feature  importance_score\n",
       "0                          Basic_Demos-Age          0.137804\n",
       "4                          Physical-Height          0.126698\n",
       "24  PreInt_EduHx-computerinternet_hoursday          0.118666\n",
       "18                             BIA-BIA_FFM          0.077628\n",
       "23                       SDS-SDS_Total_Raw          0.074039\n",
       "5                          Physical-Weight          0.072494\n",
       "26            ENMO_Avg_Active_Days_MVPA110          0.065296\n",
       "11                              FGC-FGC_CU          0.055829\n",
       "19                            BIA-BIA_FFMI          0.023911\n",
       "13                              FGC-FGC_PU          0.023766\n",
       "21                             BIA-BIA_Fat          0.023610\n",
       "3                             Physical-BMI          0.018582\n",
       "30                               PAQ_Total          0.018188\n",
       "15                              FGC-FGC_TL          0.015412\n",
       "10             Fitness_Endurance-Max_Stage          0.014949\n",
       "6             Physical-Waist_Circumference          0.013865\n",
       "28                              FGC-FGC_SR          0.013734\n",
       "2                          CGAS-CGAS_Score          0.012663\n",
       "20                             BIA-BIA_FMI          0.011455\n",
       "25            ENMO_Avg_Active_Days_MVPA192          0.011191\n",
       "9                     Physical-Systolic_BP          0.011164\n",
       "27             Positive_Anglez_Active_Days          0.010808\n",
       "32        Fitness_Endurance_Total_Time_Sec          0.009825\n",
       "8                       Physical-HeartRate          0.008186\n",
       "17              BIA-BIA_Activity_Level_num          0.007720\n",
       "7                    Physical-Diastolic_BP          0.007447\n",
       "22                       BIA-BIA_Frame_num          0.006813\n",
       "1                          Basic_Demos-Sex          0.004352\n",
       "16                         FGC-FGC_TL_Zone          0.001435\n",
       "12                         FGC-FGC_CU_Zone          0.001110\n",
       "14                         FGC-FGC_PU_Zone          0.000719\n",
       "29                         FGC-FGC_SR_Zone          0.000643\n",
       "31                                PAQ_Zone          0.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "\n",
    "pipe_mice = Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "                    ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                    ('rf', RandomForestRegressor(n_estimators = 300, max_features = 'sqrt', max_depth = 5, random_state = 216))])\n",
    "\n",
    "pipe_mice.fit(train_cleaned[predictors],train_cleaned['PCIAT-PCIAT_Total'])\n",
    "\n",
    "train_pred_mice = pipe_mice.predict(train_cleaned[predictors])\n",
    "\n",
    "#Get feature importance from the rf inside pipe\n",
    "score_mice_df = pd.DataFrame({'feature':train_cleaned[predictors].columns,\n",
    "                            'importance_score': pipe_mice.named_steps['rf'].feature_importances_})\n",
    "\n",
    "score_mice_df.sort_values('importance_score',ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyfeatures = ['Basic_Demos-Age',\n",
    " 'Physical-Height',\n",
    " 'PreInt_EduHx-computerinternet_hoursday',\n",
    " 'BIA-BIA_FFM',\n",
    " 'SDS-SDS_Total_Raw',\n",
    " 'Physical-Weight',\n",
    " 'ENMO_Avg_Active_Days_MVPA110',\n",
    " 'FGC-FGC_CU']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Constructing some Linear Models**\n",
    "\n",
    "In this section, I'll make linear models with:\n",
    "* A single predictor (hours spent on the internet)\n",
    "* A small number of predictors (taken from the importance scores generated above)\n",
    "* All the predictors\n",
    "\n",
    "Each of these will be run through a KFold split with a 20% validation set; for each model we'll compute several stats to compare the predictions with PCIAT scores and also with sii scores:\n",
    "* MSE\n",
    "* kappa\n",
    "\n",
    "Note: Column selector documented here: https://stackoverflow.com/questions/62416223/how-to-select-only-few-columns-in-scikit-learn-column-selector-pipeline\n",
    "\n",
    "Note: custom loss functions for linear models are documented here: https://alexmiller.phd/posts/linear-model-custom-loss-function-regularization-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(334.7045879129308)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First I'll see if I can get a pipe set up to do prediction on a split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "train_tt, train_ho = train_test_split(train_cleaned, test_size=0.2)\n",
    "\n",
    "slr = Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "                ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                ('selector', ColumnTransformer([('selector', 'passthrough', ['PreInt_EduHx-computerinternet_hoursday'])], remainder=\"drop\")),\n",
    "                ('linear', LinearRegression())])\n",
    "\n",
    "slr.fit(train_tt[predictors], train_tt['PCIAT-PCIAT_Total'])\n",
    "mean_squared_error(train_ho['PCIAT-PCIAT_Total'], slr.predict(train_ho))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse for {'slr_pipe'}  for predicting PCIAT: 338.05763334754164\n",
      "mse for {'mlr_key_pipe'}  for predicting PCIAT: 286.5867728570368\n",
      "mse for {'mlr_all_pipe'}  for predicting PCIAT: 279.5746351811864\n",
      "mse for {'knn_pipe'}  for predicting PCIAT: 328.532522095672\n",
      "mse for {'svr_pipe'}  for predicting PCIAT: 335.7269738263128\n",
      "mse for {'rf_pipe'}  for predicting PCIAT: 292.73626295475833\n",
      "mse for {'ada_pipe'}  for predicting PCIAT: 314.5687564410354\n",
      "mse for {'grad_pipe'}  for predicting PCIAT: 275.7163715653296\n",
      "mse for {'xgb_pipe'}  for predicting PCIAT: 327.7242209269396\n",
      "mse for {'slr_pipe'}  for predicting sii: 0.5242391554928125\n",
      "mse for {'mlr_key_pipe'}  for predicting sii: 0.45133623223521907\n",
      "mse for {'mlr_all_pipe'}  for predicting sii: 0.45038779965994613\n",
      "mse for {'knn_pipe'}  for predicting sii: 0.5196583143507972\n",
      "mse for {'svr_pipe'}  for predicting sii: 0.5008264204733712\n",
      "mse for {'rf_pipe'}  for predicting sii: 0.4534406772968868\n",
      "mse for {'ada_pipe'}  for predicting sii: 0.5364020513967693\n",
      "mse for {'grad_pipe'}  for predicting sii: 0.4446319280974969\n",
      "mse for {'xgb_pipe'}  for predicting sii: 0.5404081404607434\n",
      "kappa for {'slr_pipe'}  for predicting sii: 0.24032897307556333\n",
      "kappa for {'mlr_key_pipe'}  for predicting sii: 0.3523607225714829\n",
      "kappa for {'mlr_all_pipe'}  for predicting sii: 0.34142184189174196\n",
      "kappa for {'knn_pipe'}  for predicting sii: 0.3066630145289033\n",
      "kappa for {'svr_pipe'}  for predicting sii: 0.3474277532050698\n",
      "kappa for {'rf_pipe'}  for predicting sii: 0.3837251825549435\n",
      "kappa for {'ada_pipe'}  for predicting sii: 0.1756851807678429\n",
      "kappa for {'grad_pipe'}  for predicting sii: 0.3981520950219468\n",
      "kappa for {'xgb_pipe'}  for predicting sii: 0.3452879767061998\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Next either stick this in a kfold split or use cross_val_score\n",
    "\n",
    "train_tt, train_ho = train_test_split(train_cleaned, test_size=0.2)\n",
    "\n",
    "models = {\n",
    "'slr_pipe' : Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "                ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                ('selector', ColumnTransformer([('selector', 'passthrough', ['PreInt_EduHx-computerinternet_hoursday'])], remainder=\"drop\")),\n",
    "                ('linear', LinearRegression())]),\n",
    "\n",
    "'mlr_key_pipe' : Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "                ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                ('selector', ColumnTransformer([('selector', 'passthrough', keyfeatures)], remainder=\"drop\")),\n",
    "                ('linear', LinearRegression())]),\n",
    "\n",
    "'mlr_all_pipe' : Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "                ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                ('linear', LinearRegression())]),\n",
    "\n",
    "'knn_pipe' : Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "                ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                ('knn', KNeighborsRegressor(10))]),\n",
    "\n",
    "'svr_pipe' : Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "                ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                ('rf', SVR())]),\n",
    "\n",
    "'rf_pipe' : Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "                ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                ('rf', RandomForestRegressor())]),\n",
    "\n",
    "'ada_pipe' : Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "                ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                ('ada', AdaBoostRegressor())]),\n",
    "\n",
    "'grad_pipe' : Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "                ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                ('grad', GradientBoostingRegressor())]),\n",
    "\n",
    "'xgb_pipe' : Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "                ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                ('xgb', XGBRegressor())])\n",
    "}\n",
    "\n",
    "for pipeline_name, pipeline_obj in models.items():\n",
    "    # print(f\"Pipeline: {pipeline_name}\")\n",
    "    # Perform some operation on the pipeline, e.g., fit, predict, evaluate\n",
    "    pipeline_obj.fit(train_tt[predictors], train_tt['PCIAT-PCIAT_Total'])\n",
    "    pred = pipeline_obj.predict(train_ho[predictors])\n",
    "    mse = mean_squared_error(train_ho['PCIAT-PCIAT_Total'], pred)\n",
    "    print('mse for', {pipeline_name},' for predicting PCIAT:',mse)\n",
    "    #print(f\"Pipeline {pipeline_name} predictions: {y_pred}\")\n",
    "\n",
    "for pipeline_name, pipeline_obj in models.items():\n",
    "    # print(f\"Pipeline: {pipeline_name}\")\n",
    "    # Perform some operation on the pipeline, e.g., fit, predict, evaluate\n",
    "    pipeline_obj.fit(train_tt[predictors], train_tt['sii'])\n",
    "    pred = pipeline_obj.predict(train_ho[predictors])\n",
    "    mse = mean_squared_error(train_ho['sii'], pred)\n",
    "    print('mse for', {pipeline_name},' for predicting sii:',mse)\n",
    "    #print(f\"Pipeline {pipeline_name} predictions: {y_pred}\")\n",
    "\n",
    "for pipeline_name, pipeline_obj in models.items():\n",
    "    # print(f\"Pipeline: {pipeline_name}\")\n",
    "    # Perform some operation on the pipeline, e.g., fit, predict, evaluate\n",
    "    pipeline_obj.fit(train_tt[predictors], train_tt['PCIAT-PCIAT_Total'])\n",
    "    pred = pipeline_obj.predict(train_ho[predictors])\n",
    "\n",
    "    bins = [0, 30, 49,79,100]\n",
    "    labels = [0,1,2,3]\n",
    "    train_imp_KNN['sii'] = pd.cut(train_imp_KNN['PCIAT-PCIAT_Total'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "    mse = mean_squared_error(train_ho['PCIAT-PCIAT_Total'], pred)\n",
    "    print('mse for', {pipeline_name},' for predicting sii computed from PCIAT:',mse)\n",
    "    #print(f\"Pipeline {pipeline_name} predictions: {y_pred}\")\n",
    "\n",
    "##Also see if we can compute sii from PCIAT and then compare to actual sii\n",
    "\n",
    "0 ~ 30\tNone\t0\n",
    "31 ~ 49\tMild\t1\n",
    "50 ~ 79\tModerate\t2\n",
    "80-100\tSevere\t3\n",
    "\n",
    "for pipeline_name, pipeline_obj in models.items():\n",
    "    # print(f\"Pipeline: {pipeline_name}\")\n",
    "    # Perform some operation on the pipeline, e.g., fit, predict, evaluate\n",
    "    pipeline_obj.fit(train_tt[predictors], train_tt['sii'])\n",
    "    pred = pipeline_obj.predict(train_ho[predictors])\n",
    "    # round the predictor values\n",
    "    pred = np.round(pred)\n",
    "    kappa = cohen_kappa_score(train_ho['sii'], pred, weights='quadratic')\n",
    "    print('kappa for', {pipeline_name},' for predicting sii with regular rounding:',kappa)\n",
    "    #print(f\"Pipeline {pipeline_name} predictions: {y_pred}\")\n",
    "\n",
    "    for pipeline_name, pipeline_obj in models.items():\n",
    "    # print(f\"Pipeline: {pipeline_name}\")\n",
    "    # Perform some operation on the pipeline, e.g., fit, predict, evaluate\n",
    "    pipeline_obj.fit(train_tt[predictors], train_tt['sii'])\n",
    "    pred = pipeline_obj.predict(train_ho[predictors])\n",
    "    # round the predictor values up\n",
    "    pred = np.ceil(pred)\n",
    "    kappa = cohen_kappa_score(train_ho['sii'], pred, weights='quadratic')\n",
    "    print('kappa for', {pipeline_name},' for predicting sii with ceiling rounding:',kappa)\n",
    "    #print(f\"Pipeline {pipeline_name} predictions: {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mlxtend'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneighbors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KNeighborsRegressor\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmlxtend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ColumnSelector\n\u001b[1;32m      9\u001b[0m partial_feature_list \u001b[38;5;241m=\u001b[39m ordered_feature_list[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m8\u001b[39m]\n\u001b[1;32m     10\u001b[0m full_feature_list \u001b[38;5;241m=\u001b[39m predictors\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mlxtend'"
     ]
    }
   ],
   "source": [
    "# Import statements\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "partial_feature_list = keyfeatures\n",
    "full_feature_list = predictors\n",
    "\n",
    "## Make a KFold object\n",
    "## remember to set a random_state and set shuffle = True\n",
    "num_splits = 5\n",
    "num_models = 4\n",
    "kfold = KFold(num_splits,\n",
    "              random_state = 216,\n",
    "              shuffle=True)\n",
    "\n",
    "## This array will hold the mse for each model and split\n",
    "mses = np.zeros((num_models, num_splits))\n",
    "\n",
    "## sets a split counter\n",
    "i = 0\n",
    "\n",
    "## loop through the kfold here\n",
    "for train_index, test_index in kfold.split(train_cleaned):\n",
    "        print('split number:', i)\n",
    "        ## cv training set\n",
    "        train_tt = train_cleaned.iloc[train_index]\n",
    "\n",
    "        ## cv holdout set\n",
    "        train_ho = train_cleaned.iloc[test_index]\n",
    "\n",
    "        #cusmouse = Custom_MICE_Imputer()\n",
    "        #train_tt_imputed = cusmouse.fit_transform(train_tt)\n",
    "        #train_ho_imputed = cusmouse.fit_transform(train_ho)\n",
    "\n",
    "        #train_tt_imputed_zoned = zone_encoder(train_tt_imputed)\n",
    "        #train_ho_imputed_zoned = zone_encoder(train_ho_imputed)\n",
    "\n",
    "        ## Fit and get ho mse for slr model with one predictor\n",
    "        slr = Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "                        ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                        (\"selector\", ColumnTransformer([(\"selector\", \"passthrough\", ['PreInt_EduHx-computerinternet_hoursday', 'PCIAT-PCIAT_Total'])], remainder=\"drop\"),\n",
    "                        ('linear', LinearRegression())])\n",
    "\n",
    "        ###Now that the pipe is established, need to use it to fit and transform the data, then predict...\n",
    "        slr.fit_transform(train_tt[full_feature_list], train_tt['PCIAT-PCIAT_Total'])\n",
    "        slr.transform(train_ho)\n",
    "        #slr = LinearRegression()\n",
    "        #slr.fit(train_tt_imputed_zoned[['PreInt_EduHx-computerinternet_hoursday']],train_tt_imputed_zoned['PCIAT-PCIAT_Total'])\n",
    "\n",
    "        #mses[0, i] = mean_squared_error(train_ho_imputed_zoned['PCIAT-PCIAT_Total'], slr.predict(train_ho_imputed_zoned[['PreInt_EduHx-computerinternet_hoursday']]))\n",
    "        mses[0, i] = mean_squared_error(train_ho['PCIAT-PCIAT_Total'], slr.predict(train_ho[['PreInt_EduHx-computerinternet_hoursday']]))\n",
    "\n",
    "        ## Fit and get ho mse for mlr model with the partial_feature_list as predictors\n",
    "        #mlr_partial = Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "        #            ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "        #            ('linear', LinearRegression())])\n",
    "\n",
    "        mlr_partial = Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "                        ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                        (\"selector\", ColumnTransformer([(\"selector\", \"passthrough\", ['PreInt_EduHx-computerinternet_hoursday', 'PCIAT-PCIAT_Total'])], remainder=\"drop\"),\n",
    "                        ('linear', LinearRegression())])\n",
    "        mlr_partial.fit(train_tt_imputed_zoned[partial_feature_list],train_tt_imputed_zoned['PCIAT-PCIAT_Total'])\n",
    "\n",
    "        slr.fit_transform(train_tt[full_feature_list], train_tt['PCIAT-PCIAT_Total'])\n",
    "        slr.transform(train_ho)\n",
    "\n",
    "        #mses[0, i] = mean_squared_error(train_ho_imputed_zoned['PCIAT-PCIAT_Total'], slr.predict(train_ho_imputed_zoned[['PreInt_EduHx-computerinternet_hoursday']]))\n",
    "        mses[0, i] = mean_squared_error(train_ho['PCIAT-PCIAT_Total'], slr.predict(train_ho[['PreInt_EduHx-computerinternet_hoursday']]))\n",
    "\n",
    "\n",
    "        mses[1, i] = mean_squared_error(train_ho_imputed_zoned['PCIAT-PCIAT_Total'], mlr_partial.predict(train_ho_imputed_zoned[partial_feature_list]))\n",
    "\n",
    "        ## Fit and get ho mse for mlr model with the partial_feature_list as predictors\n",
    "        mlr_full = Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "                ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                ('linear', LinearRegression())])\n",
    "\n",
    "        mlr_full.fit(train_tt[full_feature_list],train_tt['PCIAT-PCIAT_Total'])\n",
    "\n",
    "        mses[2, i] = mean_squared_error(train_ho['PCIAT-PCIAT_Total'], mlr_full.predict(train_ho[full_feature_list]))\n",
    "\n",
    "        ## Fit and get ho mse for the knn model\n",
    "        knn = Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "                ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                ('scale', StandardScaler()),\n",
    "                ('knn', KNeighborsRegressor(10))])\n",
    "\n",
    "        knn.fit(train_tt[full_feature_list],train_tt['PCIAT-PCIAT_Total'])\n",
    "\n",
    "        mses[3, i] = mean_squared_error(train_ho['PCIAT-PCIAT_Total'], knn.predict(train_ho[full_feature_list]))\n",
    "\n",
    "        i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[331.24377889, 387.07280543, 356.84356852, 369.71754825,\n",
       "        344.97431659],\n",
       "       [282.34928638, 324.38579434, 316.24781709, 316.9878989 ,\n",
       "        280.2742685 ],\n",
       "       [278.48984768, 320.26990037, 314.9553243 , 320.71222831,\n",
       "        281.62765357],\n",
       "       [352.00947153, 373.34192802, 350.69734943, 348.904441  ,\n",
       "        308.94042466]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a column to mses that is the mean of the first num_splits columns\n",
    "mses = np.hstack((mses, mses.mean(axis=1).reshape(-1,1)))\n",
    "\n",
    "#Convert mses to a dataframe. Label the rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(models, pred_data, out_data, iterations = 100):\n",
    "    results = {}\n",
    "    for i in models:\n",
    "        mse_train = []\n",
    "        mse_test = []\n",
    "        for j in range(iterations):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(pred_data, \n",
    "                                                                out_data, \n",
    "                                                                test_size= 0.2)\n",
    "            mse_test.append(metrics.mean_squared_error(y_test,\n",
    "                                            models[i].fit(X_train, \n",
    "                                                         y_train).predict(X_test)))\n",
    "            mse_train.append(metrics.mean_squared_error(y_train, \n",
    "                                             models[i].fit(X_train, \n",
    "                                                          y_train).predict(X_train)))\n",
    "        results[i] = [np.mean(mse_train), np.mean(mse_test)]\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Construct the pipes\n",
    "pipe_linear = Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "                    ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                    ('linear', LinearRegression())])\n",
    "\n",
    "\n",
    "#Iterate through models?\n",
    "\n",
    "models = {'OLS': linear_model.LinearRegression(),\n",
    "           'Lasso': GridSearchCV(linear_model.Lasso(), \n",
    "                               param_grid=lasso_params).fit(df[X], df[Y]).best_estimator_,\n",
    "           'Ridge': GridSearchCV(linear_model.Ridge(), \n",
    "                               param_grid=ridge_params).fit(df[X], df[Y]).best_estimator_,}\n",
    "\n",
    "test(models, train_cleaned[predictors], train_cleaned['PCIAT-PCIAT_Total'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sequential Binary Classification**\n",
    "\n",
    "It looks like our attempts so far have under-predicted sii values of 2 and 3. I'm going to try to implement a method that first predicts whether or not the sii value is 3, then on the remaining values predict whether or not they are 2, etc.\n",
    "\n",
    "I came up with this idea myself, but I wasn't the first one to do it. It was described on Medium: https://towardsdatascience.com/simple-trick-to-train-an-ordinal-regression-with-any-classifier-6911183d2a3c from an article by Frank and Hal\n",
    "\n",
    "Also described on stackoverflow: https://stackoverflow.com/questions/57561189/multi-class-multi-label-ordinal-classification-with-sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class OrdinalClassifier(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, clf):\n",
    "        self.clf = clf\n",
    "        self.clfs = {}\n",
    "        self.unique_class = np.NaN\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.unique_class = np.sort(np.unique(y))\n",
    "        if self.unique_class.shape[0] > 2:\n",
    "            for i in range(self.unique_class.shape[0]-1):\n",
    "                # for each k - 1 ordinal value we fit a binary classification problem\n",
    "                binary_y = (y > self.unique_class[i]).astype(np.uint8)\n",
    "                clf = clone(self.clf)\n",
    "                clf.fit(X, binary_y)\n",
    "                self.clfs[i] = clf\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        clfs_predict = {i: self.clfs[i].predict_proba(X) for i in self.clfs}\n",
    "        predicted = []\n",
    "        k = len(self.unique_class) - 1\n",
    "        for i, y in enumerate(self.unique_class):\n",
    "            if i == 0:\n",
    "                # V1 = 1 - Pr(y > V1)\n",
    "                predicted.append(1 - clfs_predict[0][:,1])\n",
    "            elif i < k:\n",
    "                # Vi = Pr(y <= Vi) * Pr(y > Vi-1)\n",
    "                 predicted.append((1 - clfs_predict[i][:,1]) * clfs_predict[i-1][:,1])\n",
    "            else:\n",
    "                # Vk = Pr(y > Vk-1)\n",
    "                predicted.append(clfs_predict[k-1][:,1])\n",
    "        return np.vstack(predicted).T\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.unique_class[np.argmax(self.predict_proba(X), axis=1)]\n",
    "\n",
    "    def score(self, X, y, sample_weight=None):\n",
    "        return accuracy_score(y, self.predict(X), sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using LASSO for Feature Selection**\n",
    "\n",
    "First, we'll try using LASSO to identify important features.\n",
    "\n",
    "Note that it isn't possible to use LASSO with pipelines (see https://stackoverflow.com/questions/39466671/use-of-scaler-with-lassocv-ridgecv). So we'll need to do the hyperparameter tuning manually.\n",
    "\n",
    "Some of the code below was suggested by Ali Furkan Kalay: https://alfurka.github.io/2018-11-18-grid-search/\n",
    "\n",
    "Some of the code below was suggested on Medium: https://medium.com/geekculture/regularization-using-pipeline-gridsearchcv-f377946e39d1\n",
    "\n",
    "Some of the code below was suggested on geeksforgeeks (https://www.geeksforgeeks.org/feature-selection-using-selectfrommodel-and-lassocv-in-scikit-learn/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tuning Lasso inside a Pipe with GridSearchCV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
      "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 455, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1004, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 848, in score\n",
      "    y_pred = self.predict(X)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 306, in predict\n",
      "    return self._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py\", line 1147, in _decision_function\n",
      "    return super()._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 285, in _decision_function\n",
      "    X = self._validate_data(X, accept_sparse=[\"csr\", \"csc\", \"coo\"], reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 633, in _validate_data\n",
      "    out = check_array(X, input_name=\"X\", **check_params)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1064, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains NaN.\n",
      "Lasso does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
      "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 455, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1004, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 848, in score\n",
      "    y_pred = self.predict(X)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 306, in predict\n",
      "    return self._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py\", line 1147, in _decision_function\n",
      "    return super()._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 285, in _decision_function\n",
      "    X = self._validate_data(X, accept_sparse=[\"csr\", \"csc\", \"coo\"], reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 633, in _validate_data\n",
      "    out = check_array(X, input_name=\"X\", **check_params)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1064, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains NaN.\n",
      "Lasso does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
      "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 455, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1004, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 848, in score\n",
      "    y_pred = self.predict(X)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 306, in predict\n",
      "    return self._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py\", line 1147, in _decision_function\n",
      "    return super()._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 285, in _decision_function\n",
      "    X = self._validate_data(X, accept_sparse=[\"csr\", \"csc\", \"coo\"], reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 633, in _validate_data\n",
      "    out = check_array(X, input_name=\"X\", **check_params)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1064, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains NaN.\n",
      "Lasso does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
      "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 455, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1004, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 848, in score\n",
      "    y_pred = self.predict(X)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 306, in predict\n",
      "    return self._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py\", line 1147, in _decision_function\n",
      "    return super()._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 285, in _decision_function\n",
      "    X = self._validate_data(X, accept_sparse=[\"csr\", \"csc\", \"coo\"], reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 633, in _validate_data\n",
      "    out = check_array(X, input_name=\"X\", **check_params)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1064, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains NaN.\n",
      "Lasso does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
      "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 455, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1004, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 848, in score\n",
      "    y_pred = self.predict(X)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 306, in predict\n",
      "    return self._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py\", line 1147, in _decision_function\n",
      "    return super()._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 285, in _decision_function\n",
      "    X = self._validate_data(X, accept_sparse=[\"csr\", \"csc\", \"coo\"], reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 633, in _validate_data\n",
      "    out = check_array(X, input_name=\"X\", **check_params)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1064, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains NaN.\n",
      "Lasso does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
      "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 455, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1004, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 848, in score\n",
      "    y_pred = self.predict(X)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 306, in predict\n",
      "    return self._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py\", line 1147, in _decision_function\n",
      "    return super()._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 285, in _decision_function\n",
      "    X = self._validate_data(X, accept_sparse=[\"csr\", \"csc\", \"coo\"], reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 633, in _validate_data\n",
      "    out = check_array(X, input_name=\"X\", **check_params)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1064, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains NaN.\n",
      "Lasso does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
      "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 455, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1004, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 848, in score\n",
      "    y_pred = self.predict(X)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 306, in predict\n",
      "    return self._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py\", line 1147, in _decision_function\n",
      "    return super()._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 285, in _decision_function\n",
      "    X = self._validate_data(X, accept_sparse=[\"csr\", \"csc\", \"coo\"], reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 633, in _validate_data\n",
      "    out = check_array(X, input_name=\"X\", **check_params)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1064, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains NaN.\n",
      "Lasso does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
      "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 455, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1004, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 848, in score\n",
      "    y_pred = self.predict(X)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 306, in predict\n",
      "    return self._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py\", line 1147, in _decision_function\n",
      "    return super()._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 285, in _decision_function\n",
      "    X = self._validate_data(X, accept_sparse=[\"csr\", \"csc\", \"coo\"], reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 633, in _validate_data\n",
      "    out = check_array(X, input_name=\"X\", **check_params)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1064, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains NaN.\n",
      "Lasso does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.441e+04, tolerance: 4.500e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
      "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 455, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1004, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 848, in score\n",
      "    y_pred = self.predict(X)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 306, in predict\n",
      "    return self._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py\", line 1147, in _decision_function\n",
      "    return super()._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 285, in _decision_function\n",
      "    X = self._validate_data(X, accept_sparse=[\"csr\", \"csc\", \"coo\"], reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 633, in _validate_data\n",
      "    out = check_array(X, input_name=\"X\", **check_params)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1064, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains NaN.\n",
      "Lasso does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.493e+05, tolerance: 4.500e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
      "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 455, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1004, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 848, in score\n",
      "    y_pred = self.predict(X)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 306, in predict\n",
      "    return self._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py\", line 1147, in _decision_function\n",
      "    return super()._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_base.py\", line 285, in _decision_function\n",
      "    X = self._validate_data(X, accept_sparse=[\"csr\", \"csc\", \"coo\"], reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 633, in _validate_data\n",
      "    out = check_array(X, input_name=\"X\", **check_params)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1064, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains NaN.\n",
      "Lasso does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
      "10 fits failed out of a total of 20.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py\", line 980, in fit\n",
      "    X, y = self._validate_data(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py\", line 650, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1301, in check_X_y\n",
      "    X = check_array(\n",
      "        ^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1064, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains NaN.\n",
      "Lasso does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1103: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lasso__alpha': np.float64(5000000000.0)}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "  \n",
    "# Create a list of predictor variables; this eliminates id, sii, PCIAT, and Season variables\n",
    "predictors = train_cleaned.columns.tolist()\n",
    "if 'id' in predictors:\n",
    "    predictors.remove('id')\n",
    "if 'sii' in predictors:\n",
    "    predictors.remove('sii')\n",
    "predictors = [x for x in predictors if 'PCIAT' not in x]\n",
    "predictors = [x for x in predictors if 'Season' not in x]\n",
    "\n",
    "# A list of alpha (lambda) values to try in the hyperparameter tuning\n",
    "# create an array of 10**np.linspace(10,-2,100)*0.5\n",
    "#alphas = {'lasso__alpha': 10**np.linspace(10,-2,100)*0.5}\n",
    "alphas = {'lasso__alpha': 10**np.linspace(10,-2,10)*0.5}\n",
    "\n",
    "# Set up a lasso pipeline\n",
    "lasso_pipe = Pipeline([('impute', Custom_MICE_Imputer()),('fillzones', FunctionTransformer(zone_encoder)), ('lasso', Lasso())])\n",
    "\n",
    "gs_lasso_pipe = GridSearchCV(lasso_pipe, param_grid=alphas, cv=2).fit(train_cleaned[predictors], train_cleaned['PCIAT-PCIAT_Total'])\n",
    "\n",
    "gs_lasso_pipe.best_estimator_\n",
    "gs_lasso_pipe.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.feature_selection import SelectFromModel \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "  \n",
    "# Create a list of predictor variables; this eliminates id, sii, PCIAT, and Season variables\n",
    "predictors = train_cleaned.columns.tolist()\n",
    "if 'id' in predictors:\n",
    "    predictors.remove('id')\n",
    "if 'sii' in predictors:\n",
    "    predictors.remove('sii')\n",
    "predictors = [x for x in predictors if 'PCIAT' not in x]\n",
    "predictors = [x for x in predictors if 'Season' not in x]\n",
    "\n",
    "# Split the data into 80% Train/20% Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_cleaned[predictors], train_cleaned['PCIAT-PCIAT_Total'], test_size=0.2, random_state=216)\n",
    "\n",
    "# A list of alpha (lambda) values to try in the hyperparameter tuning\n",
    "alphas = 10**np.linspace(10,-2,100)*0.5\n",
    "\n",
    "# These will hold our coefficient estimates\n",
    "lasso_coefs = np.empty((len(alpha),n))\n",
    "\n",
    "# Set up a lasso pipeline\n",
    "lasso_pipe = Pipeline([('impute', Custom_MICE_Imputer()),('fillzones', FunctionTransformer(zone_encoder)), ('lasso', Lasso())])\n",
    "\n",
    "GridSearchCV(lasso_pipe, param_grid=alphas).fit(train_cleaned[predictors], train_cleaned['PCIAT-PCIAT_Total']).best_estimator_,\n",
    "\n",
    "def test(models, data, iterations = 100):\n",
    "    results = {}\n",
    "    for i in models:\n",
    "        r2_train = []\n",
    "        r2_test = []\n",
    "        for j in range(iterations):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(data[X], \n",
    "                                                                data[Y], \n",
    "                                                                test_size= 0.2)\n",
    "            r2_test.append(metrics.r2_score(y_test,\n",
    "                                            models[i].fit(X_train, \n",
    "                                                         y_train).predict(X_test)))\n",
    "            r2_train.append(metrics.r2_score(y_train, \n",
    "                                             models[i].fit(X_train, \n",
    "                                                          y_train).predict(X_train)))\n",
    "        results[i] = [np.mean(r2_train), np.mean(r2_test)]\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "models = {'OLS': linear_model.LinearRegression(),\n",
    "           'Lasso': GridSearchCV(linear_model.Lasso(), \n",
    "                               param_grid=lasso_params).fit(df[X], df[Y]).best_estimator_,\n",
    "           'Ridge': GridSearchCV(linear_model.Ridge(), \n",
    "                               param_grid=ridge_params).fit(df[X], df[Y]).best_estimator_,}\n",
    "\n",
    "test(models, df)\n",
    "\n",
    "## for each alpha value\n",
    "for i in range(len(alpha)):\n",
    "    ## set up the lasso pipeline\n",
    "    ## first scale\n",
    "    ## then make polynomial features\n",
    "    ## then fit the lasso regression model\n",
    "    lasso_pipe = Pipeline([('scale',StandardScaler()),\n",
    "                              ('poly',PolynomialFeatures(n, interaction_only=False, include_bias=False)),\n",
    "                              ('lasso', Lasso(alpha=alpha[i], max_iter=5000000))\n",
    "                          ])\n",
    "    \n",
    "    ## fit the lasso\n",
    "    lasso_pipe.fit(x.reshape(-1,1), y)\n",
    "\n",
    "    # record the coefficients\n",
    "    lasso_coefs[i,:] = lasso_pipe['lasso'].coef_\n",
    "\n",
    "\n",
    "# Fit LassoCV model with 5-fold cross-validation. It automatically evaluates performance over several folds in order to get the ideal regularization strength (alpha).\n",
    "lasso_cv = LassoCV(cv=5) \n",
    "lasso_cv.fit(X_train, y_train) \n",
    "\n",
    "# Feature selection. This selects the most significant features from the training and testing sets using the pre-trained lasso_cv model. \n",
    "# Only the features determined to be relevant by the L1 regularization are included in the final selected feature sets\n",
    "# These final selected feature sets are stored in X_train_selected and X_test_selected\n",
    "sfm = SelectFromModel(lasso_cv, prefit=True) \n",
    "X_train_selected = sfm.transform(X_train) \n",
    "X_test_selected = sfm.transform(X_test) \n",
    "\n",
    "# Train a Random Forest Classifier using the selected features \n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42) \n",
    "model.fit(X_train_selected, y_train) \n",
    "\n",
    "\n",
    "# Evaluate the model \n",
    "y_pred = model.predict(X_test_selected) \n",
    "print(classification_report(y_test, y_pred)) \n",
    "\n",
    "# Analyze selected features and their importance \n",
    "selected_feature_indices = np.where(sfm.get_support())[0] \n",
    "selected_features = train.columns[selected_feature_indices] \n",
    "coefficients = lasso_cv.coef_ \n",
    "print(\"Selected Features:\", selected_features) \n",
    "print(\"Feature Coefficients:\", coefficients) \n",
    "\n",
    "# Extract the selected features from the original dataset \n",
    "X_selected_features = X_train[:, selected_feature_indices] \n",
    "\n",
    "# Create a DataFrame for better visualization \n",
    "selected_features_df = pd.DataFrame(X_selected_features, columns=selected_features) \n",
    "\n",
    "# Add the target variable for coloring \n",
    "selected_features_df['target'] = y_train \n",
    "\n",
    "# Plot the two most important features \n",
    "sns.scatterplot(x='mean area', y='worst area', hue='target', data=selected_features_df, palette='viridis') \n",
    "plt.xlabel('Mean Area') \n",
    "plt.ylabel('Worst Area') \n",
    "plt.title('Scatter Plot of Two Most Important Features') \n",
    "plt.show() \n",
    "\n",
    "\n",
    "\n",
    "## This code will allow us to demonstrate the effect of \n",
    "## increasing alpha\n",
    "\n",
    "## set values for alpha\n",
    "alpha = [0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000]\n",
    "\n",
    "## The degree of the polynomial we will fit\n",
    "n=10\n",
    "\n",
    "#$ These will hold our coefficient estimates\n",
    "ridge_coefs = np.empty((len(alpha),n))\n",
    "lasso_coefs = np.empty((len(alpha),n))\n",
    "\n",
    "## for each alpha value\n",
    "for i in range(len(alpha)):\n",
    "    ## set up the lasso pipeline\n",
    "    ## first scale\n",
    "    ## then make polynomial features\n",
    "    ## then fit the lasso regression model\n",
    "    lasso_pipe = Pipeline([('scale',StandardScaler()),\n",
    "                              ('poly',PolynomialFeatures(n, interaction_only=False, include_bias=False)),\n",
    "                              ('lasso', Lasso(alpha=alpha[i], max_iter=5000000))\n",
    "                          ])\n",
    "    \n",
    "    ## fit the lasso\n",
    "    lasso_pipe.fit(x.reshape(-1,1), y)\n",
    "\n",
    "    # record the coefficients\n",
    "    lasso_coefs[i,:] = lasso_pipe['lasso'].coef_\n",
    "\n",
    "\n",
    "# A data frame to store the optimal alpha values\n",
    "bestalphas = pd.DataFrame(index=range(0,len(listofdatasets)))\n",
    "bestalphas['dfname'] = ''\n",
    "bestalphas['best_alpha_manual'] = np.nan\n",
    "bestalphas['best_alpha_automatic'] = np.nan\n",
    "\n",
    "\n",
    "for df in listofdatasets:\n",
    "    X_train = df.drop(columns=['PCIAT-PCIAT_Total'])\n",
    "    y_train = df['PCIAT-PCIAT_Total']\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_std = scaler.transform(X_train)\n",
    "    lassocv = LassoCV(alphas = alphas, scoring = 'neg_root_mean_squared_error')\n",
    "    lassocv.fit(X_std, y_train)\n",
    "    bestalphas.loc[bestalphas['dfname']==df.name,'best_alpha_automatic']=lassocv.alpha_.astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a Pipeline with the Custom Imputer and Transformer**\n",
    "\n",
    "Below is some code that is based on the 2_More_Advanced_Pipelines notebook from optional_extra_practice in Week 3\n",
    "\n",
    "In that code, their desired pipeline was:\n",
    "1 Impute the missing values of `body_mass_g` with the `median` value,\n",
    "2 Impute the missing values of `sex` with the most common value,\n",
    "3 One hot encode `island` and `sex` and\n",
    "4 Fit a random forest model to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "\n",
    "predictors = train_cleaned.columns.tolist()\n",
    "if 'id' in predictors:\n",
    "    predictors.remove('id')\n",
    "if 'sii' in predictors:\n",
    "    predictors.remove('sii')\n",
    "predictors = [x for x in predictors if 'PCIAT' not in x]\n",
    "predictors = [x for x in predictors if 'Season' not in x]\n",
    "\n",
    "\n",
    "pipe_knn = Pipeline([('knn_impute', Custom_KNN_Imputer()),\n",
    "                    ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                    ('rf', RandomForestRegressor(n_estimators = 300, max_features = 'sqrt', max_depth = 5, random_state = 216))])\n",
    "\n",
    "pipe_knn.fit(train_cleaned[predictors],train_cleaned['PCIAT-PCIAT_Total'])\n",
    "\n",
    "train_pred_knn = pipe_knn.predict(train_cleaned[predictors])\n",
    "\n",
    "\n",
    "\n",
    "pipe_mice = Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "                    ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                    ('rf', RandomForestRegressor(n_estimators = 300, max_features = 'sqrt', max_depth = 5, random_state = 216))])\n",
    "\n",
    "pipe_mice.fit(train_cleaned[predictors],train_cleaned['PCIAT-PCIAT_Total'])\n",
    "\n",
    "train_pred_mice = pipe_mice.predict(train_cleaned[predictors])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get feature importance from the rf inside pipe\n",
    "score_knn_df = pd.DataFrame({'feature':train_cleaned[predictors].columns,\n",
    "                            'importance_score': pipe_knn.named_steps['rf'].feature_importances_})\n",
    "\n",
    "score_knn_df.sort_values('importance_score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get feature importance from the rf inside pipe\n",
    "score_mice_df = pd.DataFrame({'feature':train_cleaned[predictors].columns,\n",
    "                            'importance_score': pipe_mice.named_steps['rf'].feature_importances_})\n",
    "\n",
    "score_mice_df.sort_values('importance_score',ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_fall_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
