**What we’ve done**

* Outcome Variable:
    * Removed 974 participants who had no data for PCIAT (i.e., the outcome variable)
		* Of the remaining participants, nearly all had values for all PCIAT variables
		* We checked the distribution of ages for participants with missing values, and saw no patterns
		* We used KNN to impute the remaining PCIAT values
* Predictor Variables:
    * Cleaned the data for outliers (eliminating values that were ≥5 SDs above/below mean)
    * Cleaned negative values
    * Computed new variables based on accelerometer data
        * Mean number of 5-minute “bouts” on “active” days where ENMO was ≥192
        * Mean number of 5-minute “bouts” on “active” days where ENMO was ≥110
        * Mean number of 5-minute “bouts” on “active” days where anglez >0
    * Computed new variable based on FitnessGram Zone for Push-Ups, Trunk Lifts, Curl Up, and Sit & Reach Left
    * Created a new indicator for PAQ≥ ** for Child/Adolescent
    * Merged the PAQ_Child and PAQ_Adolescent variable values
    * Identified correlations between predictor variables and proposed a list of highly-correlated variables
    * Attempted to identify latent variables among sets of predictors
        * Tried doing PCA for each of the three large sets of predictor variables (Physical, FitnessGram, BIA) to try to identify latent variables
        * Tried doing Ridge regression on each of these sets
        * Certainly ridge regression makes no sense here... seems like PCA might not, either?
* Two versions of Imputation for Predictors (note: this will happen after we’re done with feature engineering)
    * Used Multiple Imputation on the entire set of predictors (including binary categorical variables, but not the Season variables)
        * Used an iterative imputer
        * Used the default imputation method (Bayesian ridge)
    * (Separately) Used KNN to impute values within each large set of predictor variable, including Age and Sex variables
        * Physical (6 variables)
        * FitnessGram (16 variables, including the Zone variables)
        * BIA (15 variables)
        * CGAS (1 variable)
        * Internet Useage (1 variable)
* Modeling
    * The outcome we’re actually trying to predict (sii) is an ordinal variable whose values are based on cutoffs from the PCIAT variable. We have decided to try to predict the PCIAT value rather than the sii value

**What we’re not sure about so far**
* Are there any issues that might arise from trying to predict PCIAT rather than sii?
* We only have accelerometer data for ~800 participants (after cleaning?). We’re not sure which other variables the accelerometer data might be related to, so we’re not sure what variables to use for KNN imputation with the accelerometer data. We could do simple imputation, but that seems unreliable since there are so many missing values
* For all imputation of predictors, we have several variables that are completely determined by other variables.
    * BMI is deteremined by height and weight
    * FitnessGram Zone variables are determined by a cutoff value for FitnessGram variables (we’d have to identify these cutoff values in the literature)
    * FitnessGram Total is determined by FitnessGram Zone variables
    * PAQ Zone is determined by PAQ variable
    * Should we remove these determined variables from the imputation calculations and then re-calculate them based on their imputed predictor variables 
* For all imputation of predictors - should we remove highly correlated variables prior to doing the imputation?
* We’re not sure how to include Season variables in the imputation. If we make them dummy variables, it seems like they’d be over-weighted in imputation
	

**What we need to do**
* Imputation for PAQ scores - we know how to do this
* Imputation for the accelerometer-based variables. We have so few participants with accelerometer data that this seems potentially problematic
* Impute values for the Season variables. The only idea we have so far is to look for “cohorting” - i.e., patterns among the seasons (e.g., Spring - Fall - Spring), but we’re not sure how to do this
* Feature reduction - We’d like to try to reduce the number of predictor variables. A couple of ideas we’ve had:
    * Identify highly correlated predictors and removing predictors that are predicted by other predictors. We know how to do this, although something like ridge regression would make this unnecessary?
    * Identify “latent” variables, particularly within each large set of predictors variables (e.g., the 15 BIA variables). We tried using PCA… but it seems like that doesn’t do what we’d like. So we’re not really sure how to do this identification
* Interaction identification - It seems not unlikely that some of our predictor variables interact with each other. In addition, it seems like some of the interactions might depend on whether or not the data were collected in the same season. We have a lot of predictors, and aren’t sure how to efficiently identify potential interactions.
* Feature selection - we’ve done a bit of EDA, but haven’t yet really discussed which predictors seem most salient to include in our models
* Discuss potential models (we’ve been stuck enough on everything else that we haven’t gotten to this step)