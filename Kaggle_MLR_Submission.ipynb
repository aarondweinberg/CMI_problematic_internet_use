{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":81933,"databundleVersionId":9643020,"sourceType":"competition"}],"dockerImageVersionId":30804,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Predicting Problematic Internet Use using Multiple Linear Regression**\n\nThe goal of this notebook is to predict problematic internet use for the Child Mind Institute's Kaggle competition.\n\nThis work was done by\nAaron Weinberg, Emilie Wiesner, and Dan Visscher at Ithaca College\n\nAll code that was used to develop this model is available on GitHub: https://github.com/aarondweinberg/CMI_problematic_internet_use","metadata":{}},{"cell_type":"markdown","source":"**Import Packages and Classes**","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os #reading and writing files\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.impute import KNNImputer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LinearRegression","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T15:13:01.455980Z","iopub.execute_input":"2024-12-18T15:13:01.456816Z","iopub.status.idle":"2024-12-18T15:13:01.481320Z","shell.execute_reply.started":"2024-12-18T15:13:01.456772Z","shell.execute_reply":"2024-12-18T15:13:01.478717Z"}},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":"**Our Custom Imputer and Zone Computer**\n\nThis is an iterative imputer used to fill in missing values in the predictors.\n\nThis section also includes functions to compute \"Zone\" values for the FitnessGram and PAQ variables using documentation found online","metadata":{}},{"cell_type":"code","source":"## Define custom iterative imputer (\"MICE\")\nclass Custom_MICE_Imputer(BaseEstimator, TransformerMixin):\n    # Class Constructor \n    # This allows you to initiate the class when you call Custom_KNN_Imputer\n    def __init__(self):\n        # I want to initiate each object with both a KNNImputer and StandardScaler object/method\n        self.MICEImputer = IterativeImputer(max_iter=10, random_state=497)\n\n    \n    # For my fit method I'm just going to \"steal\" IterativeImputers's fit method using a curated collection of predictors\n    def fit(self, Z, y = None):\n        feature_list = Z.columns.tolist()\n        if 'id' in feature_list:\n            feature_list.remove('id')\n        if 'sii' in feature_list:\n            feature_list.remove('sii')\n        feature_list = [x for x in feature_list if 'PCIAT' not in x]\n        feature_list = [x for x in feature_list if 'Zone' not in x]\n        feature_list = [x for x in feature_list if 'Season' not in x]\n        Z = Z.reset_index(drop=True)\n        self.MICEImputer.fit(Z[feature_list])\n        return self\n    \n    # Now I want to transform the columns in feature list and return it with imputed values that have been un-transformed\n    def transform(self, Z, y = None):\n        feature_list = Z.columns.tolist()\n        if 'id' in feature_list:\n            feature_list.remove('id')\n        if 'sii' in feature_list:\n            feature_list.remove('sii')\n        feature_list = [x for x in feature_list if 'PCIAT' not in x]\n        feature_list = [x for x in feature_list if 'Zone' not in x]\n        feature_list = [x for x in feature_list if 'Season' not in x]\n        copy_Z = Z.copy()\n        copy_Z = copy_Z.reset_index(drop=True)\n        df2 = self.MICEImputer.transform(copy_Z[feature_list])\n        df3 = pd.DataFrame(df2, columns=feature_list)\n        copy_Z[feature_list]=copy_Z[feature_list].fillna(df3[feature_list])\n        return copy_Z\n    \n\n####Now defining zone functions.\n\n# Compute values for the 'FGC-FGC_SR_Zone' that is equal to 1 if any of the following are true:\n# Basic_Demos-Sex==0 and FGC-FGC_SR >= 8\n# Basic_Demos-Sex==1 and FGC-FGC_SR >= 9 and Basic_Demos-Age is between 5 and 10\n# Basic_Demos-Sex==1 and FGC-FGC_SR >= 10 and Basic_Demos-Age is between 11 and 14\n# Basic_Demos-Sex==1 and FGC-FGC_SR >= 12 and Basic_Demos-Age is at least 15\n# Note that Basic_Demos-Sex is coded as 0=Male and 1=Female\n\ndef sitreachzone(sex, age, sr):\n    try:\n        if np.isnan(sr) or np.isnan(sex) or np.isnan(age):\n            return np.nan\n        elif sex == 0 and sr>=8:\n            return 1\n        elif sex == 1 and age >= 15 and sr >= 12:\n            return 1\n        elif sex == 1 and age >= 11 and sr >= 10:\n            return 1\n        elif sex == 1 and age >= 5 and sr >= 9:\n            return 1\n        else:\n            return 0\n    except:\n        return np.nan\n\n# Compute values for the 'FGC-FGC_CU_Zone' that is equal to 1 if any of the following are true:\n# Basic_Demos-Sex==0 and FGC-FGC_CU >= 2 and Basic_Demos-Age is between 5 and 6\n# Basic_Demos-Sex==0 and FGC-FGC_CU >= 4 and Basic_Demos-Age is 7\n# Basic_Demos-Sex==0 and FGC-FGC_CU >= 6 and Basic_Demos-Age is 8\n# Basic_Demos-Sex==0 and FGC-FGC_CU >= 9 and Basic_Demos-Age is 9\n# Basic_Demos-Sex==0 and FGC-FGC_CU >= 12 and Basic_Demos-Age is 10\n# Basic_Demos-Sex==0 and FGC-FGC_CU >= 15 and Basic_Demos-Age is 11\n# Basic_Demos-Sex==0 and FGC-FGC_CU >= 18 and Basic_Demos-Age is 12\n# Basic_Demos-Sex==0 and FGC-FGC_CU >= 21 and Basic_Demos-Age is 13\n# Basic_Demos-Sex==0 and FGC-FGC_CU >= 24 and Basic_Demos-Age is at least 14\n# Basic_Demos-Sex==1 and FGC-FGC_CU >= 2 and Basic_Demos-Age is between 5 and 6\n# Basic_Demos-Sex==1 and FGC-FGC_CU >= 4 and Basic_Demos-Age is 7\n# Basic_Demos-Sex==1 and FGC-FGC_CU >= 6 and Basic_Demos-Age is 8\n# Basic_Demos-Sex==1 and FGC-FGC_CU >= 9 and Basic_Demos-Age is 9\n# Basic_Demos-Sex==1 and FGC-FGC_CU >= 12 and Basic_Demos-Age is 10\n# Basic_Demos-Sex==1 and FGC-FGC_CU >= 15 and Basic_Demos-Age is 11\n# Basic_Demos-Sex==1 and FGC-FGC_CU >= 18 and Basic_Demos-Age is at least 12\n\ndef curlupzone(sex, age, cu):\n    try:\n        if np.isnan(sex) or np.isnan(age) or np.isnan(cu):\n            return np.nan\n        elif sex == 0:\n            if (age >= 14 and cu >= 24) or (age == 13 and cu >= 21) or (age == 12 and cu >= 18) or (age == 11 and cu >= 15) or (age == 10 and cu >= 12) or (age == 9 and cu >= 9) or (age == 8 and cu >= 6) or (age == 7 and cu >= 4) or (age <= 6 and cu >= 2):\n                return 1\n            else:\n                return 0\n        elif sex == 1:\n            if (age >= 12 and cu >= 18) or (age == 11 and cu >= 15) or (age == 10 and cu >= 12) or (age == 9 and cu >= 9) or (age == 8 and cu >= 6) or (age == 7 and cu >= 4) or (age <= 6 and cu >= 2):\n                return 1\n            else:\n                return 0\n    except:\n        return np.nan\n\n# Compute values for the 'FGC-FGC_PU_Zone' that is equal to 1 if any of the following are true:\n# Basic_Demos-Sex==0 and FGC-FGC_PU >= 3 and Basic_Demos-Age is between 5 and 6\n# Basic_Demos-Sex==0 and FGC-FGC_PU >= 4 and Basic_Demos-Age is 7\n# Basic_Demos-Sex==0 and FGC-FGC_PU >= 5 and Basic_Demos-Age is 8\n# Basic_Demos-Sex==0 and FGC-FGC_PU >= 6 and Basic_Demos-Age is 9\n# Basic_Demos-Sex==0 and FGC-FGC_PU >= 7 and Basic_Demos-Age is 10\n# Basic_Demos-Sex==0 and FGC-FGC_PU >= 8 and Basic_Demos-Age is 11\n# Basic_Demos-Sex==0 and FGC-FGC_PU >= 10 and Basic_Demos-Age is 12\n# Basic_Demos-Sex==0 and FGC-FGC_PU >= 12 and Basic_Demos-Age is 13\n# Basic_Demos-Sex==0 and FGC-FGC_PU >= 14 and Basic_Demos-Age is 14\n# Basic_Demos-Sex==0 and FGC-FGC_PU >= 16 and Basic_Demos-Age is 15\n# Basic_Demos-Sex==0 and FGC-FGC_PU >= 18 and Basic_Demos-Age is at least 16\n# Basic_Demos-Sex==1 and FGC-FGC_PU >= 3 and Basic_Demos-Age is between 5 and 6\n# Basic_Demos-Sex==1 and FGC-FGC_PU >= 4 and Basic_Demos-Age is 7\n# Basic_Demos-Sex==1 and FGC-FGC_PU >= 5 and Basic_Demos-Age is 8\n# Basic_Demos-Sex==1 and FGC-FGC_PU >= 6 and Basic_Demos-Age is 9\n# Basic_Demos-Sex==1 and FGC-FGC_PU >= 7 and Basic_Demos-Age is at least 10\n\ndef pullupzone(sex, age, pu):\n    try:\n        if np.isnan(sex) or np.isnan(age) or np.isnan(pu):\n            return np.nan\n        elif sex == 0:\n            if (age >= 16 and pu >= 18) or (age == 15 and pu >= 16) or (age == 14 and pu >= 14) or (age == 13 and pu >= 12) or (age == 12 and pu >= 10) or (age == 11 and pu >= 8) or (age == 10 and pu >= 7) or (age == 9 and pu >= 6) or (age == 8 and pu >= 5) or (age == 7 and pu >= 4) or (age <= 6 and pu >= 2):\n                return 1\n            else:\n                return 0\n        elif sex == 1:\n            if (age >= 10 and pu >= 7) or (age == 9 and pu >= 6) or (age == 8 and pu >= 5) or (age == 7 and pu >= 4) or (age <= 6 and pu >= 3):\n                return 1\n            else:\n                return 0\n    except:\n        return np.nan\n\n# Comtlte values for the 'FGC-FGC_TL_Zone' that is equal to 1 if any of the following are true:\n# FGC-FGC_TL >= 6 and Basic_Demos-Age is between 5 and 9\n# FGC-FGC_TL >= 9 and Basic_Demos-Age is at least 10\n\ndef tlzone(age, tl):\n    try:\n        if np.isnan(tl) or np.isnan(age):\n            return np.nan\n        elif (age >= 10 and tl >= 9) or (age <= 9 and tl >= 6):\n            return 1\n        else:\n            return 0\n    except:\n        return np.nan\n\n# Comtlte values for the 'PAQ_MVPA' that is equal to 1 if any of the following are true:\n# PAQ_Total >= 2.73 and Basic_Demos-Age is between 5 and 13\n# PAQ_Total >= 2.75 and Basic_Demos-Age is at least 14\n\ndef paqzone(age, paq):\n    try:\n        if np.isnan(paq) or np.isnan(age):\n            return np.nan\n        elif (age >= 14 and paq >= 2.75) or (age <= 13 and paq >= 2.73):\n            return 1\n        else:\n            return 0\n    except:\n        return np.nan\n\n###Custom encoder function\n# \ndef zone_encoder(df):\n    df_copy = df.copy()\n\n    if 'FGC-FGC_SR_Zone' in df_copy.columns:\n        if 'Basic_Demos-Age' in df_copy.columns and 'Basic_Demos-Sex' in df_copy.columns and 'FGC-FGC_SR' in df_copy.columns:\n            df_copy['FGC-FGC_SR_Zone'] = df_copy.apply(lambda x: sitreachzone(x['Basic_Demos-Sex'], x['Basic_Demos-Age'], x['FGC-FGC_SR']), axis=1)\n        else:\n            df_copy['FGC-FGC_SR_Zone'] = df_copy['FGC-FGC_SR_Zone'].fillna(df_copy['FGC-FGC_SR_Zone'].mean())\n    if 'FGC-FGC_CU_Zone' in df_copy.columns:\n        if 'Basic_Demos-Age' in df_copy.columns and 'Basic_Demos-Sex' in df_copy.columns and 'FGC-FGC_CU' in df_copy.columns:\n            df_copy['FGC-FGC_CU_Zone'] = df_copy.apply(lambda x: curlupzone(x['Basic_Demos-Sex'], x['Basic_Demos-Age'], x['FGC-FGC_CU']), axis=1)\n        else:\n            df_copy['FGC-FGC_CU_Zone'] = df_copy['FGC-FGC_CU_Zone'].fillna(df_copy['FGC-FGC_CU_Zone'].mean())\n    if 'FGC-FGC_PU_Zone' in df_copy.columns:\n        if 'Basic_Demos-Age' in df_copy.columns and 'Basic_Demos-Sex' in df_copy.columns and 'FGC-FGC_PU' in df_copy.columns:\n            df_copy['FGC-FGC_PU_Zone'] = df_copy.apply(lambda x: pullupzone(x['Basic_Demos-Sex'], x['Basic_Demos-Age'], x['FGC-FGC_PU']), axis=1)\n        else:\n            df_copy['FGC-FGC_PU_Zone'] = df_copy['FGC-FGC_PU_Zone'].fillna(df_copy['FGC-FGC_PU_Zone'].mean())\n    if 'FGC-FGC_TL_Zone' in df_copy.columns:\n        if 'Basic_Demos-Age' in df_copy.columns and 'FGC-FGC_TL' in df_copy.columns:\n            df_copy['FGC-FGC_TL_Zone'] = df_copy.apply(lambda x: tlzone(x['Basic_Demos-Age'], x['FGC-FGC_TL']), axis=1)\n        else:\n            df_copy['FGC-FGC_TL_Zone'] = df_copy['FGC-FGC_TL_Zone'].fillna(df_copy['FGC-FGC_TL_Zone'].mean())\n    if 'PAQ_Zone' in df_copy.columns:\n        if 'Basic_Demos-Age' in df_copy.columns and 'PAQ_Total' in df_copy.columns:\n            df_copy['PAQ_Zone'] = df_copy.apply(lambda x: tlzone(x['Basic_Demos-Age'], x['PAQ_Total']), axis=1)\n        else:\n            df_copy['PAQ_Zone'] = df_copy.apply(lambda x: paqzone(x['Basic_Demos-Age'], x['PAQ_Total']), axis=1)\n    return df_copy   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T13:49:02.866346Z","iopub.execute_input":"2024-12-18T13:49:02.866900Z","iopub.status.idle":"2024-12-18T13:49:02.897987Z","shell.execute_reply.started":"2024-12-18T13:49:02.866860Z","shell.execute_reply":"2024-12-18T13:49:02.896943Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"**Code the Accelerometer Data**\n\nWe segment each actigraphy file into 5-minute bouts\n\nFor each bout, we indicate whether the mean ENMO value is above thresholds identified in the research literature\n\nWe also identify how often the anglez was positive\n\nThen we create a dataframe that records these values for each participant ID","metadata":{}},{"cell_type":"code","source":"#ENMO cutoffs in mg for MVPA\nmvpa_cutoff1 = 0.192\nmvpa_cutoff2 = 0.110\n\n# Number of 'active' bouts required for a day to count as 'active'\nactive_bout_cutoff = 150\n\n# Specify the length of the bouts\nboutlength = '5min'\n\n# Maximum number of 5-minute bouts that can be imputed as zeroes to account for the accelerometer not collected data when at rest\nimpute_max = 6\n\n# Minimum number of 5-second intervals (within a 5-minute bout) that need to have data for the bout to be counted\nimpute_sec_min = 29\n\n# Create a new data frame with columns 'ID', 'ENMO_Avg_Active_Days_MVPA192', 'ENMO_Avg_Active_Days_MVPA110', and 'Positive_Anglez_Active_Days\naccel = pd.DataFrame(columns=['ID', 'ENMO_Avg_Active_Days_MVPA192', 'ENMO_Avg_Active_Days_MVPA110', 'Positive_Anglez_Active_Days'])\n\n\n# Walk through the files in the directory\n# For testing purposes, we'll just do this for test data\nfor dirname, _, filenames in os.walk('/kaggle/input/child-mind-institute-problematic-internet-use'):\n#for dirname, _, filenames in os.walk('/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet'):\n    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n        #Check to see if filename is a parquet file; if it is, read the file and extract the ID from the directory name\n        if filename.endswith('.parquet'):\n            data = pd.read_parquet(os.path.join(dirname, filename))\n            id = dirname[-8:]\n\n            # Remove any rows where the variable non-wear_flag is nonzero\n            data = data[data['non-wear_flag'] == 0]\n\n            # Change the time_of_day variable to a datetime and make it into the index\n            data['dt'] = pd.to_datetime(data['time_of_day'])\n            data['dt_mod'] = data['dt'] + pd.to_timedelta(data['relative_date_PCIAT'], unit='D')\n            data.set_index('dt_mod', inplace=True)\n\n            # Create a new data frame that counts the number of valid data points within each 5-minute ('boutlength') interval\n            # This will later be used to exclude intervals that had fewer than 30 (out of 60) valid data points\n            data['count'] = 1\n            number_of_data_points = data.resample(boutlength).agg({'count':'sum'})\n            data.drop('count', axis=1, inplace=True)\n\n            # Create 5-minute \"bouts\" of averaged data and incorporate the number of valid data points within each interval as a new variable 'count'\n            data_resampled_5min = data.resample(boutlength).mean()\n            data_resampled_5min = data_resampled_5min.merge(number_of_data_points, left_index=True, right_index=True)\n\n            # Some of the accelerometers stopped collecting data if they were stationary (but still on/worn)\n            # This next section is an attempt to identify and fill in these seemingly missing values with \"0\" for the enmo value\n            # It does this by identifying the length of each sequence of NaN values and filling them with 0 if thery are at most 30 minutes long\n            # This also restricts this process to 5-minute bouts that had data for at least 30 of the 5-second-intervals within the bout\n            data_resampled_5min['enmogroup'] = data_resampled_5min['enmo'].notna().cumsum()\n            enmogroupcount = data_resampled_5min.groupby(by=[\"enmogroup\"]).size().to_frame()\n            enmogroupcount = enmogroupcount.rename(columns={0: 'enmogroupsize'})\n            data_resampled_5min = data_resampled_5min.merge(enmogroupcount, how='left', left_on='enmogroup', right_index=True)\n            data_resampled_5min['smallinterval'] = (data_resampled_5min['enmogroupsize'] < impute_max+2) & (data_resampled_5min['count']>impute_sec_min)\n            data_resampled_5min['filled_enmo'] = np.where(data_resampled_5min.smallinterval, data_resampled_5min.enmo.fillna(0), data_resampled_5min.enmo)\n\n            # Also fill in only anglez values where the count is large enough\n            data_resampled_5min['filled_anglez'] = np.where(data_resampled_5min['count']>impute_sec_min, data_resampled_5min.anglez, np.nan)\n\n            # The next code chunk will create a new data frame that lists the total number of valid bouts for the participant\n            # and will count the number of bouts with filled_enmo values over a particular threshold\n            # and then count the number of bouts with positive anglez values\n\n            # Start by counting the number of valid bouts in each day as a data frame\n            boutcount_filled = data_resampled_5min.groupby(data_resampled_5min.index.date).count()['filled_enmo'].to_frame()\n            boutcount_filled = boutcount_filled.rename(columns={'filled_enmo': 'valid_bouts'})\n\n            # Count the number of bouts in each day with filled_enmo at least mvpa_cutoff1\n            boutcount_MVPA1 = data_resampled_5min[data_resampled_5min['filled_enmo'] >= mvpa_cutoff1].groupby(data_resampled_5min[data_resampled_5min['filled_enmo'] >= mvpa_cutoff1].index.date).count()['filled_enmo'].to_frame()\n            boutcount_MVPA1 = boutcount_MVPA1.rename(columns={'filled_enmo': 'MVPA_bouts_over_cutoff1'})\n            boutcount = boutcount_filled.merge(boutcount_MVPA1, how='left', left_index=True, right_index=True)\n\n            # Count the number of bouts in each day with filled_enmo at least mvpa_cutoff2\n            boutcount_MVPA2 = data_resampled_5min[data_resampled_5min['filled_enmo'] >= mvpa_cutoff2].groupby(data_resampled_5min[data_resampled_5min['filled_enmo'] >= mvpa_cutoff2].index.date).count()['filled_enmo'].to_frame()\n            boutcount_MVPA2 = boutcount_MVPA2.rename(columns={'filled_enmo': 'MVPA_bouts_over_cutoff2'})\n            boutcount = boutcount.merge(boutcount_MVPA2, how='left', left_index=True, right_index=True)\n\n            # Count the number of bouts in each day with anglez at least 0\n            boutcount_anglez = data_resampled_5min[data_resampled_5min['filled_anglez'] > 0].groupby(data_resampled_5min[data_resampled_5min['filled_anglez'] > 0].index.date).count()['filled_anglez'].to_frame()\n            boutcount_anglez = boutcount_anglez.rename(columns={'filled_anglez': 'Positive_Anglez_Bouts'})\n            boutcount = boutcount.merge(boutcount_anglez, how='left', left_index=True, right_index=True)\n\n            # Compute a new variable 'included_day' to be True if valid_bouts is at least active_bout_cutoff\n            boutcount['included_day'] = boutcount['valid_bouts'] >= active_bout_cutoff\n\n            # Compute the mean of MVPA bouts over each cutoff\n            # Note: We are only using the \"included day\" data in our final analysis, so we'll restrict the output accordingly\n            MVPA_mean1 = boutcount[boutcount['included_day'] == True]['MVPA_bouts_over_cutoff1'].mean()\n            MVPA_mean2 = boutcount[boutcount['included_day'] == True]['MVPA_bouts_over_cutoff2'].mean()\n            Anglez_mean1 = boutcount[boutcount['included_day'] == True]['Positive_Anglez_Bouts'].mean()\n\n            new_row = pd.DataFrame({\"ID\": [id], \"ENMO_Avg_Active_Days_MVPA192\": [MVPA_mean1], \"ENMO_Avg_Active_Days_MVPA110\": [MVPA_mean2], \"Positive_Anglez_Active_Days\": [Anglez_mean1]})\n\n            # Replace any NaN values in df with 0\n            new_row.fillna(0, inplace=True)\n            \n            #Create a new row in accel where 'ID'=id, 'ENMO_Avg_Active_Days_MVPA192'=MVPA_mean1, 'ENMO_Avg_Active_Days_MVPA110'=MVPA_mean2, and 'Positive_Anglez_Active_Days'=Anglez_mean1\n            accel = pd.concat([accel, new_row], ignore_index=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T13:49:27.559151Z","iopub.execute_input":"2024-12-18T13:49:27.559661Z","iopub.status.idle":"2024-12-18T14:03:45.470860Z","shell.execute_reply.started":"2024-12-18T13:49:27.559617Z","shell.execute_reply":"2024-12-18T14:03:45.468965Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_24/2087160359.py:102: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n  accel = pd.concat([accel, new_row], ignore_index=True)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"**Load the Data and Join with Accelerometer Data**\n\nWe noticed that the versions of these files stored in /kaggle/input included some duplicate rows, so we drop these.","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\ntrain = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\n\n# Join train/test and accel  on the 'id' column and accel on the 'ID' column\ntest_original = test.join(accel.set_index('ID'), on='id', how='left')\ntrain_original = train.join(accel.set_index('ID'), on='id', how='left')\n\n# Remove duplicate rows\ntest = test_original.drop_duplicates()\ntrain = train_original.drop_duplicates()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T14:37:09.053710Z","iopub.execute_input":"2024-12-18T14:37:09.054129Z","iopub.status.idle":"2024-12-18T14:37:09.135786Z","shell.execute_reply.started":"2024-12-18T14:37:09.054093Z","shell.execute_reply":"2024-12-18T14:37:09.134596Z"}},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"**Data Cleaning**\n\nThis section creates some new variables\n* Sit & Reach is the average of the \"Left\" and \"Right\" Sit & Reach predictors\n* A Zone variable for the combined Sit & Reach variable\n* Combining the PAQ-A and PAQ-C into a single PAQ predictor\n* Creating a \"Zone\" for PAQ based on cutoffs in the research literature\n* Combining the minutes and seconds of the Fitness Endurance variable\n* Dropping SDS_T, which seems to be a duplicate of SDS\n* Removing negative values from numerical predictors\n* Removing 0 values from physical predictors\n* Removing outliers","metadata":{}},{"cell_type":"code","source":"###################################\n#Now we do a variety of data cleaning.\n# Create a new variable 'FGC-FGC_SR' that is the mean of FGC-FGC_SRL and FGC-FGC_SRR\ntrain['FGC-FGC_SR'] = train[['FGC-FGC_SRL', 'FGC-FGC_SRR']].mean(axis=1)\ntest['FGC-FGC_SR'] = test[['FGC-FGC_SRL', 'FGC-FGC_SRR']].mean(axis=1)\n\n# Remove the old sit & reach variables\ntrain = train.drop(columns=['FGC-FGC_SRL', 'FGC-FGC_SRR', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR_Zone'])\ntest = test.drop(columns=['FGC-FGC_SRL', 'FGC-FGC_SRR', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR_Zone'])\n\n# Create a new variable 'FGC-FGC_SR_Zone' that is equal to 1 if any of the following are true:\n# Basic_Demos-Sex==0 and FGC-FGC_SR >= 8\n# Basic_Demos-Sex==1 and FGC-FGC_SR_Zone >= 9 and Basic_Demos-Age is between 5 and 10\n# Basic_Demos-Sex==1 and FGC-FGC_SR_Zone >= 10 and Basic_Demos-Age is between 11 and 14\n# Basic_Demos-Sex==1 and FGC-FGC_SR_Zone >= 12 and Basic_Demos-Age is at least 15\n\n# One way to do this is to define a function that would take sex, age, and SR value as inputs and output 1 or 0\ndef sitreachzone(sex, age, sr):\n    try:\n        if np.isnan(sr):\n            return np.nan\n        elif sex == 0 and sr>=8:\n            return 1\n        elif sex == 1 and age >= 15 and sr >= 12:\n            return 1\n        elif sex == 1 and age >= 11 and sr >= 10:\n            return 1\n        elif sex == 1 and age >= 5 and sr >= 9:\n            return 1\n        else:\n            return 0\n    except:\n        return np.nan\n\n# Apply sitreachzone to create a new column using the columns Basic_Demos-Sex, Basic_Demos-Age, and FGC-FGC_SR as inputs\ntrain['FGC-FGC_SR_Zone'] = train.apply(lambda x: sitreachzone(x['Basic_Demos-Sex'], x['Basic_Demos-Age'], x['FGC-FGC_SR']), axis=1)\ntest['FGC-FGC_SR_Zone'] = test.apply(lambda x: sitreachzone(x['Basic_Demos-Sex'], x['Basic_Demos-Age'], x['FGC-FGC_SR']), axis=1)\n\n# Create a new variable that is 1 when PAQA/C Total is at least 2.75/2.73, 0 if it's less than these cutoffs, and NaN if PAQA/C is NaN\ntrain['PAQA_Zone'] = np.where(train['PAQ_A-PAQ_A_Total']>=2.75, 1, 0)\ntrain['PAQA_Zone'] = np.where(train['PAQ_A-PAQ_A_Total'].isnull(), np.nan, train['PAQA_Zone'])\ntrain['PAQC_Zone'] = np.where(train['PAQ_C-PAQ_C_Total']>=2.73, 1, 0)\ntrain['PAQC_Zone'] = np.where(train['PAQ_C-PAQ_C_Total'].isnull(), np.nan, train['PAQC_Zone'])\ntest['PAQA_Zone'] = np.where(test['PAQ_A-PAQ_A_Total']>=2.75, 1, 0)\ntest['PAQA_Zone'] = np.where(test['PAQ_A-PAQ_A_Total'].isnull(), np.nan, test['PAQA_Zone'])\ntest['PAQC_Zone'] = np.where(test['PAQ_C-PAQ_C_Total']>=2.73, 1, 0)\ntest['PAQC_Zone'] = np.where(test['PAQ_C-PAQ_C_Total'].isnull(), np.nan, test['PAQC_Zone'])\n\n# Create new variables that merge the three PAQA/C variables\ntrain['PAQ_Total']=train['PAQ_C-PAQ_C_Total']\ntrain.loc[train['PAQ_Total'].isnull(),'PAQ_Total']=train['PAQ_A-PAQ_A_Total']\ntest['PAQ_Total']=test['PAQ_C-PAQ_C_Total']\ntest.loc[test['PAQ_Total'].isnull(),'PAQ_Total']=test['PAQ_A-PAQ_A_Total']\n\ntrain['PAQ_Season']=train['PAQ_C-Season']\ntrain.loc[train['PAQ_Season'].isnull(),'PAQ_Season']=train['PAQ_A-Season']\ntest['PAQ_Season']=test['PAQ_C-Season']\ntest.loc[test['PAQ_Season'].isnull(),'PAQ_Season']=test['PAQ_A-Season']\n\ntrain['PAQ_Zone']=train['PAQC_Zone']\ntrain.loc[train['PAQ_Zone'].isnull(),'PAQ_Zone']=train['PAQA_Zone']\ntest['PAQ_Zone']=test['PAQC_Zone']\ntest.loc[test['PAQ_Zone'].isnull(),'PAQ_Zone']=test['PAQA_Zone']\n\n# Drop the PAQ variables we no longer need\ntrain=train.drop(columns=['PAQ_C-PAQ_C_Total', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season', 'PAQ_A-Season', 'PAQA_Zone', 'PAQC_Zone'])\ntest=test.drop(columns=['PAQ_C-PAQ_C_Total', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season', 'PAQ_A-Season', 'PAQA_Zone', 'PAQC_Zone'])\n\n# Combine the minutes and seconds of Fitness_Endurance into a single number (total number of seconds)\ntrain['Fitness_Endurance_Total_Time_Sec'] = train['Fitness_Endurance-Time_Mins'] * 60 + train['Fitness_Endurance-Time_Sec']\ntest['Fitness_Endurance_Total_Time_Sec'] = test['Fitness_Endurance-Time_Mins'] * 60 + test['Fitness_Endurance-Time_Sec']\n\ntrain=train.drop(columns=['Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec'])\ntest=test.drop(columns=['Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec'])\n\n# Remove the SDS-SDS_Total_T variable from train\ntrain=train.drop(columns=['SDS-SDS_Total_T'])\ntest=test.drop(columns=['SDS-SDS_Total_T'])\n\n# Remove the FGC-FGC_GSND, FGC-FGC_GSND_Zone, FGC-FGC_GSD, and FGC-FGC_GSD_Zone variables\ntrain=train.drop(columns=['FGC-FGC_GSND', 'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone'])\ntest=test.drop(columns=['FGC-FGC_GSND', 'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone'])\n\n# Create a list of numerical columns of type float. Note that these columns include the \"Zone\" variables which are really categorical/ordinal:\nfloat_columns = test.select_dtypes(include=['float']).columns\n\n# Change negative values to NaN\ntrain[train[float_columns] < 0] = np.nan\ntest[test[float_columns] < 0] = np.nan\n\n# For each variable that starts with 'Physical-' replace any values that are 0 with NaN\nfor column in train.columns:\n    if column.startswith('Physical-'):\n        train[column] = train[column].replace(0, np.nan)\nfor column in test.columns:\n    if column.startswith('Physical-'):\n        test[column] = test[column].replace(0, np.nan)\n\n# For each column in float_columns, identify entries that are 5 standard deviations above or below the mean and replace them with NaN\nfor column in float_columns:\n    train[column] = train[column].mask(train[column] > train[column].mean() + 5 * train[column].std())\n    train[column] = train[column].mask(train[column] < train[column].mean() - 5 * train[column].std())\n    test[column] = test[column].mask(test[column] > test[column].mean() + 5 * test[column].std())\n    test[column] = test[column].mask(test[column] < test[column].mean() - 5 * test[column].std())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T14:37:12.649432Z","iopub.execute_input":"2024-12-18T14:37:12.649825Z","iopub.status.idle":"2024-12-18T14:37:12.942650Z","shell.execute_reply.started":"2024-12-18T14:37:12.649790Z","shell.execute_reply":"2024-12-18T14:37:12.941549Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_24/2159061128.py:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  train['FGC-FGC_SR'] = train[['FGC-FGC_SRL', 'FGC-FGC_SRR']].mean(axis=1)\n/tmp/ipykernel_24/2159061128.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  test['FGC-FGC_SR'] = test[['FGC-FGC_SRL', 'FGC-FGC_SRR']].mean(axis=1)\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"# Rename the files for future code compatibility\n\ntrain_cleaned = train\ntest_cleaned= test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T14:37:16.939145Z","iopub.execute_input":"2024-12-18T14:37:16.939554Z","iopub.status.idle":"2024-12-18T14:37:16.946566Z","shell.execute_reply.started":"2024-12-18T14:37:16.939517Z","shell.execute_reply":"2024-12-18T14:37:16.945107Z"}},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":"**Feature Reduction**\n\nWe noticed a large correlation (>0.98) between some predictors. We'll remove these, as well as the season variables, which we won't use in our predictions.","metadata":{}},{"cell_type":"code","source":"# Remove some variables we won't need\n\n# Remove the variable 'BIA-BIA_BMI'\ntrain_cleaned = train_cleaned.drop(['BIA-BIA_BMI'], axis=1)\n\n# Remove FGC-FGC_CU_Zone, FGC-FGC_PU_Zone, and FGC-FGC_TL_Zone\ntrain_cleaned = train_cleaned.drop(['FGC-FGC_CU_Zone', 'FGC-FGC_PU_Zone', 'FGC-FGC_TL_Zone'], axis=1)\n\n# Remove the following variables from train: BIA-BIA_BMR, BIA-BIA_TBW, BIA-BIA_ECW, BIA-BIA_LDM, BIA-BIA_ICW, BIA-BIA_SMM, BIA-BIA_DEE, BIA-BIA_LST, and BIA-BIA_BMC\ntrain_cleaned=train_cleaned.drop(columns=['BIA-BIA_BMR', 'BIA-BIA_TBW', 'BIA-BIA_ECW', 'BIA-BIA_LDM', 'BIA-BIA_ICW', 'BIA-BIA_SMM', 'BIA-BIA_DEE', 'BIA-BIA_LST', 'BIA-BIA_BMC'])\n\n# Remove the Fitness_Endurance-Max_Stage variable (based on previous exploration)\ntrain_cleaned = train_cleaned.drop(['Fitness_Endurance-Max_Stage'], axis=1)\n\n# Remove all variables with Season in their name\ntrain_cleaned = train_cleaned.loc[:,~train_cleaned.columns.str.contains('Season')]\n\n#We also remove these variables from the test data.\ntest_cleaned = test_cleaned.drop(['BIA-BIA_BMI'], axis=1)\ntest_cleaned = test_cleaned.drop(['FGC-FGC_CU_Zone', 'FGC-FGC_PU_Zone', 'FGC-FGC_TL_Zone'], axis=1)\ntest_cleaned = test_cleaned.drop(columns=['BIA-BIA_BMR', 'BIA-BIA_TBW', 'BIA-BIA_ECW', 'BIA-BIA_LDM', 'BIA-BIA_ICW', 'BIA-BIA_SMM', 'BIA-BIA_DEE', 'BIA-BIA_LST', 'BIA-BIA_BMC'])\ntest_cleaned = test_cleaned.drop(['Fitness_Endurance-Max_Stage'], axis=1)\ntest_cleaned = test_cleaned.loc[:,~test_cleaned.columns.str.contains('Season')]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T14:37:19.757381Z","iopub.execute_input":"2024-12-18T14:37:19.757804Z","iopub.status.idle":"2024-12-18T14:37:19.786883Z","shell.execute_reply.started":"2024-12-18T14:37:19.757768Z","shell.execute_reply":"2024-12-18T14:37:19.785626Z"}},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":"**Impute Missing Outcome Values**\n\nSome participants have missing PCIAT scores. We'll impute these using KNN","metadata":{}},{"cell_type":"code","source":"#First we'll create a list of columns that hold the PCIAT values\npciats = [col for col in train_cleaned.columns if 'PCIAT' in col]\npciats.remove('PCIAT-PCIAT_Total')\n\n#Create a new copy of the data frame for imputation. Remove rows where all values in pciats are NaN\ntrain_imp_KNN = train_cleaned.copy()\ntrain_imp_KNN['pciatsnotna_sum'] = train_imp_KNN[pciats].notna().sum(axis=1)\ntrain_imp_KNN = train_imp_KNN[train_imp_KNN['pciatsnotna_sum'] != 0]\ntrain_imp_KNN.reset_index(drop=True, inplace=True)\n\n#Remove the pciatsnotna_sum variable\ntrain_imp_KNN.drop(columns=['pciatsnotna_sum'], inplace=True)\n\n#Identify the rows with at least one NaN value\ntrain_imp_KNN['nan_rows'] = train_imp_KNN[pciats].isnull().any(axis=1)\n\n# Create a copy of train_imp_KNN\ntrain_imp_KNN2 = train_imp_KNN.copy()\n# define imputer\nNumber_Neighbors=5\nimputer = KNNImputer(n_neighbors=Number_Neighbors, weights='uniform', metric='nan_euclidean')\n\n#The imputer.fit_transform function outputs a numpy array. So first I do the fitting, then convert the output back to a pandas dataframe.\n\nimputations=imputer.fit_transform(train_imp_KNN[pciats])\ndf2 = pd.DataFrame(imputations, columns=pciats)\n\n#Next take the result and insert into the original dataframe. \n\ntrain_imp_KNN[pciats]=train_imp_KNN[pciats].fillna(df2[pciats])\n\n#Remove the nan_rows variable\ntrain_imp_KNN.drop(columns=['nan_rows'], inplace=True)\n\n#Recalculate the PCIAT total score.\ntrain_imp_KNN['PCIAT-PCIAT_Total'] = train_imp_KNN[pciats].sum(axis=1)\n\n#Now we can calculate a new sii score with the imputed values. \nbins = [0, 30, 49,79,101]\nlabels = [0,1,2,3]\n#train_imp_KNN['sii'] = pd.cut(train_imp_KNN['PCIAT-PCIAT_Total'], bins=bins, labels=labels, right=False)\ntrain_imp_KNN['sii'] = np.digitize(train_imp_KNN['PCIAT-PCIAT_Total'], bins=bins)-1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T14:37:23.317619Z","iopub.execute_input":"2024-12-18T14:37:23.318129Z","iopub.status.idle":"2024-12-18T14:37:23.474007Z","shell.execute_reply.started":"2024-12-18T14:37:23.318081Z","shell.execute_reply":"2024-12-18T14:37:23.472669Z"}},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":"**The Model**\n\nThe process:\n1. Create a list of predictors\n2. Impute missing predictor values for the predictor variables\n3. Use SMOTE to oversample the minority class sii=3\n4. Use a tuned gradient boosting regressor to predict PCIAT scores\n5. Use a \"tuned\" set of bins to convert PCIAT scores to sii scores\n6. Output the prediction","metadata":{}},{"cell_type":"code","source":"#Create an initial list of predictor columns\npredictors = train_cleaned.columns.tolist()\nif 'id' in predictors:\n    predictors.remove('id')\nif 'sii' in predictors:\n    predictors.remove('sii')\npredictors = [x for x in predictors if 'PCIAT' not in x]\npredictors = [x for x in predictors if 'Season' not in x]\n\n# Create an augmented list that will be used for oversampling\npredictors_plus = predictors + ['PCIAT-PCIAT_Total']\n\n# Create a list of \"key features\" based on a previously-run random forest\nkeyfeatures = ['Basic_Demos-Age',\n 'Physical-Height',\n 'PreInt_EduHx-computerinternet_hoursday',\n 'BIA-BIA_FFM',\n 'SDS-SDS_Total_Raw',\n 'Physical-Weight',\n 'ENMO_Avg_Active_Days_MVPA110',\n 'FGC-FGC_CU']\n\n\n# Impute missing values\nmice = Custom_MICE_Imputer()\ntrain_cleaned_imputed = mice.fit_transform(train_imp_KNN)\ntrain_cleaned_imputed = zone_encoder(train_cleaned_imputed)\ntest_cleaned_imputed = mice.fit_transform(test_cleaned)\ntest_cleaned_imputed = zone_encoder(test_cleaned_imputed)\n\n# Set up SMOTE to create 148 (rather than 37) instances of sii=3\nsiiratios = {0: 1530, 1: 765, 2:403, 3:148}\noversample = SMOTE(sampling_strategy=siiratios)\n\n# Oversample with SMOTE\nX, y = oversample.fit_resample(train_cleaned_imputed[predictors_plus], train_cleaned_imputed['sii'])\n\n# Create the MLR model\nmlr_key_pipe = Pipeline([\n                ('selector', ColumnTransformer([('selector', 'passthrough', keyfeatures)], remainder=\"drop\")),\n                ('linear', LinearRegression())])\n\n# Fit and make predictions\nmlr_key_pipe.fit(X[predictors], X['PCIAT-PCIAT_Total'])\npred = mlr_key_pipe.predict(test_cleaned_imputed[predictors])\n\n# \"Tuned\" bins.\nbins_mod = [0, 27, 39, 79, 100]\npred_bin_mod = np.digitize(pred, bins_mod)-1\n\n# Add predictions to test_cleaned_imputed\ntest_cleaned_imputed['sii'] = pred_bin_mod\n\n# Create final output df\npredicted_sii = test_cleaned_imputed[['id','sii']]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T15:13:08.428900Z","iopub.execute_input":"2024-12-18T15:13:08.430131Z","iopub.status.idle":"2024-12-18T15:13:12.362070Z","shell.execute_reply.started":"2024-12-18T15:13:08.430087Z","shell.execute_reply":"2024-12-18T15:13:12.360858Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/impute/_iterative.py:785: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"# Save the predicted sii values\npredicted_sii.to_csv('submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T14:37:36.873567Z","iopub.execute_input":"2024-12-18T14:37:36.873998Z","iopub.status.idle":"2024-12-18T14:37:36.887766Z","shell.execute_reply.started":"2024-12-18T14:37:36.873960Z","shell.execute_reply":"2024-12-18T14:37:36.886557Z"}},"outputs":[],"execution_count":36}]}