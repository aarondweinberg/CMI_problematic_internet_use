{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by loading the data and separating out the three subsets of variables we'll be regressing: physical, fitness, and BIA\n",
    "\n",
    "Big question (for our mentor): for ridge regression, do we actually want to split this up three ways...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/94/nhwg0cqn70nf3n1xr6j68x440000gp/T/ipykernel_63774/959472695.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_physical['PCIAT-PCIAT_Total'] = train['PCIAT-PCIAT_Total']\n"
     ]
    }
   ],
   "source": [
    "# Import the data, including the outcome variable (in this case, PCIAT-PCIAT_Total)\n",
    "# Note that (ridge) regression can't handle missing values. Since we'll do imputation later, we'll need to drop NaNs\n",
    "\n",
    "# Load the (split) data set train_imp.csv\n",
    "train = pd.read_csv('train_cleaned.csv')\n",
    "\n",
    "# Create a list of the 'physical' variables. Based on previous exploration, we can remove some variables and observations due to high rates of NaNs\n",
    "physical_vars = [col for col in train.columns if col.startswith('Physical') and train[col].dtype in ['float64', 'int64']]\n",
    "train_physical = train[physical_vars]\n",
    "train_physical['PCIAT-PCIAT_Total'] = train['PCIAT-PCIAT_Total']\n",
    "train_physical = train_physical.drop(columns=['Physical-Waist_Circumference'])\n",
    "train_physical = train_physical.dropna()\n",
    "train_physical.name = 'train_physical'\n",
    "\n",
    "# Create a list of the 'fitness' variables. Based on previous exploration, we can remove some variables and observations due to high rates of NaNs\n",
    "# Note that the Fitness_Endurance variables and the grip strength variables have too many missing values to include in the list\n",
    "train['FGC_Zone_Total'] = train['FGC-FGC_CU_Zone'] + train['FGC-FGC_PU_Zone'] + train['FGC-FGC_SRL_Zone'] + train['FGC-FGC_SRR_Zone'] + train['FGC-FGC_TL_Zone']\n",
    "fitness_vars = ['FGC-FGC_CU','FGC-FGC_PU','FGC-FGC_SRL','FGC-FGC_SRR','FGC-FGC_TL', 'FGC_Zone_Total', 'PCIAT-PCIAT_Total']\n",
    "train_fitness = train[fitness_vars]\n",
    "train_fitness = train_fitness.dropna()\n",
    "train_fitness.name = 'train_fitness'\n",
    "\n",
    "# Create a new data set from train called train_bia that includes all variables that start with BIA-BIA_\n",
    "train_bia = train[[col for col in train.columns if col.startswith('BIA-BIA_')]]\n",
    "train_bia = train_bia.drop(columns=['BIA-BIA_Activity_Level_num','BIA-BIA_Frame_num'])\n",
    "train_bia['PCIAT-PCIAT_Total'] = train['PCIAT-PCIAT_Total']\n",
    "train_bia = train_bia.dropna()\n",
    "train_bia.name = 'train_bia'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll do some hyperparameter tuning for the ridge regression\n",
    "\n",
    "We'll make a Ridge regression model and compute the MSE for each of several values of alpha\n",
    "\n",
    "We'll do this on a k-fold split of the training data (because that's always a good thing to do?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The alpha value with the lowest RMSE for the train_physical variables is 1 . The mean RMSE was 18.651445261657724  and the standard deviation was 0.06647389746200107\n",
      "The alpha value with the lowest RMSE for the train_fitness variables is 10 . The mean RMSE was 19.069065125676516  and the standard deviation was 0.12410237832274675\n",
      "The alpha value with the lowest RMSE for the train_bia variables is 10 . The mean RMSE was 18.680638382918563  and the standard deviation was 0.03897038533887072\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set up the kfold split\n",
    "num_splits = 5\n",
    "kfold = KFold(num_splits, shuffle=True)\n",
    "\n",
    "# Define a range of alpha values\n",
    "alphas = [0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000]\n",
    "numalphas = len(alphas)\n",
    "\n",
    "# Create an empty array with num_splits rows and numalphas columns\n",
    "rmses = np.zeros((num_splits, numalphas))\n",
    "\n",
    "# Iterate over the three data sets\n",
    "listofdatasets = [train_physical, train_fitness, train_bia]\n",
    "\n",
    "# A data frame to store the optimal alpha values\n",
    "bestalphas = pd.DataFrame(index=range(0,len(listofdatasets)))\n",
    "bestalphas['dfname'] = ''\n",
    "bestalphas['best_alpha'] = 0\n",
    "\n",
    "k=0\n",
    "for df in listofdatasets:\n",
    "    i = 0\n",
    "    for train_index, test_index in kfold.split(df):\n",
    "        tt_X = df.iloc[train_index].drop(columns=['PCIAT-PCIAT_Total'])\n",
    "        tt_y = df.iloc[train_index]['PCIAT-PCIAT_Total']\n",
    "        ho_X = df.iloc[test_index].drop(columns=['PCIAT-PCIAT_Total'])\n",
    "        ho_y = df.iloc[test_index]['PCIAT-PCIAT_Total']\n",
    "\n",
    "        # Iterate over alpha values with counter j\n",
    "        j = 0\n",
    "        for alpha in alphas:\n",
    "            ridge_pipe = Pipeline([('scale', StandardScaler()),('ridge', Ridge(alpha=alpha, max_iter=5000000) )])\n",
    "            ridge_pipe.fit(tt_X, tt_y)\n",
    "            y_pred = ridge_pipe.predict(ho_X)\n",
    "            rmses[i, j] = root_mean_squared_error(ho_y, y_pred)\n",
    "            \n",
    "            j=j+1\n",
    "\n",
    "        i=i+1\n",
    "\n",
    "    # Compute the mean of each column of rmses\n",
    "    mean_rmses_within_alphas = np.mean(rmses, axis=0)\n",
    "\n",
    "    # Compute the mean and standard deviation of each row of rmses\n",
    "    mean_rmses = np.mean(mean_rmses_within_alphas, axis=0)\n",
    "    std_rmses = np.std(mean_rmses_within_alphas, axis=0)\n",
    "\n",
    "    # Identify the column of min_rmse that contains the minimum value\n",
    "    best_alpha_index = np.argmin(mean_rmses_within_alphas)\n",
    "\n",
    "    bestalphas.loc[k,'dfname'] = df.name\n",
    "    bestalphas.loc[k,'best_alpha'] = alphas[best_alpha_index]\n",
    "\n",
    "    print('The alpha value with the lowest RMSE for the', df.name ,'variables is', alphas[best_alpha_index],'. The mean RMSE was', mean_rmses, ' and the standard deviation was', std_rmses )\n",
    "    i=0\n",
    "    j=0\n",
    "    k=k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dfname</th>\n",
       "      <th>best_alpha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_physical</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_fitness</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_bia</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dfname  best_alpha\n",
       "0  train_physical           1\n",
       "1   train_fitness          10\n",
       "2       train_bia          10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestalphas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the hyperparameter is tuned, we'll compare the performance of the ridge regression and a PCA.\n",
    "\n",
    "Note that in previous explorations we've identified n=3 as the \"ideal\" number of PCA components for each set of predictor variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_physical Ridge Training MSE: 18.533899980839752\n",
      "train_physical PCA Training MSE: 19.235425201158005\n",
      "train_fitness Ridge Training MSE: 18.695066004261484\n",
      "train_fitness PCA Training MSE: 19.37497603121806\n",
      "train_bia Ridge Training MSE: 18.770987125663712\n",
      "train_bia PCA Training MSE: 19.057632568137585\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "for df in listofdatasets:\n",
    "    # Identify the best alpha value we computed earlier\n",
    "    best_alpha = bestalphas.loc[bestalphas['dfname'] == df.name, 'best_alpha'].values[0]\n",
    "    \n",
    "    # Instantiate some models. From previous exploration, we've been using 3 components for the PCA\n",
    "    ridge_pipe = Pipeline([('scale', StandardScaler()), ('ridge', Ridge(alpha = best_alpha, max_iter=5000000))])\n",
    "    pca_pipe = Pipeline([('scale', StandardScaler()), ('pca', PCA(n_components=3)), ('reg', LinearRegression())])\n",
    "\n",
    "    # The training data\n",
    "    X_train = df.iloc[train_index].drop(columns=['PCIAT-PCIAT_Total'])\n",
    "    y_train =  df.iloc[train_index]['PCIAT-PCIAT_Total']\n",
    "\n",
    "    # Fit the models to the training data\n",
    "    ridge_pipe.fit(X_train, y_train)\n",
    "    pca_pipe.fit(X_train, y_train)\n",
    "\n",
    "    # Find the model predictions on the training set\n",
    "    ridge_train_preds = ridge_pipe.predict(X_train)\n",
    "    pca_train_preds = pca_pipe.predict(X_train)\n",
    "\n",
    "    # Find the mse on the training set\n",
    "    ridge_train_rmse = root_mean_squared_error(y_train, ridge_train_preds)\n",
    "    pca_train_rmse = root_mean_squared_error(y_train, pca_train_preds)\n",
    "\n",
    "    # Results\n",
    "    print(df.name, f\"Ridge Training MSE: {ridge_train_rmse}\")\n",
    "    print(df.name, f\"PCA Training MSE: {pca_train_rmse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_fall_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
