{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tuning the Gradient Boosting Regressor**\n",
    "\n",
    "The purpose of this notebook is to tune a gradient boosting regressor model for Kaggle submission using the full training dataset.\n",
    "\n",
    "Like in the Kaggle competition, performance will be measured using Cohen's kappa.\n",
    "\n",
    "Our target variable is sii. However, in the original data set, sii was computed from adding the twenty PCIAT variables and then grouping the total into bins. We will predict sii by first predicting PCIAT_Total, then computing sii using the \"tuned\" bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# CustomImputers includes two imputers (using KNN and interative imputing) for this dataset\n",
    "# and also two functions to transform fitnessgram values into fitnessgram zones, and to round sii values\n",
    "from CustomImputers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading and Cleaning**\n",
    "\n",
    "Our original code generated an 80/20 test/train split of the original data at the end of the Data_Cleaning notebook.\n",
    "\n",
    "We'll re-do that work here using the full training data, starting by cleaning the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the original training data\n",
    "train=pd.read_csv('train_original.csv')\n",
    "\n",
    "# Load the accelerometer data set Accelerometer_enmo_anglez_daily_averages.csv\n",
    "accel = pd.read_csv('Accelerometer_enmo_anglez_daily_averages.csv')\n",
    "\n",
    "# Join train and accel  on the 'id' column and accel on the 'ID' column\n",
    "train = train.join(accel.set_index('ID'), on='id', how='left')\n",
    "\n",
    "# Create a new variable 'FGC-FGC_SR' that is the mean of FGC-FGC_SRL and FGC-FGC_SRR\n",
    "train['FGC-FGC_SR'] = train[['FGC-FGC_SRL', 'FGC-FGC_SRR']].mean(axis=1)\n",
    "\n",
    "# Remove the old sit & reach variables\n",
    "train = train.drop(columns=['FGC-FGC_SRL', 'FGC-FGC_SRR', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR_Zone'])\n",
    "\n",
    "# Create a new variable 'FGC-FGC_SR_Zone' that is equal to 1 if any of the following are true:\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_SR >= 8\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_SR_Zone >= 9 and Basic_Demos-Age is between 5 and 10\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_SR_Zone >= 10 and Basic_Demos-Age is between 11 and 14\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_SR_Zone >= 12 and Basic_Demos-Age is at least 15\n",
    "\n",
    "# One way to do this is to define a function that would take sex, age, and SR value as inputs and output 1 or 0\n",
    "def sitreachzone(sex, age, sr):\n",
    "    try:\n",
    "        if np.isnan(sr):\n",
    "            return np.nan\n",
    "        elif sex == 0 and sr>=8:\n",
    "            return 1\n",
    "        elif sex == 1 and age >= 15 and sr >= 12:\n",
    "            return 1\n",
    "        elif sex == 1 and age >= 11 and sr >= 10:\n",
    "            return 1\n",
    "        elif sex == 1 and age >= 5 and sr >= 9:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# Apply sitreachzone to create a new column using the columns Basic_Demos-Sex, Basic_Demos-Age, and FGC-FGC_SR as inputs\n",
    "train['FGC-FGC_SR_Zone'] = train.apply(lambda x: sitreachzone(x['Basic_Demos-Sex'], x['Basic_Demos-Age'], x['FGC-FGC_SR']), axis=1)\n",
    "\n",
    "# Create a new variable that is 1 when PAQA/C Total is at least 2.75/2.73, 0 if it's less than these cutoffs, and NaN if PAQA/C is NaN\n",
    "train['PAQA_Zone'] = np.where(train['PAQ_A-PAQ_A_Total']>=2.75, 1, 0)\n",
    "train['PAQA_Zone'] = np.where(train['PAQ_A-PAQ_A_Total'].isnull(), np.nan, train['PAQA_Zone'])\n",
    "\n",
    "train['PAQC_Zone'] = np.where(train['PAQ_C-PAQ_C_Total']>=2.73, 1, 0)\n",
    "train['PAQC_Zone'] = np.where(train['PAQ_C-PAQ_C_Total'].isnull(), np.nan, train['PAQC_Zone'])\n",
    "\n",
    "# Create new variables that merge the three PAQA/C variables\n",
    "train['PAQ_Total']=train['PAQ_C-PAQ_C_Total']\n",
    "train.loc[train['PAQ_Total'].isnull(),'PAQ_Total']=train['PAQ_A-PAQ_A_Total']\n",
    "\n",
    "train['PAQ_Season']=train['PAQ_C-Season']\n",
    "train.loc[train['PAQ_Season'].isnull(),'PAQ_Season']=train['PAQ_A-Season']\n",
    "\n",
    "train['PAQ_Zone']=train['PAQC_Zone']\n",
    "train.loc[train['PAQ_Zone'].isnull(),'PAQ_Zone']=train['PAQA_Zone']\n",
    "\n",
    "# Drop the PAQ variables we no longer need\n",
    "train=train.drop(columns=['PAQ_C-PAQ_C_Total', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season', 'PAQ_A-Season', 'PAQA_Zone', 'PAQC_Zone'])\n",
    "\n",
    "# Combine the minutes and seconds of Fitness_Endurance into a single number (total number of seconds)\n",
    "train['Fitness_Endurance_Total_Time_Sec'] = train['Fitness_Endurance-Time_Mins'] * 60 + train['Fitness_Endurance-Time_Sec']\n",
    "\n",
    "train=train.drop(columns=['Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec'])\n",
    "\n",
    "# Remove the SDS-SDS_Total_T variable from train\n",
    "train=train.drop(columns=['SDS-SDS_Total_T'])\n",
    "\n",
    "# Remove the FGC-FGC_GSND, FGC-FGC_GSND_Zone, FGC-FGC_GSD, and FGC-FGC_GSD_Zone variables\n",
    "train=train.drop(columns=['FGC-FGC_GSND', 'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone'])\n",
    "\n",
    "# Create a list of numerical columns of type float. Note that these columns include the \"Zone\" variables which are really categorical/ordinal:\n",
    "float_columns = train.select_dtypes(include=['float']).columns\n",
    "\n",
    "# Change negative values to NaN\n",
    "train[train[float_columns] < 0] = np.nan\n",
    "\n",
    "# For each variable that starts with 'Physical-' replace any values that are 0 with NaN\n",
    "for column in train.columns:\n",
    "    if column.startswith('Physical-'):\n",
    "        train[column] = train[column].replace(0, np.nan)\n",
    "\n",
    "# For each column in float_columns, identify entries that are 5 standard deviations above or below the mean and replace them with NaN\n",
    "for column in float_columns:\n",
    "    train[column] = train[column].mask(train[column] > train[column].mean() + 5 * train[column].std())\n",
    "    train[column] = train[column].mask(train[column] < train[column].mean() - 5 * train[column].std())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outcome-Imputing**\n",
    "\n",
    "The original data had missing PCIAT and sii scores. We'll impute them using KNN.\n",
    "\n",
    "Original code is in the Outcome_Imputing notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we'll create a list of columns that hold the PCIAT values\n",
    "pciats = [col for col in train.columns if 'PCIAT' in col]\n",
    "pciats.remove('PCIAT-PCIAT_Total')\n",
    "pciats.remove('PCIAT-Season')\n",
    "\n",
    "#Create a new copy of the data frame for imputation. Remove rows where all values in pciats are NaN\n",
    "train_imp_KNN = train.copy()\n",
    "train_imp_KNN['pciatsnotna_sum'] = train_imp_KNN[pciats].notna().sum(axis=1)\n",
    "train_imp_KNN = train_imp_KNN[train_imp_KNN['pciatsnotna_sum'] != 0]\n",
    "train_imp_KNN.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#Remove the pciatsnotna_sum variable\n",
    "train_imp_KNN.drop(columns=['pciatsnotna_sum'], inplace=True)\n",
    "\n",
    "#Identify the rows with at least one NaN value\n",
    "train_imp_KNN['nan_rows'] = train_imp_KNN[pciats].isnull().any(axis=1)\n",
    "\n",
    "# Create a copy of train_imp_KNN\n",
    "train_imp_KNN2 = train_imp_KNN.copy()\n",
    "# define imputer\n",
    "Number_Neighbors=5\n",
    "imputer = KNNImputer(n_neighbors=Number_Neighbors, weights='uniform', metric='nan_euclidean')\n",
    "\n",
    "#The imputer.fit_transform function outputs a numpy array. So first I do the fitting, then convert the output back to a pandas dataframe.\n",
    "\n",
    "imputations=imputer.fit_transform(train_imp_KNN[pciats])\n",
    "df2 = pd.DataFrame(imputations, columns=pciats)\n",
    "\n",
    "#Next take the result and insert into the original dataframe. \n",
    "\n",
    "train_imp_KNN[pciats]=train_imp_KNN[pciats].fillna(df2[pciats])\n",
    "\n",
    "#Remove the nan_rows variable\n",
    "train_imp_KNN.drop(columns=['nan_rows'], inplace=True)\n",
    "\n",
    "#Recalculate the PCIAT total score.\n",
    "train_imp_KNN['PCIAT-PCIAT_Total'] = train_imp_KNN[pciats].sum(axis=1)\n",
    "\n",
    "#Now we can calculate a new sii score with the imputed values. \n",
    "bins = [0, 30, 49,79,100]\n",
    "labels = [0,1,2,3]\n",
    "train_imp_KNN['sii'] = pd.cut(train_imp_KNN['PCIAT-PCIAT_Total'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "train_cleaned = train_imp_KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Reduction**\n",
    "\n",
    "Here we reproduce the Feature_Reduction notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of numeric features\n",
    "numeric_features = train_cleaned.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Remove all PCIAT component variables from numeric features\n",
    "numeric_features = numeric_features.drop(list(train_cleaned.filter(regex='PCIAT-PCIAT_0').columns))\n",
    "numeric_features = numeric_features.drop(list(train_cleaned.filter(regex='PCIAT-PCIAT_1').columns))\n",
    "numeric_features = numeric_features.drop(list(train_cleaned.filter(regex='PCIAT-PCIAT_2').columns))\n",
    "\n",
    "# Remove sii and PCIAT-PCIAT_Total from numeric_features\n",
    "numeric_features = numeric_features.drop(['PCIAT-PCIAT_Total'])\n",
    "\n",
    "# Remove some variables we won't need\n",
    "\n",
    "# Remove the variables 'id', 'BIA-BIA_BMI'\n",
    "train_cleaned = train_cleaned.drop(['BIA-BIA_BMI'], axis=1)\n",
    "\n",
    "# Remove FGC-FGC_CU_Zone, FGC-FGC_PU_Zone, and FGC-FGC_TL_Zone\n",
    "train_cleaned = train_cleaned.drop(['FGC-FGC_CU_Zone', 'FGC-FGC_PU_Zone', 'FGC-FGC_TL_Zone'], axis=1)\n",
    "\n",
    "# Remove the following variables from train: BIA-BIA_BMR, BIA-BIA_TBW, BIA-BIA_ECW, BIA-BIA_LDM, BIA-BIA_ICW, BIA-BIA_SMM, BIA-BIA_DEE, BIA-BIA_LST, and BIA-BIA_BMC\n",
    "train_cleaned=train_cleaned.drop(columns=['BIA-BIA_BMR', 'BIA-BIA_TBW', 'BIA-BIA_ECW', 'BIA-BIA_LDM', 'BIA-BIA_ICW', 'BIA-BIA_SMM', 'BIA-BIA_DEE', 'BIA-BIA_LST', 'BIA-BIA_BMC'])\n",
    "\n",
    "# Remove the Fitness_Endurance-Max_Stage variable (based on previous exploration)\n",
    "train_cleaned = train_cleaned.drop(['Fitness_Endurance-Max_Stage'], axis=1)\n",
    "\n",
    "# Remove all variables with Season in their name\n",
    "train_cleaned = train_cleaned.loc[:,~train_cleaned.columns.str.contains('Season')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating Lists**\n",
    "\n",
    "In this section, we'll create lists of predictors based on previous exploration\n",
    "\n",
    "For the purpose of developing our model(s), we'll work with data that include the imputed outcome (PCIAT_Total and/or sii) scores AND have cleaned predictors.\n",
    "\n",
    "In the final version of our code (which we'll submit to Kaggle), we'll work with data with cleaned predictors but won't have any access to the outcome scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an initial list of predictor columns\n",
    "\n",
    "predictors = train_cleaned.columns.tolist()\n",
    "if 'id' in predictors:\n",
    "    predictors.remove('id')\n",
    "if 'sii' in predictors:\n",
    "    predictors.remove('sii')\n",
    "predictors = [x for x in predictors if 'PCIAT' not in x]\n",
    "predictors = [x for x in predictors if 'Season' not in x]\n",
    "\n",
    "# Create an augmented list that will be used for \n",
    "predictors_plus = predictors + ['PCIAT-PCIAT_Total']\n",
    "\n",
    "# Create a list of \"key features\" based on work in the Feature Selection notebook\n",
    "keyfeatures = ['Basic_Demos-Age',\n",
    " 'Physical-Height',\n",
    " 'PreInt_EduHx-computerinternet_hoursday',\n",
    " 'BIA-BIA_FFM',\n",
    " 'SDS-SDS_Total_Raw',\n",
    " 'Physical-Weight',\n",
    " 'ENMO_Avg_Active_Days_MVPA110',\n",
    " 'FGC-FGC_CU']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tuning the Bins**\n",
    "\n",
    "We noticed that our model struggled to predict higher output values.\n",
    "\n",
    "When we adjusted the values for converting PCIAT scores to sii scores, we noticed an improvement in prediction when we lowered the cutpoints.\n",
    "\n",
    "We sought to \"tune\" these cutpoints. However, we need to be mindful of overfitting. \n",
    "\n",
    "We'll look at the combination of cutpoints that maximize kappa, and then from the top 20 (or so) select the cutpoints that are closest to the original ones (as measured by euclidean distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/impute/_iterative.py:825: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/impute/_iterative.py:825: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load packages\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# We'll use the multiple linear regression with \"key\" features to test the cutpoints\n",
    "# This model performed as good or better than most of the other (untuned) models\n",
    "# Since it is quick to run, it should be a decent choice for doing this tuning\n",
    "\n",
    "# Start by setting up the MLR pipeline\n",
    "mlr_key_pipe=Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "                ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                ('selector', ColumnTransformer([('selector', 'passthrough', keyfeatures)], remainder=\"drop\")),\n",
    "                ('linear', LinearRegression())])\n",
    "\n",
    "# Set the number of k-fold splits\n",
    "num_splits = 5\n",
    "\n",
    "# Set the number of different cutpoints to try\n",
    "num_bincuts=15\n",
    "\n",
    "# Set up SMOTE\n",
    "siiratios = {0: 1530, 1: 765, 2:403, 3:148}\n",
    "oversample = SMOTE(sampling_strategy=siiratios)\n",
    "\n",
    "# Create an array with len(modellist) rows and len(methodlist) columns\n",
    "output = np.zeros((num_bincuts, num_bincuts,num_bincuts, num_splits))\n",
    "\n",
    "#Make a KFold object, stratified on sii=3 values\n",
    "kfold= StratifiedKFold(n_splits=num_splits, shuffle=True)\n",
    "\n",
    "## i will count the split number \n",
    "i = 0\n",
    "\n",
    "for train_index, test_index in kfold.split(train_cleaned, train_cleaned['sii']):\n",
    "    train_tt = train_cleaned.iloc[train_index]\n",
    "    train_ho = train_cleaned.iloc[test_index]\n",
    "\n",
    "    # Compute and impute\n",
    "    mice = Custom_MICE_Imputer()\n",
    "    train_tt = mice.fit_transform(train_tt)\n",
    "    train_tt = zone_encoder(train_tt)\n",
    "    train_ho = mice.fit_transform(train_ho)\n",
    "    train_ho = zone_encoder(train_ho)\n",
    "\n",
    "    #Oversample with SMOTE\n",
    "    X, y = oversample.fit_resample(train_tt[predictors_plus], train_tt['sii'])\n",
    "\n",
    "    \n",
    "    # Fit the pipe and make predictions\n",
    "    mlr_key_pipe.fit(train_tt[predictors], train_tt['PCIAT-PCIAT_Total'])\n",
    "    pred = mlr_key_pipe.predict(train_ho[predictors])\n",
    "\n",
    "    # Iterate through values of the three cutpoints    \n",
    "    for r in range(num_bincuts):\n",
    "        for s in range(num_bincuts):\n",
    "            for t in range(num_bincuts):\n",
    "                bins = [0, 30-r, 49-s,79-t,100]\n",
    "                pred_bin_mod = np.digitize(pred, bins)-1\n",
    "                # Compute kappa for the binned predictions\n",
    "                kappa_sii_comp_mod = cohen_kappa_score(train_ho['sii'], pred_bin_mod, weights='quadratic')\n",
    "                output[r,s,t,i]=kappa_sii_comp_mod\n",
    "                \n",
    "    i=i+1\n",
    "\n",
    "# Create a new array by computing the average of the values in output along the third axis\n",
    "output_avg = np.mean(output, axis=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll examine the output of the tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top values: [0.45330481 0.45330481 0.45330481 0.45330481 0.45301702 0.45301702\n",
      " 0.45301702 0.45301702 0.45301702 0.45301702 0.45301702 0.45301702\n",
      " 0.45301702 0.45301702 0.45301702 0.4519785  0.4519785  0.4519785\n",
      " 0.4519785  0.45192565]\n",
      "Locations of the top values: [(np.int64(3), np.int64(11), np.int64(14)), (np.int64(3), np.int64(11), np.int64(12)), (np.int64(3), np.int64(11), np.int64(13)), (np.int64(3), np.int64(11), np.int64(11)), (np.int64(3), np.int64(11), np.int64(1)), (np.int64(3), np.int64(11), np.int64(2)), (np.int64(3), np.int64(11), np.int64(0)), (np.int64(3), np.int64(11), np.int64(5)), (np.int64(3), np.int64(11), np.int64(3)), (np.int64(3), np.int64(11), np.int64(4)), (np.int64(3), np.int64(11), np.int64(6)), (np.int64(3), np.int64(11), np.int64(10)), (np.int64(3), np.int64(11), np.int64(7)), (np.int64(3), np.int64(11), np.int64(8)), (np.int64(3), np.int64(11), np.int64(9)), (np.int64(3), np.int64(12), np.int64(11)), (np.int64(3), np.int64(12), np.int64(14)), (np.int64(3), np.int64(12), np.int64(13)), (np.int64(3), np.int64(12), np.int64(12)), (np.int64(5), np.int64(11), np.int64(12))]\n"
     ]
    }
   ],
   "source": [
    "# Flatten the array and sort the indices in descending order\n",
    "sorted_indices_flat = np.argsort(output_avg.ravel())[::-1]\n",
    "\n",
    "#Decide how many top values you want to look at. \n",
    "n=20\n",
    "\n",
    "# Get the flat indices of the top two values\n",
    "top_flat_indices = sorted_indices_flat[:n]\n",
    "\n",
    "# Convert the flat indices to 3D indices\n",
    "top_indices = [np.unravel_index(idx, output_avg.shape) for idx in top_flat_indices]\n",
    "\n",
    "# Retrieve the top two values\n",
    "top_values = output_avg.ravel()[top_flat_indices]\n",
    "\n",
    "print(\"Top values:\", top_values)\n",
    "print(\"Locations of the top values:\", top_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results of Cutpoint Tuning**\n",
    "\n",
    "We ran the code above, looked at the top 20 (in terms of kappa) sets of cutpoints, and selected the combination of cutpoints that minimized the Euclidean distance to the original cutpoints. Then we repeated this two more times. The results were:\n",
    "\n",
    "* First time: [0, 30-3, 49-6,79-8,100] = [0, 27, 43, 71, 100]\n",
    "* Second time: [0, 30-3, 49-11,79-0,100] = [0, 27, 38, 79, 100]\n",
    "* Third time: [0, 30-3, 49-10,79-0,100] = [0, 27, 39, 79, 100]\n",
    "\n",
    "We'll use this third set of cutpoints.\n",
    "\n",
    "Re-running with the full data gives us cutpoints of:\n",
    "* [0, 30-3, 46-11, 79-14, 100] (top performer)\n",
    "* [0, 30-3, 46-11, 79-0, 100] (not far behind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter Tuning**\n",
    "\n",
    "The models above were run \"out of the box.\" In the next sections, we'll try to tune a few of them (random forest, gradient boost, and logistic regression inside an ordinal classifier) to see how much we can improve their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check for Overfitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tuning Gradient Boosting Regressor for PCIAT**\n",
    "\n",
    "There are lots of parameters to consider here.\n",
    "\n",
    "We'll use some suggestions from various websites to think about a parameter grid, being mindful of the amount of time it will take to run the code.\n",
    "\n",
    "Sources consulted:\n",
    "* https://stackoverflow.com/questions/49500313/tune-parameters-in-gradient-boosting-reggression-with-cross-validation-sklearn\n",
    "* https://medium.com/all-things-ai/in-depth-parameter-tuning-for-gradient-boosting-3363992e9bae\n",
    "* https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n",
    "\n",
    "Note that we have a total of ~2500 observations with PCIAT scores in the entire dataset, a very small (32) number of scores with sii=3 and a relatively small number of scores with sii=2. SMOTE could help with this, but we can't use it inside a pipe when we're trying to predict PCIAT. (We could do this tuning manually, but due to time constraints, we're going to stick with using GridSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-9 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-9 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-9 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-9 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-9 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-9 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-9 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-9 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-9 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-9 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-9 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-9 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;mice_impute&#x27;, Custom_MICE_Imputer()),\n",
       "                                       (&#x27;add_zones&#x27;,\n",
       "                                        FunctionTransformer(func=&lt;function zone_encoder at 0x305cc5800&gt;)),\n",
       "                                       (&#x27;grad&#x27;, GradientBoostingRegressor())]),\n",
       "             param_grid={&#x27;grad__learning_rate&#x27;: [0.5, 0.1, 0.05],\n",
       "                         &#x27;grad__max_depth&#x27;: [3, 5, 7],\n",
       "                         &#x27;grad__max_features&#x27;: [10, 15, 20, 25],\n",
       "                         &#x27;grad__min_samples_leaf&#x27;: [5, 8, 11],\n",
       "                         &#x27;grad__min_samples_split&#x27;: [3, 5, 7],\n",
       "                         &#x27;grad__n_estimators&#x27;: [50, 100, 200]},\n",
       "             return_train_score=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-42\" type=\"checkbox\" ><label for=\"sk-estimator-id-42\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;GridSearchCV<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.GridSearchCV.html\">?<span>Documentation for GridSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;mice_impute&#x27;, Custom_MICE_Imputer()),\n",
       "                                       (&#x27;add_zones&#x27;,\n",
       "                                        FunctionTransformer(func=&lt;function zone_encoder at 0x305cc5800&gt;)),\n",
       "                                       (&#x27;grad&#x27;, GradientBoostingRegressor())]),\n",
       "             param_grid={&#x27;grad__learning_rate&#x27;: [0.5, 0.1, 0.05],\n",
       "                         &#x27;grad__max_depth&#x27;: [3, 5, 7],\n",
       "                         &#x27;grad__max_features&#x27;: [10, 15, 20, 25],\n",
       "                         &#x27;grad__min_samples_leaf&#x27;: [5, 8, 11],\n",
       "                         &#x27;grad__min_samples_split&#x27;: [3, 5, 7],\n",
       "                         &#x27;grad__n_estimators&#x27;: [50, 100, 200]},\n",
       "             return_train_score=True)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-43\" type=\"checkbox\" ><label for=\"sk-estimator-id-43\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">best_estimator_: Pipeline</label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;mice_impute&#x27;, Custom_MICE_Imputer()),\n",
       "                (&#x27;add_zones&#x27;,\n",
       "                 FunctionTransformer(func=&lt;function zone_encoder at 0x305cc5800&gt;)),\n",
       "                (&#x27;grad&#x27;,\n",
       "                 GradientBoostingRegressor(max_features=10, min_samples_leaf=8,\n",
       "                                           min_samples_split=3,\n",
       "                                           n_estimators=50))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-44\" type=\"checkbox\" ><label for=\"sk-estimator-id-44\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">Custom_MICE_Imputer</label><div class=\"sk-toggleable__content fitted\"><pre>Custom_MICE_Imputer()</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-45\" type=\"checkbox\" ><label for=\"sk-estimator-id-45\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;FunctionTransformer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.preprocessing.FunctionTransformer.html\">?<span>Documentation for FunctionTransformer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>FunctionTransformer(func=&lt;function zone_encoder at 0x305cc5800&gt;)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-46\" type=\"checkbox\" ><label for=\"sk-estimator-id-46\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;GradientBoostingRegressor<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html\">?<span>Documentation for GradientBoostingRegressor</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>GradientBoostingRegressor(max_features=10, min_samples_leaf=8,\n",
       "                          min_samples_split=3, n_estimators=50)</pre></div> </div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('mice_impute', Custom_MICE_Imputer()),\n",
       "                                       ('add_zones',\n",
       "                                        FunctionTransformer(func=<function zone_encoder at 0x305cc5800>)),\n",
       "                                       ('grad', GradientBoostingRegressor())]),\n",
       "             param_grid={'grad__learning_rate': [0.5, 0.1, 0.05],\n",
       "                         'grad__max_depth': [3, 5, 7],\n",
       "                         'grad__max_features': [10, 15, 20, 25],\n",
       "                         'grad__min_samples_leaf': [5, 8, 11],\n",
       "                         'grad__min_samples_split': [3, 5, 7],\n",
       "                         'grad__n_estimators': [50, 100, 200]},\n",
       "             return_train_score=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'grad__learning_rate': [0.5, 0.1, 0.05],\n",
    "    'grad__max_depth': [3,5,7],\n",
    "    'grad__max_features': [10, 15, 20, 25],\n",
    "    'grad__min_samples_leaf': [5, 8, 11],\n",
    "    'grad__min_samples_split': [3, 5, 7],\n",
    "    'grad__n_estimators': [50, 100, 200]\n",
    "}\n",
    "\n",
    "# Instantiate a gradient boosting regressor pipeline\n",
    "grad_pipe = Pipeline([('mice_impute', Custom_MICE_Imputer()),\n",
    "                ('add_zones', FunctionTransformer(zone_encoder)),\n",
    "                ('grad', GradientBoostingRegressor())])\n",
    "\n",
    "grid_cv_grad = GridSearchCV(grad_pipe, \n",
    "                          param_grid = param_grid, \n",
    "                          cv = 5,\n",
    "                          return_train_score=True)\n",
    "\n",
    "# We'll start by tuning on PCIAT, and can tune separately on sii\n",
    "grid_cv_grad.fit(train_cleaned[predictors], train_cleaned['PCIAT-PCIAT_Total'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'grad__learning_rate': 0.1, 'grad__max_depth': 3, 'grad__max_features': 10, 'grad__min_samples_leaf': 8, 'grad__min_samples_split': 3, 'grad__n_estimators': 50}\n",
      "Best score: 0.292846048851039\n"
     ]
    }
   ],
   "source": [
    "# Report the results\n",
    "print('Best parameters:',grid_cv_grad.best_params_)\n",
    "print('Best score:',grid_cv_grad.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tuning Results**\n",
    "\n",
    "Best parameters (first run): \n",
    "* 'grad__learning_rate': 0.05\n",
    "* 'grad__max_depth': 3\n",
    "* 'grad__max_features': 15\n",
    "* 'grad__min_samples_leaf': 11\n",
    "* 'grad__min_samples_split': 6\n",
    "* 'grad__n_estimators': 100\n",
    "\n",
    "Best parameters (second run):\n",
    "* learning_rate: 0.1\n",
    "* max_depth: 3\n",
    "* max_features: 10\n",
    "* min_samples_leaf: 8\n",
    "* min_samples_spit: 3\n",
    "* n_estimators: 50\n",
    "\n",
    "The differences suggest that there is some variance based on the random selection of the CV split and/or that the maximum values of kappa among these random variations are quite similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time', 'param_grad__learning_rate', 'param_grad__max_depth', 'param_grad__max_features', 'param_grad__min_samples_leaf', 'param_grad__min_samples_split', 'param_grad__n_estimators', 'params', 'split0_test_score', 'split1_test_score', 'split2_test_score', 'split3_test_score', 'split4_test_score', 'mean_test_score', 'std_test_score', 'rank_test_score', 'split0_train_score', 'split1_train_score', 'split2_train_score', 'split3_train_score', 'split4_train_score', 'mean_train_score', 'std_train_score'])\n",
      "(972,)\n",
      "(972,)\n",
      "[0.65189586 0.80029376 0.92135735 0.65409586 0.7987357  0.92351374\n",
      " 0.64791556 0.79767938 0.92096862 0.64412865 0.78515763 0.9157732\n",
      " 0.64353646 0.78795314 0.914249   0.64554734 0.78419286 0.91561709\n",
      " 0.63226408 0.77356867 0.9070727  0.64147087 0.78337    0.91038977\n",
      " 0.63575623 0.77927076 0.9084253  0.66322111 0.8184424  0.93250769\n",
      " 0.66289117 0.80994802 0.93586184 0.66342119 0.81080982 0.9343366\n",
      " 0.65728543 0.80195557 0.92413289 0.66170712 0.80956102 0.92983166\n",
      " 0.66690465 0.80886754 0.92763755 0.65245624 0.79517396 0.92254721\n",
      " 0.65708512 0.79551046 0.92154243 0.64641113 0.79731922 0.9232357\n",
      " 0.67395427 0.8209492  0.94129207 0.67336322 0.81823353 0.94052184\n",
      " 0.67307346 0.82435958 0.94229279 0.67027595 0.8161572  0.93279165\n",
      " 0.66951364 0.80842316 0.9356467  0.66684437 0.81502087 0.93455539\n",
      " 0.65871962 0.81136805 0.92548785 0.66300114 0.80087669 0.93025916\n",
      " 0.66866546 0.80794351 0.92887239 0.68210305 0.83330467 0.94302522\n",
      " 0.69170109 0.83379137 0.94662273 0.67610895 0.82751615 0.946011\n",
      " 0.67426306 0.82449329 0.93983009 0.67979508 0.819426   0.93901929\n",
      " 0.67275613 0.82728062 0.94141588 0.67270287 0.81174117 0.93502558\n",
      " 0.67511422 0.81289105 0.93277821 0.66923346 0.81463129 0.93362977\n",
      " 0.91751972 0.98841317 0.99946262 0.91666775 0.98643493 0.9994655\n",
      " 0.91537775 0.98754719 0.99948084 0.90275999 0.98208128 0.99912071\n",
      " 0.91229273 0.98333791 0.99910289 0.90131056 0.98307634 0.99911301\n",
      " 0.89007044 0.97856423 0.99866204 0.89530804 0.97640983 0.9985671\n",
      " 0.89129443 0.97734963 0.99855026 0.92279771 0.99010257 0.99954754\n",
      " 0.92409105 0.9907938  0.99956096 0.92985305 0.99008895 0.99956657\n",
      " 0.91706437 0.98495038 0.99935393 0.91204967 0.98635712 0.99928184\n",
      " 0.91620783 0.98680001 0.99937509 0.89773649 0.98042453 0.99905379\n",
      " 0.89997687 0.98047789 0.99889472 0.9037276  0.9821602  0.99899429\n",
      " 0.9357123  0.99179629 0.99960838 0.93560098 0.99125544 0.99961875\n",
      " 0.93402468 0.99086088 0.99960304 0.91780146 0.98758699 0.99944282\n",
      " 0.91686563 0.98628468 0.99939939 0.91889319 0.98649071 0.99941258\n",
      " 0.90994207 0.98306271 0.9990908  0.90468921 0.98295134 0.99908396\n",
      " 0.90791496 0.9835853  0.99905876 0.9355014  0.99177515 0.99964257\n",
      " 0.93433668 0.99226094 0.99961381 0.93982019 0.99167147 0.99962623\n",
      " 0.92319384 0.98811497 0.99945602 0.92710627 0.98807768 0.99944747\n",
      " 0.92394716 0.98926133 0.99940344 0.91688043 0.98281587 0.99922348\n",
      " 0.90599984 0.98517155 0.99920554 0.91424386 0.98266491 0.99915476\n",
      " 0.99476105 0.99972722 0.9997829  0.99495547 0.99974018 0.9997829\n",
      " 0.99549358 0.99973501 0.9997829  0.99015282 0.99959799 0.99978277\n",
      " 0.98881403 0.99949812 0.99978278 0.98947958 0.99954805 0.99978278\n",
      " 0.98295544 0.99918161 0.99978192 0.98199159 0.99911534 0.99978205\n",
      " 0.98154192 0.99914661 0.99978205 0.99600448 0.99975362 0.9997829\n",
      " 0.99612487 0.99974303 0.9997829  0.99537081 0.99974962 0.9997829\n",
      " 0.99104965 0.99962928 0.99978285 0.99020582 0.99962534 0.99978283\n",
      " 0.99078898 0.99959881 0.99978284 0.98540103 0.99935877 0.99978212\n",
      " 0.98494374 0.99928847 0.99978232 0.98511233 0.9993713  0.99978238\n",
      " 0.99615597 0.9997474  0.9997829  0.99639123 0.99975988 0.9997829\n",
      " 0.99607088 0.99975028 0.9997829  0.99372537 0.99964937 0.99978285\n",
      " 0.99237177 0.99964519 0.99978285 0.99262526 0.99965909 0.99978285\n",
      " 0.98735872 0.99942024 0.99978233 0.98708263 0.99939418 0.99978245\n",
      " 0.98715078 0.99932578 0.99978248 0.99611203 0.99974451 0.9997829\n",
      " 0.99652984 0.99975656 0.9997829  0.99628838 0.99975641 0.9997829\n",
      " 0.99230035 0.99962858 0.99978285 0.99219493 0.99961086 0.99978286\n",
      " 0.99150688 0.99964999 0.99978284 0.98744078 0.99944219 0.99978243\n",
      " 0.98865237 0.99941179 0.99978254 0.98739893 0.99941977 0.99978244\n",
      " 0.43682959 0.5136357  0.62553357 0.43920817 0.51822348 0.6245182\n",
      " 0.44029062 0.51706883 0.62715725 0.43540853 0.51362844 0.61757978\n",
      " 0.43663504 0.5079673  0.62025441 0.43183163 0.51160866 0.61821451\n",
      " 0.43260209 0.50933522 0.61603188 0.43247889 0.50879558 0.61663474\n",
      " 0.43165482 0.50890509 0.61369591 0.44223821 0.52384178 0.63703217\n",
      " 0.44292435 0.52235867 0.63694584 0.44081048 0.52487705 0.63759399\n",
      " 0.44131661 0.52012899 0.63206377 0.44005295 0.51946394 0.6299831\n",
      " 0.44092345 0.51837583 0.63051045 0.43811283 0.51607572 0.62762384\n",
      " 0.43723527 0.52098315 0.62479603 0.43649648 0.51824175 0.62690304\n",
      " 0.44784486 0.52807751 0.64900492 0.44719588 0.52795516 0.64246258\n",
      " 0.44757721 0.52613161 0.64975599 0.44555478 0.52332981 0.63969805\n",
      " 0.44566482 0.52761295 0.6390784  0.44437371 0.52229302 0.63326306\n",
      " 0.44221483 0.5226617  0.63374036 0.44012084 0.52120878 0.63293079\n",
      " 0.4422455  0.52335738 0.63494361 0.44822609 0.53157196 0.65090789\n",
      " 0.45085352 0.53121967 0.65053154 0.44939441 0.53188962 0.65093344\n",
      " 0.44906622 0.5283654  0.63916335 0.44746232 0.52907358 0.6438742\n",
      " 0.44850524 0.53025858 0.64176298 0.44340216 0.52379204 0.63826635\n",
      " 0.44626461 0.5254695  0.63777162 0.44373927 0.52679198 0.64016163\n",
      " 0.64319774 0.76818896 0.89234069 0.649009   0.75917029 0.89567114\n",
      " 0.64784095 0.76572622 0.89724652 0.63511062 0.74412107 0.87701456\n",
      " 0.62944096 0.74628004 0.87666666 0.6298198  0.74627837 0.87956166\n",
      " 0.61751684 0.72599214 0.86445441 0.61503324 0.73108014 0.86320669\n",
      " 0.61333821 0.73081763 0.86095362 0.65729273 0.77264774 0.90267469\n",
      " 0.65331283 0.77656762 0.90442902 0.65614827 0.77906615 0.90586482\n",
      " 0.64463525 0.75807694 0.88888338 0.64838523 0.76086604 0.88786657\n",
      " 0.64133217 0.76300232 0.88599578 0.62951886 0.74093501 0.87188562\n",
      " 0.62970335 0.74400436 0.87119059 0.6273998  0.74314278 0.87107661\n",
      " 0.66275573 0.78236953 0.90672308 0.66342646 0.7797085  0.90966989\n",
      " 0.66450532 0.78007414 0.90803706 0.64656096 0.76239749 0.89373482\n",
      " 0.64699103 0.76487472 0.89108029 0.64873546 0.76802536 0.89425403\n",
      " 0.64154216 0.74675622 0.87742289 0.63767367 0.74934804 0.88510255\n",
      " 0.63663135 0.74776956 0.88143246 0.66737251 0.78482746 0.91193219\n",
      " 0.67054399 0.78229244 0.91061372 0.67040115 0.78138679 0.90840411\n",
      " 0.65557117 0.7641979  0.89600448 0.65662688 0.7651104  0.89431651\n",
      " 0.65259445 0.76844801 0.89683788 0.63771172 0.75424883 0.88020019\n",
      " 0.637727   0.75792506 0.88089331 0.63959158 0.753618   0.88171128\n",
      " 0.84128978 0.93370834 0.99017442 0.84227423 0.93362657 0.98955956\n",
      " 0.8401258  0.93552757 0.98996484 0.80664171 0.91011194 0.98040657\n",
      " 0.80414716 0.90609927 0.98111933 0.80474679 0.90696371 0.97942346\n",
      " 0.78328474 0.88118102 0.96942264 0.7720528  0.88422917 0.96856284\n",
      " 0.78098525 0.88226324 0.97013272 0.85135859 0.93979405 0.99062672\n",
      " 0.8504134  0.93997135 0.99149857 0.85296255 0.94006277 0.99023327\n",
      " 0.81512305 0.91656521 0.98065085 0.82400808 0.91496438 0.98293241\n",
      " 0.81462685 0.91082619 0.98236659 0.79024643 0.89237001 0.97288727\n",
      " 0.79154708 0.89342116 0.97210164 0.79374507 0.8904585  0.97162161\n",
      " 0.85583142 0.93884484 0.99049795 0.85749648 0.94116535 0.99132358\n",
      " 0.86128968 0.94362983 0.9917296  0.82392285 0.9188499  0.98225432\n",
      " 0.83042873 0.91511917 0.98225223 0.82469195 0.92089186 0.98258322\n",
      " 0.79533415 0.89894615 0.97421619 0.79923701 0.89300766 0.97359081\n",
      " 0.7935201  0.89110833 0.97262418 0.86131501 0.94450431 0.99045321\n",
      " 0.85991512 0.94442385 0.99052906 0.86359015 0.94437392 0.99020085\n",
      " 0.82570303 0.9234215  0.98388475 0.83396485 0.91934099 0.98346847\n",
      " 0.8283308  0.91937894 0.98406591 0.80047566 0.89807971 0.97459641\n",
      " 0.80015766 0.89796131 0.97313676 0.80626242 0.8975238  0.97140455\n",
      " 0.36768514 0.43952503 0.51633853 0.36638413 0.43898848 0.51656327\n",
      " 0.3661885  0.43907655 0.51482842 0.3646454  0.43719925 0.51079294\n",
      " 0.3668104  0.4359594  0.51018509 0.365976   0.43724126 0.51214759\n",
      " 0.36574109 0.43226868 0.50853186 0.36639618 0.43425903 0.50775625\n",
      " 0.36398291 0.43296402 0.50969381 0.37281512 0.44402068 0.524907\n",
      " 0.37293145 0.44416156 0.5250409  0.37373622 0.44248364 0.52368178\n",
      " 0.37076692 0.44239374 0.51863619 0.37134021 0.44243627 0.51913149\n",
      " 0.37133915 0.44270135 0.52095949 0.36993979 0.43848342 0.51974993\n",
      " 0.37018337 0.43789456 0.51756007 0.37154611 0.43794056 0.51691056\n",
      " 0.37479865 0.44619696 0.52619022 0.37479211 0.44656099 0.52775733\n",
      " 0.37532679 0.44670959 0.52566209 0.37523527 0.44334825 0.52395336\n",
      " 0.37541745 0.44532239 0.52408284 0.37468213 0.44434622 0.52381602\n",
      " 0.37272935 0.44131547 0.52390385 0.37333889 0.44203527 0.52221851\n",
      " 0.37241516 0.44083802 0.5222766  0.37664359 0.4484597  0.53000371\n",
      " 0.3761538  0.45037874 0.53278521 0.37702238 0.4492728  0.52998936\n",
      " 0.37566575 0.44807377 0.5264418  0.37610478 0.44780653 0.52698651\n",
      " 0.37630337 0.44786431 0.52622743 0.37495788 0.44456191 0.52425456\n",
      " 0.37468053 0.4440553  0.52376112 0.37488912 0.44478274 0.5264603\n",
      " 0.53566489 0.65055857 0.76215032 0.53764488 0.64752584 0.76294156\n",
      " 0.53734567 0.65247892 0.76184819 0.5266742  0.63352476 0.74418126\n",
      " 0.52677426 0.63266995 0.74494177 0.52772735 0.63242824 0.74341406\n",
      " 0.51556549 0.62105894 0.73008329 0.51697457 0.62189008 0.73042005\n",
      " 0.51659361 0.62184972 0.72878982 0.54671361 0.6601144  0.77436815\n",
      " 0.54814935 0.65716184 0.77264095 0.54583615 0.65968156 0.7699634\n",
      " 0.5353181  0.64777189 0.75771923 0.53741579 0.64387521 0.75538489\n",
      " 0.53590882 0.64466162 0.75367741 0.52710086 0.62782268 0.74100867\n",
      " 0.52722669 0.62894366 0.74194112 0.52672531 0.62960146 0.74176605\n",
      " 0.55507499 0.66466283 0.77971857 0.55280244 0.66473038 0.7761752\n",
      " 0.55387446 0.66445556 0.77917344 0.54346485 0.65370914 0.76298467\n",
      " 0.54403118 0.64948031 0.76237704 0.54179433 0.64820463 0.76466469\n",
      " 0.53171931 0.63610135 0.74544592 0.53135149 0.63806115 0.74894166\n",
      " 0.5337991  0.63301834 0.74685495 0.55812657 0.67048915 0.78054429\n",
      " 0.55783273 0.66487248 0.78026493 0.55810698 0.66708043 0.78177251\n",
      " 0.54568707 0.65399549 0.76800809 0.54522742 0.65260725 0.76319428\n",
      " 0.5457115  0.65328886 0.76703122 0.53485984 0.64092955 0.75265433\n",
      " 0.53427148 0.63862074 0.75228041 0.53588518 0.63908106 0.75313142\n",
      " 0.73011934 0.84634463 0.93537649 0.72902129 0.84728073 0.93452531\n",
      " 0.72804958 0.84809065 0.93331477 0.69457414 0.81067026 0.90607483\n",
      " 0.69814851 0.81000924 0.91016224 0.69711124 0.80744463 0.90697942\n",
      " 0.6675442  0.78400101 0.88129721 0.66948275 0.78059922 0.88341059\n",
      " 0.66990733 0.77974222 0.88217016 0.74336981 0.85228698 0.93885928\n",
      " 0.7440995  0.85392664 0.93855652 0.74168485 0.84987138 0.9367766\n",
      " 0.70841742 0.82183978 0.91428038 0.71128404 0.82028481 0.91529306\n",
      " 0.70923419 0.82487995 0.91522624 0.68316704 0.79475814 0.88809713\n",
      " 0.68185415 0.79052785 0.88686254 0.67927544 0.79231348 0.89149339\n",
      " 0.74774912 0.85435806 0.94020091 0.75063006 0.86082025 0.93968135\n",
      " 0.74718131 0.86058643 0.93962606 0.7129145  0.82655422 0.91566621\n",
      " 0.71684553 0.82541389 0.91645264 0.71522926 0.82554868 0.91621766\n",
      " 0.68602422 0.79822483 0.89511608 0.68824464 0.79669529 0.89560907\n",
      " 0.68752693 0.79420909 0.89317256 0.75013918 0.86195089 0.93980164\n",
      " 0.75169011 0.86185621 0.94244348 0.74930201 0.86356661 0.94340717\n",
      " 0.72056989 0.82725353 0.9158662  0.71757478 0.8271955  0.91659254\n",
      " 0.71600511 0.82937335 0.92040562 0.69430044 0.80518017 0.89536203\n",
      " 0.69033399 0.80414824 0.89713109 0.69251321 0.80415361 0.89571548]\n",
      "[ 1.77738922e-01  1.41821611e-01  2.19705293e-02  1.93512305e-01\n",
      "  9.37041825e-02  3.26215154e-02  1.74448412e-01  1.29094001e-01\n",
      "  4.26280089e-02  1.63547502e-01  8.00610224e-02  5.37326566e-02\n",
      "  1.81135375e-01  1.01399587e-01  5.81596377e-02  1.85814168e-01\n",
      "  1.09788410e-01  3.88582100e-02  1.89859202e-01  1.31600775e-01\n",
      "  1.12771838e-01  1.89320416e-01  1.28006274e-01  3.27665949e-02\n",
      "  1.58137432e-01  1.13433597e-01  7.05833153e-02  1.72388722e-01\n",
      "  7.43069028e-02  2.87015098e-02  1.84036656e-01  1.01737807e-01\n",
      "  1.93505113e-02  1.99474394e-01  1.19141340e-01  4.33871694e-02\n",
      "  1.68863820e-01  9.77763764e-02  7.29209374e-02  1.59977960e-01\n",
      "  1.23744039e-01  5.21386117e-02  1.84911659e-01  9.58371479e-02\n",
      "  3.66036490e-02  1.92671244e-01  1.19997104e-01  4.91098091e-02\n",
      "  1.96722512e-01  1.34869918e-01  6.60440692e-02  1.77078940e-01\n",
      "  1.00969492e-01  4.50276631e-02  1.79144704e-01  1.08172616e-01\n",
      "  6.20263360e-02  1.48351780e-01  9.15504188e-02  4.78222048e-02\n",
      "  1.64440808e-01  8.41246926e-02  3.94354396e-02  1.58711135e-01\n",
      "  1.06447669e-01  4.23521423e-02  1.77779475e-01  9.70612447e-02\n",
      "  4.26731255e-02  1.61793231e-01  1.14678502e-01  1.54347669e-02\n",
      "  1.84545418e-01  1.20403746e-01  3.43904054e-02  1.72715243e-01\n",
      "  1.00546464e-01  7.35404750e-02  1.75719136e-01  1.09980789e-01\n",
      "  5.97767925e-02  1.65912196e-01  8.05383525e-02  5.08488615e-02\n",
      "  1.65767408e-01  1.14481164e-01  3.13206038e-02  1.67410457e-01\n",
      "  9.37103467e-02  3.21615203e-02  1.65317461e-01  1.05870142e-01\n",
      "  5.48085743e-02  1.78526238e-01  1.13037303e-01  2.03130436e-02\n",
      "  1.54614995e-01  9.79687061e-02  2.65472064e-02  1.83805128e-01\n",
      "  1.23214323e-01  4.89352190e-02  1.72771903e-01  1.09360839e-01\n",
      "  5.24981732e-02  1.74263228e-01  1.29036973e-01  1.29859411e-02\n",
      "  6.63488188e-02  5.49041195e-02  3.00536114e-02  6.05143667e-02\n",
      "  2.91337726e-03  2.24605597e-02  4.71823491e-02  4.10128386e-02\n",
      " -6.28824510e-03  5.94817904e-02  3.66049073e-02  1.03467088e-02\n",
      "  9.80318358e-02  2.67755261e-02  1.00762626e-02  6.00052122e-02\n",
      "  2.53063114e-02  1.59128533e-02  1.06297912e-01  2.82186939e-02\n",
      "  3.79991508e-02  8.97413043e-02  5.09012747e-02  1.08880252e-02\n",
      "  8.72204866e-02  5.32995193e-02  2.40579656e-02  4.99784785e-02\n",
      " -1.58957908e-02  2.88768891e-02  8.60887031e-02  2.85869889e-02\n",
      "  1.59644781e-02  5.32055931e-02  3.11700915e-02  5.01192487e-02\n",
      "  6.89711201e-02  2.87446122e-02  1.03689164e-02  6.16325066e-02\n",
      "  2.79575083e-02  4.48790076e-02  6.08065505e-02  3.78321586e-02\n",
      "  5.10594424e-02  7.33547435e-02  4.62660000e-02  1.43528621e-02\n",
      "  9.11848172e-02  7.06095788e-02  1.84294345e-02  7.76937764e-02\n",
      "  5.37263924e-03  3.05087415e-02  5.95586729e-02  9.85532772e-03\n",
      "  2.81345642e-02  8.83000042e-02  2.64436492e-02  4.34976469e-02\n",
      "  8.14280804e-02  4.41813398e-02  4.97437042e-02  9.53645193e-02\n",
      "  4.97935316e-02  2.86115615e-02  5.96283531e-02  2.42731597e-02\n",
      " -2.37466410e-04  7.86943832e-02  2.33721077e-02  3.23896954e-02\n",
      "  8.35177449e-02  1.58800223e-02  2.24591401e-02  8.50363988e-02\n",
      "  3.66314614e-02  9.61374625e-03  7.19940019e-02  5.75755523e-03\n",
      "  2.11446706e-02  5.52237988e-02  5.86761569e-02  4.91356856e-02\n",
      "  6.21539343e-02  3.49108094e-02  3.52609554e-02  6.48319511e-02\n",
      "  3.31278665e-02  1.86462377e-02  8.26080533e-02  5.64459421e-02\n",
      "  1.45507960e-02  5.18733465e-02  5.55744226e-02  2.43840578e-02\n",
      "  8.63436012e-02  3.25910575e-02  5.55573228e-02  1.02261385e-01\n",
      "  4.37774234e-02  1.64016400e-02  9.88328638e-02  5.01028829e-02\n",
      " -2.90265224e-03  7.90869667e-02  6.55809371e-02  2.44732400e-02\n",
      "  2.76719236e-03  3.11564676e-02  3.80609999e-02  2.83116462e-02\n",
      " -4.24661158e-03  1.35990119e-02  4.38176702e-02  3.21256300e-02\n",
      "  5.15224525e-02  5.49898993e-02  4.50072468e-02  2.86553885e-02\n",
      "  6.97275844e-02  3.46608623e-02  5.02095650e-03  5.57927363e-02\n",
      "  2.13037913e-02  2.88082218e-02  6.29330025e-02  4.41482623e-02\n",
      "  1.89929897e-02  3.49148408e-02  1.03720995e-02  1.69260177e-02\n",
      "  2.80545523e-02  5.70976439e-02  5.15393384e-03 -8.37830632e-04\n",
      "  5.13001749e-02  3.46890950e-02  4.18351337e-02  2.90463093e-02\n",
      "  6.03861411e-02  4.09402500e-02  2.77334642e-02  4.06564604e-02\n",
      "  3.59053770e-02  3.98810297e-02  3.81936865e-02  3.81072646e-02\n",
      "  4.52131933e-02  1.78306730e-02  4.96256356e-02  1.90957520e-02\n",
      "  6.01260053e-02  4.95208031e-02  3.37957180e-02  2.18667421e-02\n",
      "  3.58751867e-02  4.11663664e-02  1.70209230e-02  4.45920797e-02\n",
      "  2.55992535e-02  2.38487587e-02  5.72204418e-02  5.62790475e-02\n",
      "  3.66390671e-03  3.54112068e-02  2.86699091e-02  1.58325309e-02\n",
      "  3.72512265e-02  1.64708454e-02  7.40703343e-02  7.12989762e-02\n",
      "  1.90419277e-02  4.84466302e-02  4.59678120e-02 -1.26019022e-02\n",
      "  3.04902415e-02  3.88521512e-02  6.61260085e-02  2.64647104e-02\n",
      "  2.61779871e-02  5.67505571e-02  3.72845493e-02  3.21892383e-02\n",
      "  4.43142550e-02  3.08236467e-02  4.68324508e-02 -2.10256552e-02\n",
      "  4.58976793e-02  4.03872103e-02  6.29789144e-03  5.29252899e-02\n",
      "  3.53980951e-02  2.92110175e-02  2.04689277e-02  1.76144230e-02\n",
      "  1.92918112e-02  5.18949416e-02  4.14942768e-02  1.38022461e-02\n",
      "  1.53706213e-02  4.37736665e-02  1.80256864e-03  5.24129664e-02\n",
      "  4.49488634e-02  2.86825981e-02  8.13819696e-02  1.59268641e-02\n",
      "  2.32820554e-02  2.81561537e-02  1.83691656e-02  2.27792263e-02\n",
      "  5.63910851e-02  2.81555817e-02  4.37869378e-02 -2.51913447e-03\n",
      "  2.90697308e-01  2.82419445e-01  2.57213285e-01  2.90143940e-01\n",
      "  2.73724954e-01  2.54913142e-01  2.90629432e-01  2.74952903e-01\n",
      "  2.60596685e-01  2.92846049e-01  2.74945768e-01  2.55629956e-01\n",
      "  2.87942298e-01  2.74446126e-01  2.57623934e-01  2.86822140e-01\n",
      "  2.74386494e-01  2.55004954e-01  2.87527078e-01  2.80972414e-01\n",
      "  2.61172995e-01  2.89548492e-01  2.73839817e-01  2.58217891e-01\n",
      "  2.85378778e-01  2.77483992e-01  2.66767894e-01  2.87287409e-01\n",
      "  2.73501415e-01  2.56792674e-01  2.86205413e-01  2.74253827e-01\n",
      "  2.50396579e-01  2.83809123e-01  2.74066708e-01  2.61047171e-01\n",
      "  2.86813277e-01  2.69297006e-01  2.48847475e-01  2.92450334e-01\n",
      "  2.73683810e-01  2.53839465e-01  2.85987319e-01  2.77307839e-01\n",
      "  2.52077549e-01  2.85702390e-01  2.79473243e-01  2.57048550e-01\n",
      "  2.82789382e-01  2.79075437e-01  2.57082650e-01  2.88428695e-01\n",
      "  2.77318176e-01  2.57391379e-01  2.89192249e-01  2.78422642e-01\n",
      "  2.57356879e-01  2.86689796e-01  2.74193405e-01  2.49543354e-01\n",
      "  2.82930791e-01  2.79542005e-01  2.52813484e-01  2.88295068e-01\n",
      "  2.74975778e-01  2.44798497e-01  2.84768281e-01  2.70293006e-01\n",
      "  2.42881331e-01  2.81846390e-01  2.72222139e-01  2.50345490e-01\n",
      "  2.84763479e-01  2.75178555e-01  2.55527542e-01  2.84436169e-01\n",
      "  2.78333520e-01  2.54541970e-01  2.85453055e-01  2.79009353e-01\n",
      "  2.57103915e-01  2.86270270e-01  2.78115935e-01  2.54362439e-01\n",
      "  2.85943995e-01  2.73499821e-01  2.48950245e-01  2.85718830e-01\n",
      "  2.71893787e-01  2.57273617e-01  2.82848665e-01  2.73987888e-01\n",
      "  2.51720422e-01  2.86091089e-01  2.65739682e-01  2.47718479e-01\n",
      "  2.87243366e-01  2.69793248e-01  2.45022372e-01  2.87782871e-01\n",
      "  2.80606075e-01  2.52702122e-01  2.86103017e-01  2.78974331e-01\n",
      "  2.52086669e-01  2.89448415e-01  2.75660269e-01  2.56702677e-01\n",
      "  2.58226171e-01  2.45275623e-01  2.17701731e-01  2.71233535e-01\n",
      "  2.40727354e-01  2.23346593e-01  2.71863526e-01  2.47108082e-01\n",
      "  2.33076532e-01  2.65403990e-01  2.35012044e-01  2.18735729e-01\n",
      "  2.53735408e-01  2.57654942e-01  2.16981361e-01  2.72131905e-01\n",
      "  2.44142729e-01  2.23580523e-01  2.67802364e-01  2.44498054e-01\n",
      "  2.24067416e-01  2.74560904e-01  2.42034860e-01  2.19212658e-01\n",
      "  2.72968324e-01  2.50937647e-01  2.12140262e-01  2.66699576e-01\n",
      "  2.53565074e-01  2.24447276e-01  2.66764358e-01  2.53053543e-01\n",
      "  2.10328602e-01  2.67244131e-01  2.53639733e-01  2.21017234e-01\n",
      "  2.68924934e-01  2.41682107e-01  2.24309692e-01  2.62746567e-01\n",
      "  2.39329016e-01  2.15603983e-01  2.61393022e-01  2.46528450e-01\n",
      "  2.09330238e-01  2.73136131e-01  2.62261932e-01  2.17593609e-01\n",
      "  2.68376115e-01  2.51192683e-01  2.23606275e-01  2.64106198e-01\n",
      "  2.55518839e-01  2.23973066e-01  2.64683726e-01  2.48768118e-01\n",
      "  2.17999484e-01  2.60497880e-01  2.51977416e-01  2.27990289e-01\n",
      "  2.68461245e-01  2.56381715e-01  2.30030548e-01  2.61909117e-01\n",
      "  2.46743027e-01  2.09085483e-01  2.65414626e-01  2.49317087e-01\n",
      "  2.09538480e-01  2.62057801e-01  2.42168755e-01  2.26878341e-01\n",
      "  2.72229221e-01  2.48176343e-01  2.17236388e-01  2.70411423e-01\n",
      "  2.51735218e-01  2.15110864e-01  2.64239382e-01  2.47885462e-01\n",
      "  2.12989877e-01  2.69893409e-01  2.45674804e-01  2.22478785e-01\n",
      "  2.65396881e-01  2.49776907e-01  2.27819435e-01  2.64615369e-01\n",
      "  2.43451633e-01  2.22291358e-01  2.54747177e-01  2.44241303e-01\n",
      "  2.12955900e-01  2.62338076e-01  2.31733312e-01  2.02665440e-01\n",
      "  2.67886346e-01  2.42131624e-01  2.14853442e-01  2.68014134e-01\n",
      "  2.50237754e-01  2.30510474e-01  2.67444812e-01  2.50767108e-01\n",
      "  2.22241498e-01  2.74940255e-01  2.44228760e-01  2.18659484e-01\n",
      "  2.47488174e-01  2.22475781e-01  2.16017003e-01  2.40574570e-01\n",
      "  2.23851351e-01  2.10816661e-01  2.44805634e-01  2.12721308e-01\n",
      "  2.11153157e-01  2.53409993e-01  2.35442242e-01  1.96318703e-01\n",
      "  2.40998845e-01  2.22302648e-01  2.17326463e-01  2.33964061e-01\n",
      "  2.34136673e-01  1.97346961e-01  2.47355578e-01  2.23751210e-01\n",
      "  1.98672191e-01  2.49185745e-01  2.20542367e-01  2.02910551e-01\n",
      "  2.56092942e-01  2.35609280e-01  2.10252423e-01  2.37778168e-01\n",
      "  2.33162785e-01  2.12480670e-01  2.49394597e-01  2.24185494e-01\n",
      "  2.19169500e-01  2.36783462e-01  2.31687604e-01  2.06584509e-01\n",
      "  2.41093498e-01  2.17005313e-01  2.07544168e-01  2.44972267e-01\n",
      "  2.08737726e-01  2.00088436e-01  2.50465035e-01  2.27001417e-01\n",
      "  2.08069011e-01  2.58384090e-01  2.29411250e-01  2.09015343e-01\n",
      "  2.45924604e-01  2.24428929e-01  2.00977236e-01  2.41183583e-01\n",
      "  2.32568265e-01  2.06245860e-01  2.42875736e-01  2.21789779e-01\n",
      "  2.11137586e-01  2.40249690e-01  2.25700844e-01  2.12878193e-01\n",
      "  2.46398647e-01  2.25897591e-01  2.05611976e-01  2.39418435e-01\n",
      "  2.19283200e-01  2.09430373e-01  2.50332493e-01  2.21957135e-01\n",
      "  2.09624160e-01  2.52208948e-01  2.33116256e-01  2.05715472e-01\n",
      "  2.43168796e-01  2.30954402e-01  1.96964580e-01  2.53068693e-01\n",
      "  2.25277493e-01  2.01680567e-01  2.44353008e-01  2.30346686e-01\n",
      "  2.02686357e-01  2.47710802e-01  2.25547671e-01  2.11686944e-01\n",
      "  2.42574850e-01  2.35979510e-01  2.10550734e-01  2.40539333e-01\n",
      "  2.21650997e-01  2.11010786e-01  2.43052304e-01  2.25566359e-01\n",
      "  2.08588117e-01  2.40743924e-01  2.33158493e-01  2.07707043e-01\n",
      "  2.38805146e-01  2.29213298e-01  1.94779104e-01  2.47957845e-01\n",
      "  2.23899030e-01  2.02458634e-01  2.46648449e-01  2.29025081e-01\n",
      "  2.00697996e-01  2.54633266e-01  2.29932897e-01  2.09799361e-01\n",
      "  2.81633184e-01  2.89141265e-01  2.82547694e-01  2.83575362e-01\n",
      "  2.91121545e-01  2.83744156e-01  2.84628267e-01  2.92381285e-01\n",
      "  2.82021441e-01  2.84302671e-01  2.91036053e-01  2.80104155e-01\n",
      "  2.79858190e-01  2.92053972e-01  2.78896754e-01  2.86122853e-01\n",
      "  2.89231096e-01  2.80126532e-01  2.85681330e-01  2.89962947e-01\n",
      "  2.85367696e-01  2.82476304e-01  2.91630095e-01  2.87095262e-01\n",
      "  2.81949935e-01  2.89179949e-01  2.79726597e-01  2.85542687e-01\n",
      "  2.90221928e-01  2.74748179e-01  2.83487585e-01  2.86958168e-01\n",
      "  2.74332748e-01  2.83703065e-01  2.91427286e-01  2.80021866e-01\n",
      "  2.85550255e-01  2.91227273e-01  2.76181082e-01  2.84724474e-01\n",
      "  2.91364756e-01  2.78534928e-01  2.88607236e-01  2.91169271e-01\n",
      "  2.76474467e-01  2.85450615e-01  2.89892274e-01  2.81166659e-01\n",
      "  2.86664350e-01  2.89645868e-01  2.82492705e-01  2.85928551e-01\n",
      "  2.92624250e-01  2.78819344e-01  2.85887888e-01  2.86009677e-01\n",
      "  2.77263710e-01  2.84353634e-01  2.89646827e-01  2.78240257e-01\n",
      "  2.87939431e-01  2.88798560e-01  2.74982524e-01  2.86363114e-01\n",
      "  2.88683777e-01  2.74115716e-01  2.85904214e-01  2.86080572e-01\n",
      "  2.73286059e-01  2.86619988e-01  2.88088848e-01  2.71646989e-01\n",
      "  2.87844547e-01  2.90375223e-01  2.77313306e-01  2.86471458e-01\n",
      "  2.92528809e-01  2.79564995e-01  2.87076948e-01  2.90686629e-01\n",
      "  2.78737698e-01  2.86270823e-01  2.87126716e-01  2.72406997e-01\n",
      "  2.86584417e-01  2.88356388e-01  2.76145877e-01  2.86101150e-01\n",
      "  2.87971435e-01  2.74866541e-01  2.86804866e-01  2.88419970e-01\n",
      "  2.71679950e-01  2.83887113e-01  2.86376876e-01  2.70567044e-01\n",
      "  2.87049612e-01  2.85249539e-01  2.71960752e-01  2.86766315e-01\n",
      "  2.88872629e-01  2.77777797e-01  2.88648879e-01  2.90076553e-01\n",
      "  2.80060101e-01  2.88624225e-01  2.91282929e-01  2.76514602e-01\n",
      "  2.79515245e-01  2.73991021e-01  2.60869174e-01  2.79918644e-01\n",
      "  2.80876097e-01  2.63004704e-01  2.73925156e-01  2.74786166e-01\n",
      "  2.58906850e-01  2.82596272e-01  2.71515065e-01  2.59839567e-01\n",
      "  2.84691245e-01  2.77411819e-01  2.56539123e-01  2.77298138e-01\n",
      "  2.76462638e-01  2.53553557e-01  2.82297547e-01  2.70918687e-01\n",
      "  2.55984850e-01  2.78777643e-01  2.80687413e-01  2.61654850e-01\n",
      "  2.78001026e-01  2.79401264e-01  2.62044618e-01  2.78996000e-01\n",
      "  2.74965140e-01  2.59692944e-01  2.81216807e-01  2.77671588e-01\n",
      "  2.54374980e-01  2.84360512e-01  2.73969857e-01  2.57006065e-01\n",
      "  2.76810682e-01  2.70842101e-01  2.57401711e-01  2.81661887e-01\n",
      "  2.69609960e-01  2.54538660e-01  2.79614178e-01  2.73204450e-01\n",
      "  2.52908081e-01  2.75215078e-01  2.76411016e-01  2.59159241e-01\n",
      "  2.79361657e-01  2.76250943e-01  2.59820044e-01  2.78354860e-01\n",
      "  2.79660412e-01  2.60244707e-01  2.77786415e-01  2.75432402e-01\n",
      "  2.51741673e-01  2.83003466e-01  2.72546780e-01  2.53562313e-01\n",
      "  2.78873506e-01  2.68430541e-01  2.53949909e-01  2.79269571e-01\n",
      "  2.69312284e-01  2.54816461e-01  2.79605827e-01  2.71059174e-01\n",
      "  2.52601044e-01  2.80029644e-01  2.78233443e-01  2.56870924e-01\n",
      "  2.83508898e-01  2.78760172e-01  2.64059720e-01  2.82044596e-01\n",
      "  2.78557619e-01  2.56488978e-01  2.83195439e-01  2.73870233e-01\n",
      "  2.58711191e-01  2.78900763e-01  2.76171087e-01  2.61968225e-01\n",
      "  2.77844557e-01  2.74187354e-01  2.52392277e-01  2.78080769e-01\n",
      "  2.69549100e-01  2.54264290e-01  2.83275847e-01  2.67322935e-01\n",
      "  2.48135351e-01  2.80262055e-01  2.70174694e-01  2.45117711e-01\n",
      "  2.77405300e-01  2.67893129e-01  2.46552193e-01  2.80185545e-01\n",
      "  2.72593989e-01  2.55229452e-01  2.79578411e-01  2.73364041e-01\n",
      "  2.56943587e-01  2.78070479e-01  2.74357014e-01  2.58903297e-01\n",
      "  2.70469498e-01  2.59581472e-01  2.41083782e-01  2.68749952e-01\n",
      "  2.56786218e-01  2.42847306e-01  2.62178519e-01  2.52947715e-01\n",
      "  2.42471755e-01  2.69454608e-01  2.55823630e-01  2.42628462e-01\n",
      "  2.70780198e-01  2.61500948e-01  2.41605457e-01  2.56941167e-01\n",
      "  2.65800658e-01  2.38893245e-01  2.75330948e-01  2.60639549e-01\n",
      "  2.33152263e-01  2.67566278e-01  2.63645690e-01  2.45741914e-01\n",
      "  2.65974406e-01  2.67740206e-01  2.47506902e-01  2.64496569e-01\n",
      "  2.57190987e-01  2.42664961e-01  2.68490416e-01  2.58222412e-01\n",
      "  2.44829025e-01  2.67222723e-01  2.58419550e-01  2.43432222e-01\n",
      "  2.63171619e-01  2.54455116e-01  2.42232846e-01  2.73718303e-01\n",
      "  2.53617838e-01  2.38560134e-01  2.66871142e-01  2.62747063e-01\n",
      "  2.37273902e-01  2.70216232e-01  2.61740637e-01  2.42874180e-01\n",
      "  2.75576445e-01  2.48589803e-01  2.38481286e-01  2.70659167e-01\n",
      "  2.62889482e-01  2.49189940e-01  2.67654880e-01  2.59584594e-01\n",
      "  2.36213423e-01  2.66495407e-01  2.58411331e-01  2.40227688e-01\n",
      "  2.66971067e-01  2.56916097e-01  2.32861766e-01  2.67311384e-01\n",
      "  2.52713189e-01  2.35507645e-01  2.64847138e-01  2.48288739e-01\n",
      "  2.32604190e-01  2.65327184e-01  2.54133088e-01  2.43449952e-01\n",
      "  2.71268228e-01  2.53741506e-01  2.40687197e-01  2.76292764e-01\n",
      "  2.57717182e-01  2.43295346e-01  2.67573245e-01  2.60320068e-01\n",
      "  2.40206173e-01  2.58224793e-01  2.45908543e-01  2.42094983e-01\n",
      "  2.61499913e-01  2.45460280e-01  2.33604401e-01  2.54155394e-01\n",
      "  2.49561296e-01  2.35046286e-01  2.61569083e-01  2.51645780e-01\n",
      "  2.31998544e-01  2.61639213e-01  2.55028513e-01  2.29306743e-01\n",
      "  2.60172020e-01  2.57850728e-01  2.37544978e-01  2.70288574e-01\n",
      "  2.46763217e-01  2.42789573e-01  2.63936572e-01  2.50686309e-01\n",
      "  2.42960835e-01  2.66696841e-01  2.55962515e-01  2.34262750e-01]\n"
     ]
    }
   ],
   "source": [
    "# Check for overfitting\n",
    "\n",
    "print(grid_cv_grad.cv_results_.keys())\n",
    "print(grid_cv_grad.cv_results_[\"mean_train_score\"].shape)  # n_estimators: 11 values, max_depth: 4 values. Thus shape, 11*4=44\n",
    "print(grid_cv_grad.cv_results_[\"mean_test_score\"].shape)\n",
    "\n",
    "print(grid_cv_grad.cv_results_[\"mean_train_score\"])\n",
    "print(grid_cv_grad.cv_results_[\"mean_test_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best mean_train_score: [0.43540853]\n",
      "Best mean_test_score: [0.29284605]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Identify the mean_train_score and mean_test_score values associated with best_score\n",
    "best_score_index = np.where(grid_cv_grad.cv_results_[\"mean_test_score\"]==grid_cv_grad.best_score_)\n",
    "print('Best mean_train_score:', grid_cv_grad.cv_results_[\"mean_train_score\"][best_score_index])\n",
    "print('Best mean_test_score:', grid_cv_grad.cv_results_[\"mean_test_score\"][best_score_index])\n",
    "\n",
    "#Create a df with grid_cv_grad.cv_results_[\"mean_train_score\"] as the first column and grid_cv_grad.cv_results_[\"mean_test_score\"] as the second column\n",
    "grad_results_df = pd.DataFrame({'mean_train_score': grid_cv_grad.cv_results_[\"mean_train_score\"], 'mean_test_score': grid_cv_grad.cv_results_[\"mean_test_score\"]})\n",
    "\n",
    "# Compute a new collumn that is the ratio of mean_train_score to mean_test_score\n",
    "grad_results_df['ratio'] = grad_results_df['mean_train_score']/grad_results_df['mean_test_score']\n",
    "\n",
    "# Identify outliers in ratio\n",
    "Q1 = grad_results_df['ratio'].quantile(0.25)\n",
    "Q3 = grad_results_df['ratio'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers = grad_results_df[(grad_results_df['ratio'] < (Q1 - 1.5 * IQR)) | (grad_results_df['ratio'] > (Q3 + 1.5 * IQR))]\n",
    "\n",
    "# Remove the outliers from grad_results_df\n",
    "grad_results_df = grad_results_df[~grad_results_df['ratio'].isin(outliers['ratio'])]\n",
    "\n",
    "# Create a histogram of ratio\n",
    "plt.hist(grad_results_df['ratio'], bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "The ratio of train kappas to test kappas using our \"best\" set of parameters was 1.4868. This seems like it might correspond to overfitting. But it seems in line with the (non-outlier) ratios among all tested sets of hyperparameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_fall_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
