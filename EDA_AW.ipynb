{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook includes Aaron's attempts at EDA, feature selection/reduction, and initial attempts at trying a couple of different predictive regression models (including hyperparameter tuning)\n",
    "\n",
    "The outline:\n",
    "\n",
    "1. Import the data. These include the accelerometer predictors. We should consider doing a separate analysis without these variables, since relatively few participants have these data\n",
    "2. Remove a couple of variables we won't be using, including:\n",
    "    * id\n",
    "    * ID\n",
    "    * BIA-BIA_BMI (since we already have a BMI variable as part of the physical group)\n",
    "    * PCIAT-PCIAT_01 through 20\n",
    "    * FitnessGram Zone variables that were used to compute the Zone Total (i.e., CU, PU, SRL, SRR, TL) *Note - not entirely sure if we should be doing this*\n",
    "3. Examining correlations between each predictor (individually) and PCIAT-PCIAT_Total\n",
    "4. Examining NaN counts for all variables and, potentially, removing variables that:\n",
    "    * Have very large NaN counts AND\n",
    "    * Don't have face value as predictors AND\n",
    "    * Have low correlations with the outcome variable\n",
    "5. Examining correlations between all predictors and, potentially, removing variables that have extremely high (>0.9) correlation\n",
    "6. Exploring interactions between Seasons and associated predictors:\n",
    "    1. For each predictor that is associated with a Season variable (e.g., within the Physical variables), make a scatterplot of the predictor vs. outcome and display regression lines by Season\n",
    "    2. If there aren't any clear interactions, removing the Season variable from the list of predictors\n",
    "    3. If there do appear to be interactions, creating dummy variables from the Season variable\n",
    "7. Create a linear regression model using a greedy algorithm from the \"bottom up\"\n",
    "    1. Make a list of all numerical predictors and also a new empty data frame with 100(?) rows and the predictors as variables\n",
    "    2. Randomly select a predictor from the list and create a linear model\n",
    "    3. Randomly select a second predictor from the list and add it to the model\n",
    "    4. Perform an F test to see if the new model is significantly better than the old\n",
    "    5. Repeat until the F test is no longer significant\n",
    "    6. Record the predictors that are in the model in the newly-created data frame\n",
    "    7. Repeat the above steps 100 (??) times\n",
    "    8. Compute the mean for each predictor in the data frame. This should give some sense of the \"importance\" of each predictor\n",
    "8. Repeat the previous method but using a \"top down\" algorithm, starting with a full model and removing predictors one-by-one\n",
    "9. *Maybe* Trying to use PCA and either linear or KNN regression to see if it appears to improve prediction\n",
    "    * PCA on the entire set of predictors\n",
    "    * On each set of grouped predictors\n",
    "10. Using RandomForest Regression on the entire set of predictors and examining the importance matrix to try to find a potential list of predictors\n",
    "11. *Maybe* using XGBoost to do stuff. (Need to learn what this is)\n",
    "12. Removing highly-correlated predictors and using LASSO and using LASSO regression (with hyperparameter tuning) to identify important predictors\n",
    "13. Comparing the apparent predictive power of all the previous methods. If none stand out, then stick with linear regression(?)\n",
    "14. Start to engage more formally with the modeling process, using Kfold splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original data set\n",
    "train_cleaned = pd.read_csv('train_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove some variables we won't need\n",
    "\n",
    "# Remove the variables 'id', 'ID', 'BIA-BIA_BMI'\n",
    "train_cleaned = train_cleaned.drop(['id', 'ID', 'BIA-BIA_BMI'], axis=1)\n",
    "\n",
    "# Remove the PCIAT component variables\n",
    "train_cleaned = train_cleaned.loc[:,~train_cleaned.columns.str.startswith('PCIAT-PCIAT_0')]\n",
    "train_cleaned = train_cleaned.loc[:,~train_cleaned.columns.str.startswith('PCIAT-PCIAT_1')]\n",
    "train_cleaned = train_cleaned.drop(['PCIAT-PCIAT_20'], axis=1)\n",
    "\n",
    "# Remove FGC-FGC_CU_Zone, FGC-FGC_PU_Zone, FGC-FGC_SRL_Zone, and FGC-FGC_TL_Zone\n",
    "train_cleaned = train_cleaned.drop(['FGC-FGC_CU_Zone', 'FGC-FGC_PU_Zone', 'FGC-FGC_SRL_Zone', 'FGC-FGC_TL_Zone'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_fall_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
