{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this exploration is to try to identify interaction terms that might be significant predictors.\n",
    "\n",
    "For example, we already know that reference ranges for various BIA measures are differentiated by sex and age, so it seems reasonable that we might find interactions.\n",
    "\n",
    "This work is slightly informed by my reading an online textbook about feature engineering: https://bookdown.org/max/FES/\n",
    "\n",
    "This notebook will use a couple of different techniques to explore potential interactions. Some will be pretty naive, and others somewhat uninformed (i.e., informed by internet forums)\n",
    "\n",
    "Planned approaches:\n",
    "1. Use a random forest regressor (as suggested in this stackexchange forum: https://stats.stackexchange.com/questions/4901/what-are-best-practices-in-identifying-interaction-effects)\n",
    "2. Including interaction terms in a model and then using Lasso regression (as suggested in the same forum, and also in the textbook listed above). See https://www.geeksforgeeks.org/feature-selection-using-selectfrommodel-and-lassocv-in-scikit-learn/ for instructions\n",
    "3. Starting with a full model and testing interaction terms one-by-one. This has the issue of generating false positives. Potential solutions are a Bonferroni correction (likely too conservative) and a False Discovery Rate correction (which I still need to learn more about). Or maybe split the data in half, use half for testing and then run a more limited number of planned contrasts on the saved data.\n",
    "4. Comparing each pairwise model with a model that also includes the interaction term. This has the same issues/limiations as the previous approach\n",
    "\n",
    "All of these approaches need to be conducted using cross-validation in some way to try to reduce Type 1 errors.\n",
    "\n",
    "I'm also thinking that I should start with some EDA to identify variables that are highly correlated and, probably, only use one of each set of highly-correlated variables in cases where I'm using a full model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by loading the data and merging it with the accelerometer data generated by Accelerometer_enmo_anglez_daily_averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data set train_imp.csv\n",
    "train = pd.read_csv('train_cleaned.csv')\n",
    "\n",
    "# Load the accelerometer data set Accelerometer_enmo_anglez_daily_averages.csv\n",
    "accel = pd.read_csv('Accelerometer_enmo_anglez_daily_averages.csv')\n",
    "\n",
    "# Merge train  on the 'id' column and accel on the 'ID' column\n",
    "train_merged = train.merge(accel, left_on='id', right_on='ID')\n",
    "\n",
    "# Create a data frame from accel that only includes ID values for which there is no match in the id column of train\n",
    "accel_missing = accel[~accel['ID'].isin(train['id'])]\n",
    "\n",
    "# It seems unlikly that we're going to want the ENMO_Avg_All_Days_MVPA192 or ENMO_Avg_All_Days_MVPA110 or Positive_Anglez_All_Days variables, so remove them\n",
    "train_merged = train_merged.drop(columns=['ENMO_Avg_All_Days_MVPA192', 'ENMO_Avg_All_Days_MVPA110', 'Positive_Anglez_All_Days'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>ENMO_Avg_Active_Days_MVPA192</th>\n",
       "      <th>ENMO_Avg_Active_Days_MVPA110</th>\n",
       "      <th>ENMO_Avg_All_Days_MVPA192</th>\n",
       "      <th>ENMO_Avg_All_Days_MVPA110</th>\n",
       "      <th>Positive_Anglez_Active_Days</th>\n",
       "      <th>Positive_Anglez_All_Days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19455336</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>36.250000</td>\n",
       "      <td>8.777778</td>\n",
       "      <td>18.166667</td>\n",
       "      <td>151.500000</td>\n",
       "      <td>75.733333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ca33a5e7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>73.333333</td>\n",
       "      <td>32.230769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6b6467f4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>b447e66d</td>\n",
       "      <td>14.818182</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>14.869565</td>\n",
       "      <td>37.739130</td>\n",
       "      <td>136.772727</td>\n",
       "      <td>126.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>adbd6839</td>\n",
       "      <td>18.400000</td>\n",
       "      <td>42.800000</td>\n",
       "      <td>17.761905</td>\n",
       "      <td>41.523810</td>\n",
       "      <td>41.200000</td>\n",
       "      <td>37.954545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>035c96dd</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>13.250000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>9.166667</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>ab16a20d</td>\n",
       "      <td>13.800000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>11.545455</td>\n",
       "      <td>29.545455</td>\n",
       "      <td>37.400000</td>\n",
       "      <td>28.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>070386b2</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>20.714286</td>\n",
       "      <td>7.307692</td>\n",
       "      <td>20.600000</td>\n",
       "      <td>68.812500</td>\n",
       "      <td>64.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>3f1f23e7</td>\n",
       "      <td>19.428571</td>\n",
       "      <td>48.133333</td>\n",
       "      <td>17.210526</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>88.133333</td>\n",
       "      <td>75.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>e46417a7</td>\n",
       "      <td>12.842105</td>\n",
       "      <td>36.263158</td>\n",
       "      <td>12.280000</td>\n",
       "      <td>26.322581</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>35.048780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID  ENMO_Avg_Active_Days_MVPA192  ENMO_Avg_Active_Days_MVPA110  \\\n",
       "4   19455336                     14.000000                     36.250000   \n",
       "5   ca33a5e7                      0.000000                      0.000000   \n",
       "8   6b6467f4                      0.000000                      0.000000   \n",
       "22  b447e66d                     14.818182                     38.000000   \n",
       "24  adbd6839                     18.400000                     42.800000   \n",
       "38  035c96dd                      4.000000                     13.250000   \n",
       "40  ab16a20d                     13.800000                     37.000000   \n",
       "43  070386b2                      7.500000                     20.714286   \n",
       "45  3f1f23e7                     19.428571                     48.133333   \n",
       "46  e46417a7                     12.842105                     36.263158   \n",
       "\n",
       "    ENMO_Avg_All_Days_MVPA192  ENMO_Avg_All_Days_MVPA110  \\\n",
       "4                    8.777778                  18.166667   \n",
       "5                    3.000000                   5.666667   \n",
       "8                    5.000000                  13.000000   \n",
       "22                  14.869565                  37.739130   \n",
       "24                  17.761905                  41.523810   \n",
       "38                   4.000000                   9.166667   \n",
       "40                  11.545455                  29.545455   \n",
       "43                   7.307692                  20.600000   \n",
       "45                  17.210526                  40.000000   \n",
       "46                  12.280000                  26.322581   \n",
       "\n",
       "    Positive_Anglez_Active_Days  Positive_Anglez_All_Days  \n",
       "4                    151.500000                 75.733333  \n",
       "5                     73.333333                 32.230769  \n",
       "8                      0.000000                 10.000000  \n",
       "22                   136.772727                126.375000  \n",
       "24                    41.200000                 37.954545  \n",
       "38                    63.000000                 50.000000  \n",
       "40                    37.400000                 28.583333  \n",
       "43                    68.812500                 64.000000  \n",
       "45                    88.133333                 75.050000  \n",
       "46                    60.000000                 35.048780  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accel_missing.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lots of NaN values. We'll be imputing some/all of these later, but it's problematic to impute prior to doing our feature engineering. So let's explore the NaN distribution for the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3168 rows in train\n",
      "There are 996 rows in accel\n",
      "There are 796 rows in train merged\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nan_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Physical-Waist_Circumference</th>\n",
       "      <td>757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAQ_A-PAQ_A_Total</th>\n",
       "      <td>688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAQ_A-Season</th>\n",
       "      <td>688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fitness_Endurance-Time_Mins</th>\n",
       "      <td>559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fitness_Endurance-Max_Stage</th>\n",
       "      <td>559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              nan_count\n",
       "Physical-Waist_Circumference        757\n",
       "PAQ_A-PAQ_A_Total                   688\n",
       "PAQ_A-Season                        688\n",
       "Fitness_Endurance-Time_Mins         559\n",
       "Fitness_Endurance-Max_Stage         559"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the number of rows in train\n",
    "print('There are',train.shape[0],'rows in train')\n",
    "print('There are',accel.shape[0],'rows in accel')\n",
    "print('There are',train_merged.shape[0],'rows in train merged')\n",
    "\n",
    "# Compute the number of NaN values for each variable in train. Make a dataframe of the variables and their NaN counts\n",
    "nan_counts = pd.DataFrame(train_merged.isnull().sum(), columns=['nan_count'])\n",
    "\n",
    "# Identify the five variables with the largest values in nan_count\n",
    "nan_counts = nan_counts.sort_values(by='nan_count', ascending=False)\n",
    "nan_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nan_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Physical-Waist_Circumference</th>\n",
       "      <td>757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAQ_A-PAQ_A_Total</th>\n",
       "      <td>688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAQ_A-Season</th>\n",
       "      <td>688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fitness_Endurance-Time_Mins</th>\n",
       "      <td>559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fitness_Endurance-Max_Stage</th>\n",
       "      <td>559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fitness_Endurance-Time_Sec</th>\n",
       "      <td>559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FGC-FGC_GSD_Zone</th>\n",
       "      <td>545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FGC-FGC_GSND_Zone</th>\n",
       "      <td>545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FGC-FGC_GSND</th>\n",
       "      <td>542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FGC-FGC_GSD</th>\n",
       "      <td>542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fitness_Endurance-Season</th>\n",
       "      <td>373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAQ_C-Season</th>\n",
       "      <td>366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAQ_C-PAQ_C_Total</th>\n",
       "      <td>366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FGC-FGC_SRL_Zone</th>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FGC-FGC_SRR_Zone</th>\n",
       "      <td>243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FGC-FGC_PU_Zone</th>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FGC-FGC_CU_Zone</th>\n",
       "      <td>241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FGC-FGC_TL_Zone</th>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FGC-FGC_PU</th>\n",
       "      <td>236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FGC-FGC_SRL</th>\n",
       "      <td>236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              nan_count\n",
       "Physical-Waist_Circumference        757\n",
       "PAQ_A-PAQ_A_Total                   688\n",
       "PAQ_A-Season                        688\n",
       "Fitness_Endurance-Time_Mins         559\n",
       "Fitness_Endurance-Max_Stage         559\n",
       "Fitness_Endurance-Time_Sec          559\n",
       "FGC-FGC_GSD_Zone                    545\n",
       "FGC-FGC_GSND_Zone                   545\n",
       "FGC-FGC_GSND                        542\n",
       "FGC-FGC_GSD                         542\n",
       "Fitness_Endurance-Season            373\n",
       "PAQ_C-Season                        366\n",
       "PAQ_C-PAQ_C_Total                   366\n",
       "FGC-FGC_SRL_Zone                    244\n",
       "FGC-FGC_SRR_Zone                    243\n",
       "FGC-FGC_PU_Zone                     242\n",
       "FGC-FGC_CU_Zone                     241\n",
       "FGC-FGC_TL_Zone                     240\n",
       "FGC-FGC_PU                          236\n",
       "FGC-FGC_SRL                         236"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_counts.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RF Regressor Interaction Terms**\n",
    "(from Brave AI)\n",
    "\n",
    "Random Forest Regressor is a powerful ensemble learning algorithm that can capture complex relationships between variables, including interaction terms. However, it does not explicitly model interactions like linear regression does with polynomial terms. Instead, Random Forest Regressor identifies interactions through feature selection and variable importance.\n",
    "\n",
    "Approach\n",
    "\n",
    "Prepare your data: Ensure your dataset is clean, and features are scaled (e.g., using StandardScaler from scikit-learn).\n",
    "\n",
    "Split your data: Divide your dataset into training (e.g., 80%) and testing sets (e.g., 20%) using train_test_split from scikit-learn.\n",
    "\n",
    "Train a Random Forest Regressor: Use RandomForestRegressor from scikit-learn with default hyperparameters (e.g., n_estimators=100, max_depth=None, min_samples_split=2, min_samples_leaf=1).\n",
    "\n",
    "Compute feature importance: Use the feature_importances_ attribute of the trained Random Forest Regressor to identify the most important features.\n",
    "\n",
    "Inspect feature interactions: Analyze the feature importance scores and look for features that have high importance and are correlated with each other. This can indicate potential interaction terms.\n",
    "\n",
    "The code example below identifies the most important features using Random Forest Regressor’s feature importance scores. It then computes the correlation matrix between these important features and looks for pairs with a correlation coefficient greater than 0.5. This can indicate potential interaction terms between the features.\n",
    "\n",
    "Limitations\n",
    "While Random Forest Regressor can identify interactions, it may not capture all possible interactions, especially those with non-linear relationships. Additionally, the feature importance scores may not always accurately reflect the strength of interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is code to do random forest resgressor (thanks, Brave AI!)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('target', axis=1), df['target'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train Random Forest Regressor\n",
    "rf = RandomForestRegressor(n_estimators=100)\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Compute feature importance\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "# Inspect feature interactions\n",
    "correlation_matrix = X_train_scaled.corr()\n",
    "important_features = [feature for feature, importance in zip(X_train.columns, importances) if importance > 0.1]\n",
    "for feature1, feature2 in itertools.combinations(important_features, 2):\n",
    "    if abs(correlation_matrix[feature1][feature2]) > 0.5:\n",
    "        print(f\"Potential interaction: {feature1} and {feature2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll try using LASSO on a full model including interaction terms\n",
    "\n",
    "Some ideas here: https://www.geeksforgeeks.org/feature-selection-using-selectfrommodel-and-lassocv-in-scikit-learn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.linear_model import LassoCV \n",
    "from sklearn.feature_selection import SelectFromModel \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.datasets import load_breast_cancer \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "\n",
    "# Load the Breast Cancer dataset \n",
    "cancer = load_breast_cancer() \n",
    "X, y = cancer.data, cancer.target \n",
    "  \n",
    "# Split the data \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit LassoCV model \n",
    "lasso_cv = LassoCV(cv=5) \n",
    "lasso_cv.fit(X_train, y_train) \n",
    "\n",
    "# Feature selection \n",
    "sfm = SelectFromModel(lasso_cv, prefit=True) \n",
    "X_train_selected = sfm.transform(X_train) \n",
    "X_test_selected = sfm.transform(X_test) \n",
    "\n",
    "# Train a Random Forest Classifier using the selected features \n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42) \n",
    "model.fit(X_train_selected, y_train) \n",
    "\n",
    "\n",
    "# Evaluate the model \n",
    "y_pred = model.predict(X_test_selected) \n",
    "print(classification_report(y_test, y_pred)) \n",
    "\n",
    "# Analyze selected features and their importance \n",
    "selected_feature_indices = np.where(sfm.get_support())[0] \n",
    "selected_features = cancer.feature_names[selected_feature_indices] \n",
    "coefficients = lasso_cv.coef_ \n",
    "print(\"Selected Features:\", selected_features) \n",
    "print(\"Feature Coefficients:\", coefficients) \n",
    "\n",
    "# Extract the selected features from the original dataset \n",
    "X_selected_features = X_train[:, selected_feature_indices] \n",
    "\n",
    "# Create a DataFrame for better visualization \n",
    "selected_features_df = pd.DataFrame(X_selected_features, columns=selected_features) \n",
    "\n",
    "# Add the target variable for coloring \n",
    "selected_features_df['target'] = y_train \n",
    "\n",
    "# Plot the two most important features \n",
    "sns.scatterplot(x='mean area', y='worst area', hue='target', data=selected_features_df, palette='viridis') \n",
    "plt.xlabel('Mean Area') \n",
    "plt.ylabel('Worst Area') \n",
    "plt.title('Scatter Plot of Two Most Important Features - Breast Cancer Dataset') \n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is my old code for trying to do Ridge CV. Keeping it here in case it's helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The alpha value with the lowest RMSE for the train_physical variables is 1 . The mean RMSE was 18.671645670338425  and the standard deviation was 0.06737420892180142\n",
      "The alpha value with the lowest RMSE for the train_fitness variables is 10 . The mean RMSE was 19.05639718656873  and the standard deviation was 0.1264963289808006\n",
      "The alpha value with the lowest RMSE for the train_bia variables is 10 . The mean RMSE was 18.689746957787783  and the standard deviation was 0.04291388407312609\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set up the kfold split\n",
    "num_splits = 5\n",
    "kfold = KFold(num_splits, shuffle=True)\n",
    "\n",
    "# Define a range of alpha values\n",
    "alphas = [0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000]\n",
    "numalphas = len(alphas)\n",
    "\n",
    "# Create an empty array with num_splits rows and numalphas columns\n",
    "rmses = np.zeros((num_splits, numalphas))\n",
    "\n",
    "# Iterate over the three data sets\n",
    "listofdatasets = [train_physical, train_fitness, train_bia]\n",
    "\n",
    "# A data frame to store the optimal alpha values\n",
    "bestalphas = pd.DataFrame(index=range(0,len(listofdatasets)))\n",
    "bestalphas['dfname'] = ''\n",
    "bestalphas['best_alpha_manual'] = np.nan\n",
    "\n",
    "k=0\n",
    "for df in listofdatasets:\n",
    "    i = 0\n",
    "    for train_index, test_index in kfold.split(df):\n",
    "        tt_X = df.iloc[train_index].drop(columns=['PCIAT-PCIAT_Total'])\n",
    "        tt_y = df.iloc[train_index]['PCIAT-PCIAT_Total']\n",
    "        ho_X = df.iloc[test_index].drop(columns=['PCIAT-PCIAT_Total'])\n",
    "        ho_y = df.iloc[test_index]['PCIAT-PCIAT_Total']\n",
    "\n",
    "        # Iterate over alpha values with counter j\n",
    "        j = 0\n",
    "        for alpha in alphas:\n",
    "            ridge_pipe = Pipeline([('scale', StandardScaler()),('ridge', Ridge(alpha=alpha, max_iter=5000000) )])\n",
    "            ridge_pipe.fit(tt_X, tt_y)\n",
    "            y_pred = ridge_pipe.predict(ho_X)\n",
    "            rmses[i, j] = root_mean_squared_error(ho_y, y_pred)\n",
    "            \n",
    "            j=j+1\n",
    "\n",
    "        i=i+1\n",
    "\n",
    "    # Compute the mean of each column of rmses\n",
    "    mean_rmses_within_alphas = np.mean(rmses, axis=0)\n",
    "\n",
    "    # Compute the mean and standard deviation of each row of rmses\n",
    "    mean_rmses = np.mean(mean_rmses_within_alphas, axis=0)\n",
    "    std_rmses = np.std(mean_rmses_within_alphas, axis=0)\n",
    "\n",
    "    # Identify the column of min_rmse that contains the minimum value\n",
    "    best_alpha_index = np.argmin(mean_rmses_within_alphas)\n",
    "\n",
    "    bestalphas.loc[k,'dfname'] = df.name\n",
    "    bestalphas.loc[k,'best_alpha_manual'] = alphas[best_alpha_index]\n",
    "\n",
    "    print('The alpha value with the lowest RMSE for the', df.name ,'variables is', alphas[best_alpha_index],'. The mean RMSE was', mean_rmses, ' and the standard deviation was', std_rmses )\n",
    "    i=0\n",
    "    j=0\n",
    "    k=k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dfname</th>\n",
       "      <th>best_alpha_manual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_physical</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_fitness</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_bia</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dfname  best_alpha_manual\n",
       "0  train_physical                0.1\n",
       "1   train_fitness               10.0\n",
       "2       train_bia               10.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestalphas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for kicks, I'm going to try this out with the RidgeCV class\n",
    "\n",
    "This will make it easier for me to try a wider range of alpha values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# A data frame to store the optimal alpha values\n",
    "bestalphas = pd.DataFrame(index=range(0,len(listofdatasets)))\n",
    "bestalphas['dfname'] = ''\n",
    "bestalphas['best_alpha_manual'] = np.nan\n",
    "bestalphas['best_alpha_automatic'] = np.nan\n",
    "\n",
    "alphas = 10**np.linspace(10,-2,100)*0.5\n",
    "#alphas = [0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000]\n",
    "\n",
    "for df in listofdatasets:\n",
    "    X_train = df.drop(columns=['PCIAT-PCIAT_Total'])\n",
    "    y_train = df['PCIAT-PCIAT_Total']\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_std = scaler.transform(X_train)\n",
    "    lassocv = LassoCV(alphas = alphas, scoring = 'neg_root_mean_squared_error')\n",
    "    lassocv.fit(X_std, y_train)\n",
    "    bestalphas.loc[bestalphas['dfname']==df.name,'best_alpha_automatic']=lassocv.alpha_.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dfname</th>\n",
       "      <th>best_alpha_manual</th>\n",
       "      <th>best_alpha_automatic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_physical</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.320794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_fitness</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.372746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_bia</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.372746</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dfname  best_alpha_manual  best_alpha_automatic\n",
       "0  train_physical                1.0              2.320794\n",
       "1   train_fitness               10.0             16.372746\n",
       "2       train_bia               10.0             16.372746"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestalphas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the hyperparameter is tuned, we'll compare the performance of the ridge regression and a PCA.\n",
    "\n",
    "Note that in previous explorations we've identified n=3 as the \"ideal\" number of PCA components for each set of predictor variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_physical Ridge Training MSE: 18.533899980839752\n",
      "train_physical PCA Training MSE: 19.235425201158005\n",
      "train_fitness Ridge Training MSE: 18.695066004261484\n",
      "train_fitness PCA Training MSE: 19.37497603121806\n",
      "train_bia Ridge Training MSE: 18.770987125663712\n",
      "train_bia PCA Training MSE: 19.057632568137585\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "for df in listofdatasets:\n",
    "    # Identify the best alpha value we computed earlier\n",
    "    best_alpha = bestalphas.loc[bestalphas['dfname'] == df.name, 'best_alpha'].values[0]\n",
    "    \n",
    "    # Instantiate some models. From previous exploration, we've been using 3 components for the PCA\n",
    "    ridge_pipe = Pipeline([('scale', StandardScaler()), ('ridge', Ridge(alpha = best_alpha, max_iter=5000000))])\n",
    "    pca_pipe = Pipeline([('scale', StandardScaler()), ('pca', PCA(n_components=3)), ('reg', LinearRegression())])\n",
    "\n",
    "    # The training data\n",
    "    X_train = df.iloc[train_index].drop(columns=['PCIAT-PCIAT_Total'])\n",
    "    y_train =  df.iloc[train_index]['PCIAT-PCIAT_Total']\n",
    "\n",
    "    # Fit the models to the training data\n",
    "    ridge_pipe.fit(X_train, y_train)\n",
    "    pca_pipe.fit(X_train, y_train)\n",
    "\n",
    "    # Find the model predictions on the training set\n",
    "    ridge_train_preds = ridge_pipe.predict(X_train)\n",
    "    pca_train_preds = pca_pipe.predict(X_train)\n",
    "\n",
    "    # Find the mse on the training set\n",
    "    ridge_train_rmse = root_mean_squared_error(y_train, ridge_train_preds)\n",
    "    pca_train_rmse = root_mean_squared_error(y_train, pca_train_preds)\n",
    "\n",
    "    # Results\n",
    "    print(df.name, f\"Ridge Training MSE: {ridge_train_rmse}\")\n",
    "    print(df.name, f\"PCA Training MSE: {pca_train_rmse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_fall_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
