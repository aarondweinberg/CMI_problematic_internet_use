{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal**\n",
    "\n",
    "The goal of this notebook is to create code that will take as an input the raw problematic internet use data, incorporate additional variables, and output \"cleaned\" data\n",
    "\n",
    "Note that this is not written as a class that could be called from within a pipe.\n",
    "\n",
    "If we wanted to do that, here is a Kaggle notebook that has useful-looking examples:\n",
    "https://www.kaggle.com/code/ksvmuralidhar/creating-custom-transformers-using-scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The original data has been named train_original\n",
    "#Here I am splitting the data into a train and test set. I want to stratify by age. \n",
    "# Then I export as csv files, since we are working over multiple jupiter notebooks.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_original=pd.read_csv('train_original.csv')\n",
    "\n",
    "seed=1275\n",
    "train, test = train_test_split(train_original, test_size=0.2, random_state=seed, stratify=train_original['Basic_Demos-Age'])\n",
    "\n",
    "train.to_csv('train.csv', index=False)\n",
    "test.to_csv('test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the starting data.\n",
    "train=pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding Accelerometer Data**\n",
    "\n",
    "We have accelerometer data that should be merged into this data set\n",
    "\n",
    "Note that there are some participants who appear to have accelerometer data but aren't listed in train (they're likely in test). So we'll need to do a 'left' join to avoid incorporating participants who aren't in train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the accelerometer data set Accelerometer_enmo_anglez_daily_averages.csv\n",
    "accel = pd.read_csv('Accelerometer_enmo_anglez_daily_averages.csv')\n",
    "\n",
    "# Join train and accel  on the 'id' column and accel on the 'ID' column\n",
    "train = train.join(accel.set_index('ID'), on='id', how='left')\n",
    "\n",
    "# It seems unlikly that we're going to want the ENMO_Avg_All_Days_MVPA192 or ENMO_Avg_All_Days_MVPA110 or Positive_Anglez_All_Days variables, so remove them\n",
    "# Note: These variables no longer get generated from the Acclerometer_Computations, so we shouldn't need to remove them\n",
    "#train = train.drop(columns=['ENMO_Avg_All_Days_MVPA192', 'ENMO_Avg_All_Days_MVPA110', 'Positive_Anglez_All_Days']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Averaging Sit & Reach Data**\n",
    "\n",
    "The Sit & Reach test is done twice, once with the left leg extended (SRL) and once with the right leg extended (SRR). Measuring this twice seems redundant, so we'll create a new variable that is the average of the right & left variables, then delete the SRL and SRR variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new variable 'FGC-FGC_SR' that is the mean of FGC-FGC_SRL and FGC-FGC_SRR\n",
    "train['FGC-FGC_SR'] = train[['FGC-FGC_SRL', 'FGC-FGC_SRR']].mean(axis=1)\n",
    "\n",
    "# Remove the old sit & reach variables\n",
    "train = train.drop(columns=['FGC-FGC_SRL', 'FGC-FGC_SRR', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR_Zone'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculating Sit & Reach Zone**\n",
    "\n",
    "FitnessGram Healthy Fitness Zones are documented at https://pftdata.org/files/hfz-standards.pdf\n",
    "\n",
    "We can use these to compute a new Zone variable for sit & reach\n",
    "\n",
    "Note that Basic_Demos-Sex is coded as 0=Male and 1=Female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new variable 'FGC-FGC_SR_Zone' that is equal to 1 if any of the following are true:\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_SR >= 8\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_SR_Zone >= 9 and Basic_Demos-Age is between 5 and 10\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_SR_Zone >= 10 and Basic_Demos-Age is between 11 and 14\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_SR_Zone >= 12 and Basic_Demos-Age is at least 15\n",
    "\n",
    "# One way to do this is to define a function that would take sex, age, and SR value as inputs and output 1 or 0\n",
    "def sitreachzone(sex, age, sr):\n",
    "    try:\n",
    "        if np.isnan(sr):\n",
    "            return np.nan\n",
    "        elif sex == 0 and sr>=8:\n",
    "            return 1\n",
    "        elif sex == 1 and age >= 15 and sr >= 12:\n",
    "            return 1\n",
    "        elif sex == 1 and age >= 11 and sr >= 10:\n",
    "            return 1\n",
    "        elif sex == 1 and age >= 5 and sr >= 9:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# Apply sitreachzone to create a new column using the columns Basic_Demos-Sex, Basic_Demos-Age, and FGC-FGC_SR as inputs\n",
    "train['FGC-FGC_SR_Zone'] = train.apply(lambda x: sitreachzone(x['Basic_Demos-Sex'], x['Basic_Demos-Age'], x['FGC-FGC_SR']), axis=1)\n",
    "\n",
    "\n",
    "# Note: The internet suggests that using .loc is vectorized, so much faster than using .apply. Below is a faster version that we could use if necessary\n",
    "#train['FGC-FGC_SR_Zone'] = train.loc[(train['Basic_Demos-Sex']==0) & (train['FGC-FGC_SR'] >= 8)] = 1\n",
    "#train['FGC-FGC_SR_Zone'] = train.loc[(train['Basic_Demos-Sex']==1) & (train['FGC-FGC_SR'] >= 12)] = 1\n",
    "#train['FGC-FGC_SR_Zone'] = train.loc[(train['Basic_Demos-Sex']==1) & (train['FGC-FGC_SR'] >= 10) & (train['FGC-FGC_SR'] <= 14)] = 1\n",
    "#train['FGC-FGC_SR_Zone'] = train.loc[(train['Basic_Demos-Sex']==1) & (train['FGC-FGC_SR'] >= 9) & (train['FGC-FGC_SR'] <= 10)] = 1\n",
    "#train['FGC-FGC_SR_Zone'] = train['FGC-FGC_SR_Zone'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating the PAQ_MVPA_60 Variable**\n",
    "\n",
    "Some research (https://pubmed.ncbi.nlm.nih.gov/27759968/) has identified a cut-off score of 2.75 (A) and 2.73 (C) to discriminate >60 minutes of MVPA. However, the study suggests that, while the cutoff is significant for PAQ-A, it isn't for PAQ-C.\n",
    "\n",
    "With that caveat, before combining PAQ-A and PAQ-C, we'll create a new binary variable that flags for >60 minutes of MVPA, called MVPA_60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new variable that is 1 when PAQA/C Total is at least 2.75/2.73, 0 if it's less than these cutoffs, and NaN if PAQA/C is NaN\n",
    "train['PAQA_Zone'] = np.where(train['PAQ_A-PAQ_A_Total']>=2.75, 1, 0)\n",
    "train['PAQA_Zone'] = np.where(train['PAQ_A-PAQ_A_Total'].isnull(), np.nan, train['PAQA_Zone'])\n",
    "\n",
    "train['PAQC_Zone'] = np.where(train['PAQ_C-PAQ_C_Total']>=2.73, 1, 0)\n",
    "train['PAQC_Zone'] = np.where(train['PAQ_C-PAQ_C_Total'].isnull(), np.nan, train['PAQC_Zone'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combining PAQ_A and PAQ_C Predictors**\n",
    "\n",
    "The variables PAQ_A (Season and Total) and PAQ_C (Season and Total) both report \"Information about children's participation in vigorous activities over the last 7 days.\" \n",
    "* More information about PAQ-C is available here: https://fcon_1000.projects.nitrc.org/indi/cmi_healthy_brain_network_old/assessments/paq-c.html. It is administered to participnats age 8-14\n",
    "* More information about PAQ-A is available here: https://fcon_1000.projects.nitrc.org/indi/cmi_healthy_brain_network_old/assessments/paq-a.html. It is administered to participants age 14-19\n",
    "\n",
    "These scores appear to be comparable, so we can combine them. However, prior to doing so, we should note that there could be participants who had scores for both measures. This would occur if their age was recorded in a different season than the PCIAT and/or PAQ seasons.\n",
    "\n",
    "If we were being really careful, we'd construct a complicated function to account for these cases. However, exploration of the original training set (3600 participants) only found one such case, so it seems like it might be a relatively rare event. In addition, by combining the two PAQ columns we are assuming that the two scores are comparable. Thus, it makes sense to keep either of the A/C values.\n",
    "\n",
    "For the one subject in the original data, their recorded age was 13; their PCIAT-Season and PAQ_C-Season were Spring, so when we combine these variables we'll keep the PAQ_C values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new variables that merge the three PAQA/C variables\n",
    "train['PAQ_Total']=train['PAQ_C-PAQ_C_Total']\n",
    "train.loc[train['PAQ_Total'].isnull(),'PAQ_Total']=train['PAQ_A-PAQ_A_Total']\n",
    "\n",
    "train['PAQ_Season']=train['PAQ_C-Season']\n",
    "train.loc[train['PAQ_Season'].isnull(),'PAQ_Season']=train['PAQ_A-Season']\n",
    "\n",
    "train['PAQ_Zone']=train['PAQC_Zone']\n",
    "train.loc[train['PAQ_Zone'].isnull(),'PAQ_Zone']=train['PAQA_Zone']\n",
    "\n",
    "# Drop the PAQ variables we no longer need\n",
    "train=train.drop(columns=['PAQ_C-PAQ_C_Total', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season', 'PAQ_A-Season', 'PAQA_Zone', 'PAQC_Zone'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating the Fitness Endurance Variable**\n",
    "\n",
    "There are currently separate variables for fitness endurance minutes & seconds. We'll combine these into a single variable that measures the total number of seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the minutes and seconds of Fitness_Endurance into a single number (total number of seconds)\n",
    "train['Fitness_Endurance_Total_Time_Sec'] = train['Fitness_Endurance-Time_Mins'] * 60 + train['Fitness_Endurance-Time_Sec']\n",
    "\n",
    "train=train.drop(columns=['Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sleep Disturbance Scale Variable Removal**\n",
    "\n",
    "The sleep disturbance scale was created/documented in 1996: https://pubmed.ncbi.nlm.nih.gov/9065877/\n",
    "\n",
    "There are two Sleep Disturbance Scale variables: the \"Raw\" score and the Total T-Score. \n",
    "I can't find much information about what these mean. But if the T-Score is just a standardized version of the Raw score, then they should be conveying identical information.\n",
    "\n",
    "Their correlation is 0.995731. It seems reasonable and safe to drop one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the SDS-SDS_Total_T variable from train\n",
    "train=train.drop(columns=['SDS-SDS_Total_T'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BIA Variable Removal**\n",
    "\n",
    "Some of the BIA variables appear to either be redundant or computed from each other:\n",
    "* BIA-BIA_BMI is measuring BMI. It has correlation 0.965105 with Physical_BMI. I can't find any information about how it is any different. It seems likely that BMI varies from day to day and measurement to measurement, so the difference is likely due to measurement error. Going to remove BIA-BIA_BMI\n",
    "\n",
    "The other BIA variables are:\n",
    "* BIA-BIA_BMC\tBone Mineral Content\n",
    "* BIA-BIA_BMR\tBasal Metabolic Rate\n",
    "* BIA-BIA_DEE\tDaily Energy Expenditure\n",
    "* BIA-BIA_LDM\tLean Dry Mass\n",
    "* BIA-BIA_LST\tLean Soft Tissue\n",
    "* BIA-BIA_SMM\tSkeletal Muscle Mass\n",
    "* BIA-BIA_FFM\tFat Free Mass\n",
    "* BIA-BIA_Fat\tBody Fat Percentage\n",
    "* BIA-BIA_Frame_num\tBody Frame\n",
    "* BIA-BIA_TBW\tTotal Body Water\n",
    "* BIA-BIA_ECW\tExtracellular Water\n",
    "* BIA-BIA_ICW\tIntracellular Water\n",
    "* BIA-BIA_FMI\tFat Mass Index: Calcuated as FM divided by height squared\n",
    "* BIA-BIA_FFMI\tFat Free Mass Index: Calcuated as FFM divided by height squared\n",
    "\n",
    "There exist large correlations between many pairs of these predictors. Below is a list of all pairs with at least 0.99 correlation:\n",
    "* FFM\tBMR\t0.9999999999991445\n",
    "* BMR\tTBW\t0.9996843922178612\n",
    "* FFM\tTBW\t0.9996843882486972\n",
    "* ECW\tBMR\t0.99949744961038\n",
    "* ECW\tFFM\t0.9994974489820077\n",
    "* TBW\tECW\t0.9994121539401678\n",
    "* TBW\tICW\t0.9989842717508111\n",
    "* FFM\tLDM\t0.9989337841842884\n",
    "* BMR\tLDM\t0.9989337768885788\n",
    "* SMM\tICW\t0.9984531398777805\n",
    "* ICW\tBMR\t0.9981423016493606\n",
    "* FFM\tICW\t0.998142293296351\n",
    "* ECW\tLDM\t0.9980088142821305\n",
    "* TBW\tSMM\t0.9976684856970098\n",
    "* TBW\tLDM\t0.9974587197051186\n",
    "* ICW\tLST\t0.9969014072300802\n",
    "* SMM\tBMR\t0.996864975676666\n",
    "* FFM\tSMM\t0.9968649649869374\n",
    "* ECW\tICW\t0.996852207975357\n",
    "* LST\tSMM\t0.9967543066184706\n",
    "* SMM\tECW\t0.9957140182568774\n",
    "* TBW\tLST\t0.995487594282981\n",
    "* BMC\tLDM\t0.9950199542105168\n",
    "* LDM\tICW\t0.9949518859314233\n",
    "* ICW\tDEE\t0.9948088046663506\n",
    "* DEE\tTBW\t0.9944107160263206\n",
    "* DEE\tBMR\t0.9941186367579632\n",
    "* DEE\tFFM\t0.9941186344807962\n",
    "* LDM\tSMM\t0.9937473329914341\n",
    "* LST\tBMR\t0.9936088620550899\n",
    "* LST\tFFM\t0.99360884373626\n",
    "* DEE\tSMM\t0.9933843458863473\n",
    "* LST\tECW\t0.9930574064543182\n",
    "* ECW\tDEE\t0.992754802616644\n",
    "* DEE\tLDM\t0.9919453675327521\n",
    "* FFM\tBMC\t0.9914085178617896\n",
    "* BMR\tBMC\t0.9914084966327703\n",
    "* LST\tDEE\t0.9912776109118937\n",
    "* BMC\tECW\t0.9909706551293764\n",
    "\n",
    "If we keep the BIA-BIA_FFM variable, then we could eliminate:\n",
    "* BMR\n",
    "* TBW\n",
    "* ECW\n",
    "* ICW\n",
    "* LDM\n",
    "* SMM\n",
    "* DEE\n",
    "* LST\n",
    "* BMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove BIA-BIA_BMI from train\n",
    "train=train.drop(columns=['BIA-BIA_BMI'])\n",
    "\n",
    "# Remove the following variables from train: BIA-BIA_BMR, BIA-BIA_TBW, BIA-BIA_ECW, BIA-BIA_LDM, BIA-BIA_ICW, BIA-BIA_SMM, BIA-BIA_DEE, BIA-BIA_LST, and BIA-BIA_BMC\n",
    "train=train.drop(columns=['BIA-BIA_BMR', 'BIA-BIA_TBW', 'BIA-BIA_ECW', 'BIA-BIA_LDM', 'BIA-BIA_ICW', 'BIA-BIA_SMM', 'BIA-BIA_DEE', 'BIA-BIA_LST', 'BIA-BIA_BMC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing Negative Values**\n",
    "\n",
    "None of the quantitative variables should have negative values, so we'll make a list of these numerical variables and replace any negative entries with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of numerical columns of type float. Note that these columns include the \"Zone\" variables which are really categorical/ordinal:\n",
    "float_columns = train.select_dtypes(include=['float']).columns\n",
    "\n",
    "# Change negative values to NaN\n",
    "train[train[float_columns] < 0] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing 0 Values from Physical Variables**\n",
    "\n",
    "All of the Physical- variables should have non-zero values (this is not be the case for most other variables). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each variable that starts with 'Physical-' replace any values that are 0 with NaN\n",
    "for column in train.columns:\n",
    "    if column.startswith('Physical-'):\n",
    "        train[column] = train[column].replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing Outliers**\n",
    "\n",
    "We'll define an \"outlier\" as any value in a column that is more than 5 standard deviations above or below the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each column in float_columns, identify entries that are 5 standard deviations above or below the mean and replace them with NaN\n",
    "for column in float_columns:\n",
    "    train[column] = train[column].mask(train[column] > train[column].mean() + 5 * train[column].std())\n",
    "    train[column] = train[column].mask(train[column] < train[column].mean() - 5 * train[column].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Export Dataframe to CSV**\n",
    "\n",
    "We'll do this here to support our EDA work. But the code above will eventually make its way into a unified code file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train_cleaned.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_fall_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
