{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The original data has been named train_original\n",
    "#Here I am splitting the data into a train and test set. I want to stratify by age. \n",
    "# Then I export as csv files, since we are working over multiple jupiter notebooks.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_original=pd.read_csv('train_original.csv')\n",
    "\n",
    "seed=1275\n",
    "train, test = train_test_split(train_original, test_size=0.2, random_state=seed, stratify=train_original['Basic_Demos-Age'])\n",
    "\n",
    "train.to_csv('train.csv', index=False)\n",
    "test.to_csv('test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the starting data.\n",
    "train=pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding Accelerometer Data**\n",
    "\n",
    "We have accelerometer data that should be merged into this data set\n",
    "\n",
    "Note that there are some participants who appear to have accelerometer data but aren't listed in train (they're likely in test). So we'll need to do a 'left' join to avoid incorporating participants who aren't in train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the accelerometer data set Accelerometer_enmo_anglez_daily_averages.csv\n",
    "accel = pd.read_csv('Accelerometer_enmo_anglez_daily_averages.csv')\n",
    "\n",
    "# Join train and accel  on the 'id' column and accel on the 'ID' column\n",
    "train = train.join(accel.set_index('ID'), on='id', how='left')\n",
    "\n",
    "# It seems unlikly that we're going to want the ENMO_Avg_All_Days_MVPA192 or ENMO_Avg_All_Days_MVPA110 or Positive_Anglez_All_Days variables, so remove them\n",
    "train = train.drop(columns=['ENMO_Avg_All_Days_MVPA192', 'ENMO_Avg_All_Days_MVPA110', 'Positive_Anglez_All_Days']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Averaging Sit & Reach Data**\n",
    "\n",
    "The Sit & Reach test is done twice, once with the left leg extended (SRL) and once with the right leg extended (SRR). Measuring this twice seems redundant, so we'll create a new variable that is the average of the right & left variables, then delete the SRL and SRR variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new variable 'FGC-FGC_SR' that is the mean of FGC-FGC_SRL and FGC-FGC_SRR\n",
    "train['FGC-FGC_SR'] = train[['FGC-FGC_SRL', 'FGC-FGC_SRR']].mean(axis=1)\n",
    "\n",
    "# Remove the old sit & reach variables\n",
    "train = train.drop(columns=['FGC-FGC_SRL', 'FGC-FGC_SRR', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR_Zone'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculating Sit & Reach Zone**\n",
    "\n",
    "FitnessGram Healthy Fitness Zones are documented at https://pftdata.org/files/hfz-standards.pdf\n",
    "\n",
    "We can use these to compute a new Zone variable for sit & reach\n",
    "\n",
    "Note that Basic_Demos-Sex is coded as 0=Male and 1=Female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new variable 'FGC-FGC_SR_Zone' that is equal to 1 if any of the following are true:\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_SR >= 8\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_SR_Zone >= 9 and Basic_Demos-Age is between 5 and 10\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_SR_Zone >= 10 and Basic_Demos-Age is between 11 and 14\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_SR_Zone >= 12 and Basic_Demos-Age is at least 15\n",
    "\n",
    "# One way to do this is to define a function that would take sex, age, and SR value as inputs and output 1 or 0\n",
    "def sitreachzone(sex:int, age:int, sr:int):\n",
    "    if sex == 0:\n",
    "        if sr >= 8:\n",
    "            return 1\n",
    "        elif sr <= 7:\n",
    "            return 0\n",
    "        else:\n",
    "            return np.nan\n",
    "    elif sex == 1:\n",
    "        if age >= 15:\n",
    "            if sr >= 12:\n",
    "                return 1\n",
    "            elif sr <= 11:\n",
    "                return 0\n",
    "            else:\n",
    "                return np.nan\n",
    "        elif age >= 11:\n",
    "            if sr >= 10:\n",
    "                return 1\n",
    "            elif sr <=9:\n",
    "                return 0\n",
    "            else:\n",
    "                return np.nan\n",
    "        else:\n",
    "            if sr >= 9:\n",
    "                return 1\n",
    "            elif sr <= 8:\n",
    "                return 0\n",
    "            else:\n",
    "                return np.nan\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# Apply sitreachzone to create a new column using the columns Basic_Demos-Sex, Basic_Demos-Age, and FGC-FGC_SR as inputs\n",
    "train['FGC-FGC_SR_Zone'] = train.apply(lambda x: sitreachzone(x['Basic_Demos-Sex'], x['Basic_Demos-Age'], x['FGC-FGC_SR']), axis=1)\n",
    "\n",
    "\n",
    "# Note: The internet suggests that using .loc is vectorized, so much faster than using .apply. Below is a faster version that we could use if necessary\n",
    "#train['FGC-FGC_SR_Zone'] = train.loc[(train['Basic_Demos-Sex']==0) & (train['FGC-FGC_SR'] >= 8)] = 1\n",
    "#train['FGC-FGC_SR_Zone'] = train.loc[(train['Basic_Demos-Sex']==1) & (train['FGC-FGC_SR'] >= 12)] = 1\n",
    "#train['FGC-FGC_SR_Zone'] = train.loc[(train['Basic_Demos-Sex']==1) & (train['FGC-FGC_SR'] >= 10) & (train['FGC-FGC_SR'] <= 14)] = 1\n",
    "#train['FGC-FGC_SR_Zone'] = train.loc[(train['Basic_Demos-Sex']==1) & (train['FGC-FGC_SR'] >= 9) & (train['FGC-FGC_SR'] <= 10)] = 1\n",
    "#train['FGC-FGC_SR_Zone'] = train['FGC-FGC_SR_Zone'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating the PAQ_MVPA_60 Variable**\n",
    "\n",
    "Some research (https://pubmed.ncbi.nlm.nih.gov/27759968/) has identified a cut-off score of 2.75 (A) and 2.73 (C) to discriminate >60 minutes of MVPA. However, the study suggests that, while the cutoff is significant for PAQ-A, it isn't for PAQ-C.\n",
    "\n",
    "With that caveat, before combining PAQ-A and PAQ-C, we'll create a new binary variable that flags for >60 minutes of MVPA, called MVPA_60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new variable that is 1 when PAQA/C Total is at least 2.75/2.73, 0 if it's less than these cutoffs, and NaN if PAQA/C is NaN\n",
    "train['PAQA_MVPA'] = np.where(train['PAQ_A-PAQ_A_Total']>=2.75, 1, 0)\n",
    "train['PAQA_MVPA'] = np.where(train['PAQ_A-PAQ_A_Total'].isnull(), np.nan, train['PAQA_MVPA'])\n",
    "\n",
    "train['PAQC_MVPA'] = np.where(train['PAQ_C-PAQ_C_Total']>=2.73, 1, 0)\n",
    "train['PAQC_MVPA'] = np.where(train['PAQ_C-PAQ_C_Total'].isnull(), np.nan, train['PAQC_MVPA'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combining PAQ_A and PAQ_C Predictors**\n",
    "\n",
    "The variables PAQ_A (Season and Total) and PAQ_C (Season and Total) both report \"Information about children's participation in vigorous activities over the last 7 days.\" \n",
    "* More information about PAQ-C is available here: https://fcon_1000.projects.nitrc.org/indi/cmi_healthy_brain_network_old/assessments/paq-c.html. It is administered to participnats age 8-14\n",
    "* More information about PAQ-A is available here: https://fcon_1000.projects.nitrc.org/indi/cmi_healthy_brain_network_old/assessments/paq-a.html. It is administered to participants age 14-19\n",
    "\n",
    "These scores appear to be comparable, so we can combine them. However, prior to doing so, we should note that there could be participants who had scores for both measures. This would occur if their age was recorded in a different season than the PCIAT and/or PAQ seasons.\n",
    "\n",
    "If we were being really careful, we'd construct a complicated function to account for these cases. However, exploration of the original training set (3600 participants) only found one such case, so it seems like it might be a relatively rare event. In addition, by combining the two PAQ columns we are assuming that the two scores are comparable. Thus, it makes sense to keep either of the A/C values.\n",
    "\n",
    "For the one subject in the original data, their recorded age was 13; their PCIAT-Season and PAQ_C-Season were Spring, so when we combine these variables we'll keep the PAQ_C values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new variables that merge the three PAQA/C variables\n",
    "train['PAQ_Total']=train['PAQ_C-PAQ_C_Total']\n",
    "train.loc[train['PAQ_Total'].isnull(),'PAQ_Total']=train['PAQ_A-PAQ_A_Total']\n",
    "\n",
    "train['PAQ_Season']=train['PAQ_C-Season']\n",
    "train.loc[train['PAQ_Season'].isnull(),'PAQ_Season']=train['PAQ_A-Season']\n",
    "\n",
    "train['PAQ_MVPA']=train['PAQC_MVPA']\n",
    "train.loc[train['PAQ_MVPA'].isnull(),'PAQ_MVPA']=train['PAQA_MVPA']\n",
    "\n",
    "# Drop the PAQ variables we no longer need\n",
    "train=train.drop(columns=['PAQ_C-PAQ_C_Total', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season', 'PAQ_A-Season', 'PAQA_MVPA', 'PAQC_MVPA'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating the Fitness Endurance Variable**\n",
    "\n",
    "There are currently separate variables for fitness endurance minutes & seconds. We'll combine these into a single variable that measures the total number of seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the minutes and seconds of Fitness_Endurance into a single number (total number of seconds)\n",
    "train['Fitness_Endurance_Total_Time_Sec'] = train['Fitness_Endurance-Time_Mins'] * 60 + train['Fitness_Endurance-Time_Sec']\n",
    "\n",
    "train=train.drop(columns=['Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sleep Disturbance Scale Cleaning**\n",
    "\n",
    "The sleep disturbance scale was created/documented in 1996: https://pubmed.ncbi.nlm.nih.gov/9065877/\n",
    "\n",
    "There are two Sleep Disturbance Scale variables: the \"Raw\" score and the Total T-Score. \n",
    "I can't find much information about what these mean. But if the T-Score is just a standardized version of the Raw score, then they should be conveying identical information.\n",
    "\n",
    "Their correlation is 0.995731. It seems reasonable and safe to drop one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the SDS-SDS_Total_T variable from train\n",
    "train=train.drop(columns=['SDS-SDS_Total_T'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BIA Variable Cleaning**\n",
    "\n",
    "Some of the BIA variables appear to either be redundant or computed from each other:\n",
    "* BIA-BIA_BMI is measuring BMI. It has correlation 0.965105 with Physical_BMI. I can't find any information about how it is any different. It seems likely that BMI varies from day to day and measurement to measurement, so the difference is likely due to measurement error. Going to remove BIA-BIA_BMI\n",
    "\n",
    "The other BIA variables are:\n",
    "* BIA-BIA_BMC\tBone Mineral Content\n",
    "* BIA-BIA_BMR\tBasal Metabolic Rate\n",
    "* BIA-BIA_DEE\tDaily Energy Expenditure\n",
    "* BIA-BIA_LDM\tLean Dry Mass\n",
    "* BIA-BIA_LST\tLean Soft Tissue\n",
    "* BIA-BIA_SMM\tSkeletal Muscle Mass\n",
    "* BIA-BIA_FFM\tFat Free Mass\n",
    "* BIA-BIA_Fat\tBody Fat Percentage\n",
    "* BIA-BIA_Frame_num\tBody Frame\n",
    "* BIA-BIA_TBW\tTotal Body Water\n",
    "* BIA-BIA_ECW\tExtracellular Water\n",
    "* BIA-BIA_ICW\tIntracellular Water\n",
    "* BIA-BIA_FMI\tFat Mass Index: Calcuated as FM divided by height squared\n",
    "* BIA-BIA_FFMI\tFat Free Mass Index: Calcuated as FFM divided by height squared\n",
    "\n",
    "There exist large correlations between many pairs of these predictors. Below is a list of all pairs with at least 0.99 correlation:\n",
    "* FFM\tBMR\t0.9999999999991445\n",
    "* BMR\tTBW\t0.9996843922178612\n",
    "* FFM\tTBW\t0.9996843882486972\n",
    "* ECW\tBMR\t0.99949744961038\n",
    "* ECW\tFFM\t0.9994974489820077\n",
    "* TBW\tECW\t0.9994121539401678\n",
    "* TBW\tICW\t0.9989842717508111\n",
    "* FFM\tLDM\t0.9989337841842884\n",
    "* BMR\tLDM\t0.9989337768885788\n",
    "* SMM\tICW\t0.9984531398777805\n",
    "* ICW\tBMR\t0.9981423016493606\n",
    "* FFM\tICW\t0.998142293296351\n",
    "* ECW\tLDM\t0.9980088142821305\n",
    "* TBW\tSMM\t0.9976684856970098\n",
    "* TBW\tLDM\t0.9974587197051186\n",
    "* ICW\tLST\t0.9969014072300802\n",
    "* SMM\tBMR\t0.996864975676666\n",
    "* FFM\tSMM\t0.9968649649869374\n",
    "* ECW\tICW\t0.996852207975357\n",
    "* LST\tSMM\t0.9967543066184706\n",
    "* SMM\tECW\t0.9957140182568774\n",
    "* TBW\tLST\t0.995487594282981\n",
    "* BMC\tLDM\t0.9950199542105168\n",
    "* LDM\tICW\t0.9949518859314233\n",
    "* ICW\tDEE\t0.9948088046663506\n",
    "* DEE\tTBW\t0.9944107160263206\n",
    "* DEE\tBMR\t0.9941186367579632\n",
    "* DEE\tFFM\t0.9941186344807962\n",
    "* LDM\tSMM\t0.9937473329914341\n",
    "* LST\tBMR\t0.9936088620550899\n",
    "* LST\tFFM\t0.99360884373626\n",
    "* DEE\tSMM\t0.9933843458863473\n",
    "* LST\tECW\t0.9930574064543182\n",
    "* ECW\tDEE\t0.992754802616644\n",
    "* DEE\tLDM\t0.9919453675327521\n",
    "* FFM\tBMC\t0.9914085178617896\n",
    "* BMR\tBMC\t0.9914084966327703\n",
    "* LST\tDEE\t0.9912776109118937\n",
    "* BMC\tECW\t0.9909706551293764\n",
    "\n",
    "If we keep the BIA-BIA_FFM variable, then we could eliminate:\n",
    "* BMR\n",
    "* TBW\n",
    "* ECW\n",
    "* ICW\n",
    "* LDM\n",
    "* SMM\n",
    "* DEE\n",
    "* LST\n",
    "* BMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove BIA-BIA_BMI from train\n",
    "train=train.drop(columns=['BIA-BIA_BMI'])\n",
    "\n",
    "# Remove the following variables from train: BIA-BIA_BMR, BIA-BIA_TBW, BIA-BIA_ECW, BIA-BIA_LDM, BIA-BIA_ICW, BIA-BIA_SMM, BIA-BIA_DEE, BIA-BIA_LST, and BIA-BIA_BMC\n",
    "train=train.drop(columns=['BIA-BIA_BMR', 'BIA-BIA_TBW', 'BIA-BIA_ECW', 'BIA-BIA_LDM', 'BIA-BIA_ICW', 'BIA-BIA_SMM', 'BIA-BIA_DEE', 'BIA-BIA_LST', 'BIA-BIA_BMC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing Nonsensical Values and Outliers**\n",
    "\n",
    "This section focuses on removing \"bad\" data. We'll only do this for \"Float\" variables, so we'll begin by making a list for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float columns: Index(['CGAS-CGAS_Score', 'Physical-BMI', 'Physical-Height', 'Physical-Weight',\n",
      "       'Physical-Waist_Circumference', 'Physical-Diastolic_BP',\n",
      "       'Physical-HeartRate', 'Physical-Systolic_BP',\n",
      "       'Fitness_Endurance-Max_Stage', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone',\n",
      "       'FGC-FGC_GSND', 'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone',\n",
      "       'FGC-FGC_PU', 'FGC-FGC_PU_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone',\n",
      "       'BIA-BIA_Activity_Level_num', 'BIA-BIA_FFM', 'BIA-BIA_FFMI',\n",
      "       'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num', 'PCIAT-PCIAT_01',\n",
      "       'PCIAT-PCIAT_02', 'PCIAT-PCIAT_03', 'PCIAT-PCIAT_04', 'PCIAT-PCIAT_05',\n",
      "       'PCIAT-PCIAT_06', 'PCIAT-PCIAT_07', 'PCIAT-PCIAT_08', 'PCIAT-PCIAT_09',\n",
      "       'PCIAT-PCIAT_10', 'PCIAT-PCIAT_11', 'PCIAT-PCIAT_12', 'PCIAT-PCIAT_13',\n",
      "       'PCIAT-PCIAT_14', 'PCIAT-PCIAT_15', 'PCIAT-PCIAT_16', 'PCIAT-PCIAT_17',\n",
      "       'PCIAT-PCIAT_18', 'PCIAT-PCIAT_19', 'PCIAT-PCIAT_20',\n",
      "       'PCIAT-PCIAT_Total', 'SDS-SDS_Total_Raw',\n",
      "       'PreInt_EduHx-computerinternet_hoursday', 'sii',\n",
      "       'ENMO_Avg_Active_Days_MVPA192', 'ENMO_Avg_Active_Days_MVPA110',\n",
      "       'Positive_Anglez_Active_Days', 'FGC-FGC_SR', 'FGC-FGC_SR_Zone',\n",
      "       'PAQ_Total', 'PAQ_MVPA', 'Fitness_Endurance_Total_Time_Sec'],\n",
      "      dtype='object')\n",
      "57\n"
     ]
    }
   ],
   "source": [
    "#I only want to include quantitative variables for possible analysis for outliers, so first I identify the float variables.\n",
    "\n",
    "float_columns = train.select_dtypes(include=['float']).columns\n",
    "print(\"Float columns:\", float_columns)\n",
    "print(len(float_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All of the Physical- variables should have non-zero values (this may not be the case for some other variables). \n",
    "# Negative values will be taken care of later, but we'll replace the zeroes here:\n",
    "\n",
    "# For each variable that starts with 'Physical-' replace any values that are 0 with NaN\n",
    "for column in train.columns:\n",
    "    if column.startswith('Physical-'):\n",
    "        train[column] = train[column].replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[515, 1541, 1542, 3080, 2061, 2078, 1054, 1058, 1067, 3120, 1076, 573, 2116, 1092, 3142, 73, 76, 78, 3150, 1621, 92, 1119, 2656, 610, 2658, 101, 107, 2171, 1663, 127, 1671, 2185, 657, 1683, 2720, 2723, 1188, 1205, 1214, 2249, 1234, 2265, 2276, 1766, 2287, 2799, 2804, 2296, 1274, 1791, 1287, 777, 786, 793, 797, 2853, 1320, 1836, 2862, 2350, 2352, 306, 308, 2870, 2875, 2363, 2366, 2881, 329, 1354, 1867, 2387, 1893, 872, 2412, 371, 1399, 2425, 2954, 2446, 1427, 2966, 411, 1441, 1955, 2486, 962, 3012, 2518, 2525, 992, 997, 3047, 488, 498, 2042]\n",
      "96\n"
     ]
    }
   ],
   "source": [
    "#This generates a list of indices for extreme outliers or negative values that should not be negative. \n",
    "#In particular, BIA_BIA_Fat, which measures body fat percentage, has a bunch of negative values.\n",
    "threshold=5\n",
    "index_set=[]\n",
    "for i in range(len(float_columns)):\n",
    "    #print(float_columns[i])\n",
    "    z=np.abs(stats.zscore(train[[float_columns[i]]],nan_policy='omit'))\n",
    "    indices_large_z = np.where(np.all(z > threshold, axis=1))[0].flatten().tolist()\n",
    "    indices_negative=np.where(np.all(train[[float_columns[i]]]<0, axis=1))[0].flatten().tolist()\n",
    "    index_set=index_set+indices_large_z+indices_negative\n",
    "    #print(indices_large_z)\n",
    "    #print(indices_negative)\n",
    "    i+=1\n",
    "\n",
    "index_set_neg_largez=list(set(index_set))\n",
    "print(index_set_neg_largez)\n",
    "print(len(index_set_neg_largez))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sii\n",
      "0.0    38\n",
      "1.0    22\n",
      "2.0    21\n",
      "3.0     5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#This defines a new dataframe with the the entire case removed, where there is an extreme outlier or a nonsensical negative value.\n",
    "train_no_outliers_no_negatives=train.drop(index_set_neg_largez)\n",
    "\n",
    "#Here's a quick look at the effect of this cleaning. The first entry shows how many rows have been affected in each sii category. \n",
    "\n",
    "print(train['sii'].value_counts()-train_no_outliers_no_negatives['sii'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This takes extreme outliers and nonsensical negative values and replaces them with NaN.\n",
    "\n",
    "train_outliers_neg_to_NaN=train\n",
    "for i in range(len(float_columns)):\n",
    "    index_set=[]\n",
    "    z=np.abs(stats.zscore(train[[float_columns[i]]],nan_policy='omit'))\n",
    "    indices_large_z = np.where(np.all(z > threshold, axis=1))[0].flatten().tolist()\n",
    "    indices_negative=np.where(np.all(train[[float_columns[i]]]<0, axis=1))[0].flatten().tolist()\n",
    "    index_set=indices_large_z+indices_negative\n",
    "    #print(len(index_set))\n",
    "    for j in index_set:\n",
    "        train_outliers_neg_to_NaN.at[j, float_columns[i]] = np.nan\n",
    "        j+=1\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3168 entries, 0 to 3167\n",
      "Data columns (total 84 columns):\n",
      " #   Column                                  Non-Null Count  Dtype  \n",
      "---  ------                                  --------------  -----  \n",
      " 0   id                                      3168 non-null   object \n",
      " 1   Basic_Demos-Enroll_Season               3168 non-null   object \n",
      " 2   Basic_Demos-Age                         3168 non-null   int64  \n",
      " 3   Basic_Demos-Sex                         3168 non-null   int64  \n",
      " 4   CGAS-Season                             2065 non-null   object \n",
      " 5   CGAS-CGAS_Score                         1950 non-null   float64\n",
      " 6   Physical-Season                         2642 non-null   object \n",
      " 7   Physical-BMI                            2391 non-null   float64\n",
      " 8   Physical-Height                         2404 non-null   float64\n",
      " 9   Physical-Weight                         2396 non-null   float64\n",
      " 10  Physical-Waist_Circumference            695 non-null    float64\n",
      " 11  Physical-Diastolic_BP                   2343 non-null   float64\n",
      " 12  Physical-HeartRate                      2357 non-null   float64\n",
      " 13  Physical-Systolic_BP                    2345 non-null   float64\n",
      " 14  Fitness_Endurance-Season                1060 non-null   object \n",
      " 15  Fitness_Endurance-Max_Stage             607 non-null    float64\n",
      " 16  FGC-Season                              2674 non-null   object \n",
      " 17  FGC-FGC_CU                              1846 non-null   float64\n",
      " 18  FGC-FGC_CU_Zone                         1822 non-null   float64\n",
      " 19  FGC-FGC_GSND                            846 non-null    float64\n",
      " 20  FGC-FGC_GSND_Zone                       840 non-null    float64\n",
      " 21  FGC-FGC_GSD                             847 non-null    float64\n",
      " 22  FGC-FGC_GSD_Zone                        839 non-null    float64\n",
      " 23  FGC-FGC_PU                              1842 non-null   float64\n",
      " 24  FGC-FGC_PU_Zone                         1814 non-null   float64\n",
      " 25  FGC-FGC_SRL                             1842 non-null   float64\n",
      " 26  FGC-FGC_SRL_Zone                        1811 non-null   float64\n",
      " 27  FGC-FGC_SRR                             1844 non-null   float64\n",
      " 28  FGC-FGC_SRR_Zone                        1813 non-null   float64\n",
      " 29  FGC-FGC_TL                              1855 non-null   float64\n",
      " 30  FGC-FGC_TL_Zone                         1823 non-null   float64\n",
      " 31  BIA-Season                              1712 non-null   object \n",
      " 32  BIA-BIA_Activity_Level_num              1593 non-null   float64\n",
      " 33  BIA-BIA_BMC                             1567 non-null   float64\n",
      " 34  BIA-BIA_BMI                             1589 non-null   float64\n",
      " 35  BIA-BIA_BMR                             1592 non-null   float64\n",
      " 36  BIA-BIA_DEE                             1592 non-null   float64\n",
      " 37  BIA-BIA_ECW                             1592 non-null   float64\n",
      " 38  BIA-BIA_FFM                             1592 non-null   float64\n",
      " 39  BIA-BIA_FFMI                            1586 non-null   float64\n",
      " 40  BIA-BIA_FMI                             1564 non-null   float64\n",
      " 41  BIA-BIA_Fat                             1566 non-null   float64\n",
      " 42  BIA-BIA_Frame_num                       1593 non-null   float64\n",
      " 43  BIA-BIA_ICW                             1592 non-null   float64\n",
      " 44  BIA-BIA_LDM                             1592 non-null   float64\n",
      " 45  BIA-BIA_LST                             1592 non-null   float64\n",
      " 46  BIA-BIA_SMM                             1592 non-null   float64\n",
      " 47  BIA-BIA_TBW                             1592 non-null   float64\n",
      " 48  PCIAT-Season                            2195 non-null   object \n",
      " 49  PCIAT-PCIAT_01                          2192 non-null   float64\n",
      " 50  PCIAT-PCIAT_02                          2193 non-null   float64\n",
      " 51  PCIAT-PCIAT_03                          2190 non-null   float64\n",
      " 52  PCIAT-PCIAT_04                          2191 non-null   float64\n",
      " 53  PCIAT-PCIAT_05                          2190 non-null   float64\n",
      " 54  PCIAT-PCIAT_06                          2192 non-null   float64\n",
      " 55  PCIAT-PCIAT_07                          2190 non-null   float64\n",
      " 56  PCIAT-PCIAT_08                          2190 non-null   float64\n",
      " 57  PCIAT-PCIAT_09                          2189 non-null   float64\n",
      " 58  PCIAT-PCIAT_10                          2192 non-null   float64\n",
      " 59  PCIAT-PCIAT_11                          2193 non-null   float64\n",
      " 60  PCIAT-PCIAT_12                          2178 non-null   float64\n",
      " 61  PCIAT-PCIAT_13                          2189 non-null   float64\n",
      " 62  PCIAT-PCIAT_14                          2191 non-null   float64\n",
      " 63  PCIAT-PCIAT_15                          2190 non-null   float64\n",
      " 64  PCIAT-PCIAT_16                          2188 non-null   float64\n",
      " 65  PCIAT-PCIAT_17                          2185 non-null   float64\n",
      " 66  PCIAT-PCIAT_18                          2187 non-null   float64\n",
      " 67  PCIAT-PCIAT_19                          2190 non-null   float64\n",
      " 68  PCIAT-PCIAT_20                          2192 non-null   float64\n",
      " 69  PCIAT-PCIAT_Total                       2195 non-null   float64\n",
      " 70  SDS-Season                              2103 non-null   object \n",
      " 71  SDS-SDS_Total_Raw                       2094 non-null   float64\n",
      " 72  SDS-SDS_Total_T                         2092 non-null   float64\n",
      " 73  PreInt_EduHx-Season                     2823 non-null   object \n",
      " 74  PreInt_EduHx-computerinternet_hoursday  2633 non-null   float64\n",
      " 75  sii                                     2195 non-null   float64\n",
      " 76  ENMO_Avg_Active_Days_MVPA192            796 non-null    float64\n",
      " 77  ENMO_Avg_Active_Days_MVPA110            796 non-null    float64\n",
      " 78  Positive_Anglez_Active_Days             796 non-null    float64\n",
      " 79  FitnessGram_Zone_Total                  1801 non-null   float64\n",
      " 80  PAQ_Total                               1754 non-null   float64\n",
      " 81  PAQ_Season                              1754 non-null   object \n",
      " 82  PAQ_MVPA                                1754 non-null   float64\n",
      " 83  Fitness_Endurance_Total_Time_Sec        606 non-null    float64\n",
      "dtypes: float64(71), int64(2), object(11)\n",
      "memory usage: 2.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Now I replace the original train dataframe with the new cleaned data frame (where I've replaced bad values with NaN.)\n",
    "\n",
    "train=train_outliers_neg_to_NaN\n",
    "print(train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I'll export the dataframe to a csv.\n",
    "\n",
    "train_cleaned=train\n",
    "train_cleaned.to_csv('train_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I am going to impute input variables. \n",
    "I'm doing this before I remove the cases for which we can't compute sii scores, so that we have all data available.\n",
    "I am doing this in groups: For example, I will use only physical data to impute physical data values. This seems reasonable to do, although perhaps we might get more accurate results if we used more variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Because we will be using multiple imputation strategies, \n",
    "# I am going to define a new dataframe that will record all of the imputations using KNN.\n",
    "\n",
    "train_imp_KNN=train_cleaned.copy()\n",
    "\n",
    "# define a pipe that first scales the variables and then does a KNN imputation. \n",
    "# Note that when there is a case with no values at all, KNNImputer replaces fills in each variable with the group average.\n",
    "\n",
    "Number_Neighbors=5\n",
    "impute_pipe = Pipeline([('scale', StandardScaler()),\n",
    "                 ('KNN_impute', KNNImputer(n_neighbors=Number_Neighbors, weights='uniform', metric='nan_euclidean'))])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Basic_nan_count\n",
       "0    3168\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We have complete information for the basic demographics variables, age and gender.\n",
    "\n",
    "Basic_Demos = [col for col in train_imp_KNN.columns if 'Basic' in col]\n",
    "Basic_Demos.remove('Basic_Demos-Enroll_Season')\n",
    "train_imp_KNN['Basic_nan_count'] = train_imp_KNN[Basic_Demos].isna().sum(axis=1)\n",
    "train_imp_KNN['Basic_nan_count'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical_nan_count\n",
      "1    1663\n",
      "7     716\n",
      "0     664\n",
      "6      45\n",
      "4      30\n",
      "3      27\n",
      "2      22\n",
      "5       1\n",
      "Name: count, dtype: int64\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "#Next we'll consider the physical variables. There are many missing values here, including 688 cases with no values at all. We will do imputation.\n",
    "#Because age and gender are likely to be related to the Physical variables, I add these to the mix for imputation.\n",
    "#Note also that I have removed the season variable. I did this because it is not quantitative, so I can't easily run the imputation using this variable. \n",
    "#But this might be something to go back to later.\n",
    "\n",
    "Physical = [col for col in train_imp_KNN.columns if 'Physical' in col]\n",
    "Physical.remove('Physical-Season')\n",
    "Physical=Physical+Basic_Demos\n",
    "train_imp_KNN['Physical_nan_count'] = train_imp_KNN[Physical].isna().sum(axis=1)\n",
    "print(train_imp_KNN['Physical_nan_count'].value_counts())\n",
    "print(len(Physical))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3168 entries, 0 to 3167\n",
      "Data columns (total 9 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   Physical-BMI                  3168 non-null   float64\n",
      " 1   Physical-Height               3168 non-null   float64\n",
      " 2   Physical-Weight               3168 non-null   float64\n",
      " 3   Physical-Waist_Circumference  3168 non-null   float64\n",
      " 4   Physical-Diastolic_BP         3168 non-null   float64\n",
      " 5   Physical-HeartRate            3168 non-null   float64\n",
      " 6   Physical-Systolic_BP          3168 non-null   float64\n",
      " 7   Basic_Demos-Age               3168 non-null   float64\n",
      " 8   Basic_Demos-Sex               3168 non-null   float64\n",
      "dtypes: float64(9)\n",
      "memory usage: 222.9 KB\n"
     ]
    }
   ],
   "source": [
    "#Now I will impute values for these variables. First I'll define a new dataframe to work on.\n",
    "\n",
    "df=train_imp_KNN[Physical]\n",
    "\n",
    "#Now I run the impute pipe on this dataframe. First I fit the pipe to the data. I record the transform of the dataframe as imputation. \n",
    "# Imputation is a numpy array, so it needs to be converted back to a pandas dataframe.\n",
    "#Also, I reverse-transformed the data. My reasoning for doing this is that we want it in terms of the original scale to be able to make sense of things. \n",
    "#But since we are scaling twice, more rounding issues arise.\n",
    "\n",
    "impute_pipe.fit(df)\n",
    "\n",
    "imputation_physical=impute_pipe.transform(df)\n",
    "imputation_physical=impute_pipe.named_steps['scale'].inverse_transform(imputation_physical)\n",
    "df2 = pd.DataFrame(imputation_physical, columns=Physical)\n",
    "df2.info()\n",
    "\n",
    "#Lastly, I replace the original values in the dataframe with the newly imputed values.\n",
    "\n",
    "train_imp_KNN[Physical]=train_imp_KNN[Physical].fillna(df2[Physical])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitness_nan_count\n",
      "17    1306\n",
      "3      649\n",
      "7      549\n",
      "4      433\n",
      "0      152\n",
      "5       24\n",
      "9       18\n",
      "8       10\n",
      "11       7\n",
      "12       6\n",
      "2        5\n",
      "14       3\n",
      "6        2\n",
      "13       2\n",
      "16       1\n",
      "15       1\n",
      "Name: count, dtype: int64\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "#Next we'll consider the fitness test variables. \n",
    "# There are many missing values here, although it looks like we have at least some values for every case.\n",
    "#I kept in all of the zone variables, which means they have the same weight as the actual measurements. It seems like I shouldn't do this.\n",
    "\n",
    "Fitness = [col for col in train_imp_KNN.columns if 'Fitness' in col]+[col for col in train_imp_KNN.columns if 'FGC' in col]\n",
    "Fitness.remove('Fitness_Endurance-Season')\n",
    "Fitness.remove('FGC-Season')\n",
    "Fitness=Fitness+Basic_Demos\n",
    "train_imp_KNN['Fitness_nan_count'] = train_imp_KNN[Fitness].isna().sum(axis=1)\n",
    "print(train_imp_KNN['Fitness_nan_count'].value_counts())\n",
    "print(len(Fitness))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3168 entries, 0 to 3167\n",
      "Data columns (total 19 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   Fitness_Endurance-Max_Stage  3168 non-null   float64\n",
      " 1   Fitness_Endurance-Time_Mins  3168 non-null   float64\n",
      " 2   Fitness_Endurance-Time_Sec   3168 non-null   float64\n",
      " 3   FGC-FGC_CU                   3168 non-null   float64\n",
      " 4   FGC-FGC_CU_Zone              3168 non-null   float64\n",
      " 5   FGC-FGC_GSND                 3168 non-null   float64\n",
      " 6   FGC-FGC_GSND_Zone            3168 non-null   float64\n",
      " 7   FGC-FGC_GSD                  3168 non-null   float64\n",
      " 8   FGC-FGC_GSD_Zone             3168 non-null   float64\n",
      " 9   FGC-FGC_PU                   3168 non-null   float64\n",
      " 10  FGC-FGC_PU_Zone              3168 non-null   float64\n",
      " 11  FGC-FGC_SRL                  3168 non-null   float64\n",
      " 12  FGC-FGC_SRL_Zone             3168 non-null   float64\n",
      " 13  FGC-FGC_SRR                  3168 non-null   float64\n",
      " 14  FGC-FGC_SRR_Zone             3168 non-null   float64\n",
      " 15  FGC-FGC_TL                   3168 non-null   float64\n",
      " 16  FGC-FGC_TL_Zone              3168 non-null   float64\n",
      " 17  Basic_Demos-Age              3168 non-null   float64\n",
      " 18  Basic_Demos-Sex              3168 non-null   float64\n",
      "dtypes: float64(19)\n",
      "memory usage: 470.4 KB\n"
     ]
    }
   ],
   "source": [
    "#Now I will impute values for these variables. First I'll define a new dataframe to work on.\n",
    "\n",
    "df=train_imp_KNN[Fitness]\n",
    "\n",
    "#Now I run the impute pipe on this dataframe. First I fit the pipe to the data. I record the transform of the dataframe as imputation. \n",
    "# Imputation is a numpy array, so it needs to be converted back to a pandas dataframe.\n",
    "\n",
    "impute_pipe.fit(df)\n",
    "\n",
    "imputation_fitness=impute_pipe.transform(df)\n",
    "imputation_fitness=impute_pipe.named_steps['scale'].inverse_transform(imputation_fitness)\n",
    "df2 = pd.DataFrame(imputation_fitness, columns=Fitness)\n",
    "df2.info()\n",
    "\n",
    "#Lastly, I replace the original values in the dataframe with the newly imputed values.\n",
    "\n",
    "train_imp_KNN[Fitness]=train_imp_KNN[Fitness].fillna(df2[Fitness])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIA_nan_count\n",
      "16    1575\n",
      "0     1527\n",
      "1       30\n",
      "2       21\n",
      "12       7\n",
      "3        5\n",
      "4        2\n",
      "5        1\n",
      "Name: count, dtype: int64\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "#Next we'll consider the BIA variables. \n",
    "\n",
    "BIA = [col for col in train_imp_KNN.columns if 'BIA' in col]\n",
    "BIA.remove('BIA-Season')\n",
    "BIA=BIA+Basic_Demos\n",
    "train_imp_KNN['BIA_nan_count'] = train_imp_KNN[BIA].isna().sum(axis=1)\n",
    "print(train_imp_KNN['BIA_nan_count'].value_counts())\n",
    "print(len(BIA))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3168 entries, 0 to 3167\n",
      "Data columns (total 18 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   BIA-BIA_Activity_Level_num  3168 non-null   float64\n",
      " 1   BIA-BIA_BMC                 3168 non-null   float64\n",
      " 2   BIA-BIA_BMI                 3168 non-null   float64\n",
      " 3   BIA-BIA_BMR                 3168 non-null   float64\n",
      " 4   BIA-BIA_DEE                 3168 non-null   float64\n",
      " 5   BIA-BIA_ECW                 3168 non-null   float64\n",
      " 6   BIA-BIA_FFM                 3168 non-null   float64\n",
      " 7   BIA-BIA_FFMI                3168 non-null   float64\n",
      " 8   BIA-BIA_FMI                 3168 non-null   float64\n",
      " 9   BIA-BIA_Fat                 3168 non-null   float64\n",
      " 10  BIA-BIA_Frame_num           3168 non-null   float64\n",
      " 11  BIA-BIA_ICW                 3168 non-null   float64\n",
      " 12  BIA-BIA_LDM                 3168 non-null   float64\n",
      " 13  BIA-BIA_LST                 3168 non-null   float64\n",
      " 14  BIA-BIA_SMM                 3168 non-null   float64\n",
      " 15  BIA-BIA_TBW                 3168 non-null   float64\n",
      " 16  Basic_Demos-Age             3168 non-null   float64\n",
      " 17  Basic_Demos-Sex             3168 non-null   float64\n",
      "dtypes: float64(18)\n",
      "memory usage: 445.6 KB\n"
     ]
    }
   ],
   "source": [
    "#Now I will impute values for these variables. First I'll define a new dataframe to work on.\n",
    "\n",
    "df=train_imp_KNN[BIA]\n",
    "\n",
    "#Now I run the impute pipe on this dataframe. First I fit the pipe to the data. I record the transform of the dataframe as imputation. \n",
    "# Imputation is a numpy array, so it needs to be converted back to a pandas dataframe.\n",
    "\n",
    "impute_pipe.fit(df)\n",
    "\n",
    "imputation_BIA=impute_pipe.transform(df)\n",
    "imputation_BIA=impute_pipe.named_steps['scale'].inverse_transform(imputation_BIA)\n",
    "df2 = pd.DataFrame(imputation_BIA, columns=BIA)\n",
    "df2.info()\n",
    "\n",
    "#Lastly, I replace the original values in the dataframe with the newly imputed values.\n",
    "\n",
    "train_imp_KNN[BIA]=train_imp_KNN[BIA].fillna(df2[BIA])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CGAS_nan_count\n",
      "0    1950\n",
      "1    1218\n",
      "Name: count, dtype: int64\n",
      "3\n",
      "['CGAS-CGAS_Score', 'Basic_Demos-Age', 'Basic_Demos-Sex']\n"
     ]
    }
   ],
   "source": [
    "#Next we consider CGAS (Children's Global Assessment Score). This measure comes from an evaluation by a trained professional. \n",
    "#Looking at the description, it seems reasonable that it is related to gender and age, so I am going to do KNN with those variables. \n",
    "\n",
    "CGAS = ['CGAS-CGAS_Score']+Basic_Demos\n",
    "train_imp_KNN['CGAS_nan_count'] = train_imp_KNN[CGAS].isna().sum(axis=1)\n",
    "print(train_imp_KNN['CGAS_nan_count'].value_counts())\n",
    "print(len(CGAS))\n",
    "print(CGAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3168 entries, 0 to 3167\n",
      "Data columns (total 3 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   CGAS-CGAS_Score  3168 non-null   float64\n",
      " 1   Basic_Demos-Age  3168 non-null   float64\n",
      " 2   Basic_Demos-Sex  3168 non-null   float64\n",
      "dtypes: float64(3)\n",
      "memory usage: 74.4 KB\n"
     ]
    }
   ],
   "source": [
    "#Now I will impute values for this variable. First I'll define a new dataframe to work on.\n",
    "\n",
    "df=train_imp_KNN[CGAS]\n",
    "\n",
    "#Now I run the impute pipe on this dataframe. First I fit the pipe to the data. I record the transform of the dataframe as imputation. \n",
    "# Imputation is a numpy array, so it needs to be converted back to a pandas dataframe.\n",
    "\n",
    "impute_pipe.fit(df)\n",
    "\n",
    "imputation_CGAS=impute_pipe.transform(df)\n",
    "imputation_CGAS=impute_pipe.named_steps['scale'].inverse_transform(imputation_CGAS)\n",
    "df2 = pd.DataFrame(imputation_CGAS, columns=CGAS)\n",
    "df2.info()\n",
    "\n",
    "#Lastly, I replace the original values in the dataframe with the newly imputed values.\n",
    "\n",
    "train_imp_KNN[CGAS]=train_imp_KNN[CGAS].fillna(df2[CGAS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IntHrs_nan_count\n",
      "0    2633\n",
      "1     535\n",
      "Name: count, dtype: int64\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "#Next we consider  PreInt_EduHx-computerinternet_hoursday. \n",
    "#It seems reasonable that it is related to gender and age, so I am going to do KNN with those variables. \n",
    "\n",
    "IntHrs = ['PreInt_EduHx-computerinternet_hoursday']+Basic_Demos\n",
    "train_imp_KNN['IntHrs_nan_count'] = train_imp_KNN[IntHrs].isna().sum(axis=1)\n",
    "print(train_imp_KNN['IntHrs_nan_count'].value_counts())\n",
    "print(len(IntHrs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3168 entries, 0 to 3167\n",
      "Data columns (total 3 columns):\n",
      " #   Column                                  Non-Null Count  Dtype  \n",
      "---  ------                                  --------------  -----  \n",
      " 0   PreInt_EduHx-computerinternet_hoursday  3168 non-null   float64\n",
      " 1   Basic_Demos-Age                         3168 non-null   float64\n",
      " 2   Basic_Demos-Sex                         3168 non-null   float64\n",
      "dtypes: float64(3)\n",
      "memory usage: 74.4 KB\n"
     ]
    }
   ],
   "source": [
    "#Now I will impute values for this variable. First I'll define a new dataframe to work on.\n",
    "\n",
    "df=train_imp_KNN[IntHrs]\n",
    "\n",
    "#Now I run the impute pipe on this dataframe. First I fit the pipe to the data. I record the transform of the dataframe as imputation. \n",
    "# Imputation is a numpy array, so it needs to be converted back to a pandas dataframe.\n",
    "\n",
    "impute_pipe.fit(df)\n",
    "\n",
    "imputation_IntHrs=impute_pipe.transform(df)\n",
    "imputation_IntHrs=impute_pipe.named_steps['scale'].inverse_transform(imputation_IntHrs)\n",
    "df2 = pd.DataFrame(imputation_IntHrs, columns=IntHrs)\n",
    "df2.info()\n",
    "\n",
    "#Lastly, I replace the original values in the dataframe with the newly imputed values.\n",
    "\n",
    "train_imp_KNN[IntHrs]=train_imp_KNN[IntHrs].fillna(df2[IntHrs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of the file focus on imputing values for the PCIAT questionaire, based on other responses to the questionaire, using k nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN rates in original data:  pciatsnotna_sum\n",
      "20    2130\n",
      "0      974\n",
      "19      52\n",
      "18       9\n",
      "10       1\n",
      "15       1\n",
      "17       1\n",
      "Name: count, dtype: int64\n",
      "NaN rates after the non-responders have been removed: pciatsnotna_sum\n",
      "20    2130\n",
      "19      52\n",
      "18       9\n",
      "10       1\n",
      "15       1\n",
      "17       1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#First we identify the columns of interest to search for NaN variables.\n",
    "\n",
    "pciats = [col for col in train_imp_KNN.columns if 'PCIAT' in col]\n",
    "pciats.remove('PCIAT-Season')\n",
    "pciats.remove('PCIAT-PCIAT_Total')\n",
    "\n",
    "# Create 20 new variables that indicate whether or not PCIAT-PCIAT_01 through PCIAT-PCIAT_20 were NaN\n",
    "\n",
    "for pciat in pciats:\n",
    "    train_imp_KNN[pciat + '_isnotna'] = train_imp_KNN[pciat].notna().astype(int)\n",
    "\n",
    "    \n",
    "\n",
    "#Create new variable that represents the sum of the questions answered.\n",
    "\n",
    "pciatsnotna = [col for col in train_imp_KNN.columns if 'isnotna' in col]\n",
    "train_imp_KNN['pciatsnotna_sum'] = train_imp_KNN[pciatsnotna].sum(axis=1)\n",
    "\n",
    "#Here, we can see NaN rates. Note that there are almost 1000 cases where the participants did not respond to any PCIAT questions. \n",
    "#These participants are eliminated. Note that when rows are eliminated, the original row indexing is preserved; therefore, I reset the index. \n",
    "#(This step is important to make sure indices match up when I later replace NaN entries with their imputed values.)\n",
    "\n",
    "print(\"NaN rates in original data: \",train_imp_KNN['pciatsnotna_sum'].value_counts())\n",
    "train_imp_KNN = train_imp_KNN[train_imp_KNN['pciatsnotna_sum'] != 0]\n",
    "train_imp_KNN.reset_index(drop=True, inplace=True)\n",
    "print(\"NaN rates after the non-responders have been removed:\",train_imp_KNN['pciatsnotna_sum'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next we use KNN to impute the missing values.\n",
    "\n",
    "\n",
    "# define imputer\n",
    "Number_Neighbors=5\n",
    "imputer = KNNImputer(n_neighbors=Number_Neighbors, weights='uniform', metric='nan_euclidean')\n",
    "\n",
    "#The imputer.fit_transform function outputs a numpy array. So first I do the fitting, then convert the output back to a pandas dataframe.\n",
    "\n",
    "imputations=imputer.fit_transform(train_imp_KNN[pciats])\n",
    "df2 = pd.DataFrame(imputations, columns=pciats)\n",
    "\n",
    "#Next take the result and insert into the original dataframe. \n",
    "\n",
    "train_imp_KNN[pciats]=train_imp_KNN[pciats].fillna(df2[pciats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2194 entries, 0 to 2193\n",
      "Data columns (total 90 columns):\n",
      " #   Column                                  Non-Null Count  Dtype   \n",
      "---  ------                                  --------------  -----   \n",
      " 0   id                                      2194 non-null   object  \n",
      " 1   Basic_Demos-Enroll_Season               2194 non-null   object  \n",
      " 2   Basic_Demos-Age                         2194 non-null   int64   \n",
      " 3   Basic_Demos-Sex                         2194 non-null   int64   \n",
      " 4   CGAS-Season                             1893 non-null   object  \n",
      " 5   CGAS-CGAS_Score                         2194 non-null   float64 \n",
      " 6   Physical-Season                         2074 non-null   object  \n",
      " 7   Physical-BMI                            2194 non-null   float64 \n",
      " 8   Physical-Height                         2194 non-null   float64 \n",
      " 9   Physical-Weight                         2194 non-null   float64 \n",
      " 10  Physical-Waist_Circumference            2194 non-null   float64 \n",
      " 11  Physical-Diastolic_BP                   2194 non-null   float64 \n",
      " 12  Physical-HeartRate                      2194 non-null   float64 \n",
      " 13  Physical-Systolic_BP                    2194 non-null   float64 \n",
      " 14  Fitness_Endurance-Season                1019 non-null   object  \n",
      " 15  Fitness_Endurance-Max_Stage             2194 non-null   float64 \n",
      " 16  Fitness_Endurance-Time_Mins             2194 non-null   float64 \n",
      " 17  Fitness_Endurance-Time_Sec              2194 non-null   float64 \n",
      " 18  FGC-Season                              2122 non-null   object  \n",
      " 19  FGC-FGC_CU                              2194 non-null   float64 \n",
      " 20  FGC-FGC_CU_Zone                         2194 non-null   float64 \n",
      " 21  FGC-FGC_GSND                            2194 non-null   float64 \n",
      " 22  FGC-FGC_GSND_Zone                       2194 non-null   float64 \n",
      " 23  FGC-FGC_GSD                             2194 non-null   float64 \n",
      " 24  FGC-FGC_GSD_Zone                        2194 non-null   float64 \n",
      " 25  FGC-FGC_PU                              2194 non-null   float64 \n",
      " 26  FGC-FGC_PU_Zone                         2194 non-null   float64 \n",
      " 27  FGC-FGC_SRL                             2194 non-null   float64 \n",
      " 28  FGC-FGC_SRL_Zone                        2194 non-null   float64 \n",
      " 29  FGC-FGC_SRR                             2194 non-null   float64 \n",
      " 30  FGC-FGC_SRR_Zone                        2194 non-null   float64 \n",
      " 31  FGC-FGC_TL                              2194 non-null   float64 \n",
      " 32  FGC-FGC_TL_Zone                         2194 non-null   float64 \n",
      " 33  BIA-Season                              1475 non-null   object  \n",
      " 34  BIA-BIA_Activity_Level_num              2194 non-null   float64 \n",
      " 35  BIA-BIA_BMC                             2194 non-null   float64 \n",
      " 36  BIA-BIA_BMI                             2194 non-null   float64 \n",
      " 37  BIA-BIA_BMR                             2194 non-null   float64 \n",
      " 38  BIA-BIA_DEE                             2194 non-null   float64 \n",
      " 39  BIA-BIA_ECW                             2194 non-null   float64 \n",
      " 40  BIA-BIA_FFM                             2194 non-null   float64 \n",
      " 41  BIA-BIA_FFMI                            2194 non-null   float64 \n",
      " 42  BIA-BIA_FMI                             2194 non-null   float64 \n",
      " 43  BIA-BIA_Fat                             2194 non-null   float64 \n",
      " 44  BIA-BIA_Frame_num                       2194 non-null   float64 \n",
      " 45  BIA-BIA_ICW                             2194 non-null   float64 \n",
      " 46  BIA-BIA_LDM                             2194 non-null   float64 \n",
      " 47  BIA-BIA_LST                             2194 non-null   float64 \n",
      " 48  BIA-BIA_SMM                             2194 non-null   float64 \n",
      " 49  BIA-BIA_TBW                             2194 non-null   float64 \n",
      " 50  PAQ_A-Season                            289 non-null    object  \n",
      " 51  PAQ_A-PAQ_A_Total                       289 non-null    float64 \n",
      " 52  PAQ_C-Season                            1153 non-null   object  \n",
      " 53  PAQ_C-PAQ_C_Total                       1153 non-null   float64 \n",
      " 54  PCIAT-Season                            2194 non-null   object  \n",
      " 55  PCIAT-PCIAT_01                          2194 non-null   float64 \n",
      " 56  PCIAT-PCIAT_02                          2194 non-null   float64 \n",
      " 57  PCIAT-PCIAT_03                          2194 non-null   float64 \n",
      " 58  PCIAT-PCIAT_04                          2194 non-null   float64 \n",
      " 59  PCIAT-PCIAT_05                          2194 non-null   float64 \n",
      " 60  PCIAT-PCIAT_06                          2194 non-null   float64 \n",
      " 61  PCIAT-PCIAT_07                          2194 non-null   float64 \n",
      " 62  PCIAT-PCIAT_08                          2194 non-null   float64 \n",
      " 63  PCIAT-PCIAT_09                          2194 non-null   float64 \n",
      " 64  PCIAT-PCIAT_10                          2194 non-null   float64 \n",
      " 65  PCIAT-PCIAT_11                          2194 non-null   float64 \n",
      " 66  PCIAT-PCIAT_12                          2194 non-null   float64 \n",
      " 67  PCIAT-PCIAT_13                          2194 non-null   float64 \n",
      " 68  PCIAT-PCIAT_14                          2194 non-null   float64 \n",
      " 69  PCIAT-PCIAT_15                          2194 non-null   float64 \n",
      " 70  PCIAT-PCIAT_16                          2194 non-null   float64 \n",
      " 71  PCIAT-PCIAT_17                          2194 non-null   float64 \n",
      " 72  PCIAT-PCIAT_18                          2194 non-null   float64 \n",
      " 73  PCIAT-PCIAT_19                          2194 non-null   float64 \n",
      " 74  PCIAT-PCIAT_20                          2194 non-null   float64 \n",
      " 75  PCIAT-PCIAT_Total                       2194 non-null   float64 \n",
      " 76  SDS-Season                              2028 non-null   object  \n",
      " 77  SDS-SDS_Total_Raw                       2027 non-null   float64 \n",
      " 78  SDS-SDS_Total_T                         2026 non-null   float64 \n",
      " 79  PreInt_EduHx-Season                     2179 non-null   object  \n",
      " 80  PreInt_EduHx-computerinternet_hoursday  2194 non-null   float64 \n",
      " 81  sii                                     2194 non-null   float64 \n",
      " 82  Basic_nan_count                         2194 non-null   int64   \n",
      " 83  Physical_nan_count                      2194 non-null   int64   \n",
      " 84  Fitness_nan_count                       2194 non-null   int64   \n",
      " 85  BIA_nan_count                           2194 non-null   int64   \n",
      " 86  CGAS_nan_count                          2194 non-null   int64   \n",
      " 87  IntHrs_nan_count                        2194 non-null   int64   \n",
      " 88  PCIAT_Total_Imputed                     2194 non-null   float64 \n",
      " 89  sii_Imputed                             2194 non-null   category\n",
      "dtypes: category(1), float64(69), int64(8), object(12)\n",
      "memory usage: 1.5+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#I recalculate the PCIAT total score. I also drop the variables that were added to detect NaN in the PCIAT data, just to tidy up a bit.\n",
    "\n",
    "train_imp_KNN['PCIAT_Total_Imputed'] = train_imp_KNN[pciats].sum(axis=1)\n",
    "train_imp_KNN = train_imp_KNN.drop(columns=pciatsnotna)\n",
    "train_imp_KNN = train_imp_KNN.drop(columns=['pciatsnotna_sum'])\n",
    "\n",
    "\n",
    "#Now we can calculate a new sii score with the imputed values. \n",
    "\n",
    "bins = [0, 30, 49,79,100]\n",
    "labels = [0,1,2,3]\n",
    "train_imp_KNN['sii_Imputed'] = pd.cut(train_imp_KNN['PCIAT_Total_Imputed'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "print(train_imp_KNN.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imp_KNN.to_csv('train_imp_KNN.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I'm working on imputation using MICE. As you'll see below, I will use MICE to impute the missing input values, provided the variables are quantitative or categorical binary or ordinal (which are treated as integers in the data). This doesn't necessarily seem like the best idea for the ordinal data. Also, imputation can't directly handle non-ordinal categorical variables with more than 2 categories. In this case, the issue only affects seasons. I could work around this, but for now I just ignored these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex',\n",
      "       'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI',\n",
      "       'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
      "       'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
      "       'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage',\n",
      "       'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n",
      "       'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n",
      "       'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n",
      "       'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n",
      "       'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season',\n",
      "       'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n",
      "       'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n",
      "       'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n",
      "       'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n",
      "       'BIA-BIA_TBW', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season',\n",
      "       'PAQ_C-PAQ_C_Total', 'PCIAT-Season', 'PCIAT-PCIAT_01', 'PCIAT-PCIAT_02',\n",
      "       'PCIAT-PCIAT_03', 'PCIAT-PCIAT_04', 'PCIAT-PCIAT_05', 'PCIAT-PCIAT_06',\n",
      "       'PCIAT-PCIAT_07', 'PCIAT-PCIAT_08', 'PCIAT-PCIAT_09', 'PCIAT-PCIAT_10',\n",
      "       'PCIAT-PCIAT_11', 'PCIAT-PCIAT_12', 'PCIAT-PCIAT_13', 'PCIAT-PCIAT_14',\n",
      "       'PCIAT-PCIAT_15', 'PCIAT-PCIAT_16', 'PCIAT-PCIAT_17', 'PCIAT-PCIAT_18',\n",
      "       'PCIAT-PCIAT_19', 'PCIAT-PCIAT_20', 'PCIAT-PCIAT_Total', 'SDS-Season',\n",
      "       'SDS-SDS_Total_Raw', 'SDS-SDS_Total_T', 'PreInt_EduHx-Season',\n",
      "       'PreInt_EduHx-computerinternet_hoursday', 'sii'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# I am going to define a new dataframe that will record all of the imputations using MICE. I only want to apply MICE to the input variables, so I separate those out.\n",
    "#Also, MICE doesn't like categorical variables. I have just removed those--the seasons--for now.\n",
    "\n",
    "train_imp_MICE=train_cleaned.copy()\n",
    "\n",
    "print(train_imp_MICE.columns)\n",
    "\n",
    "features=['Basic_Demos-Age', 'Basic_Demos-Sex',\n",
    "        'CGAS-CGAS_Score', 'Physical-BMI',\n",
    "       'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
    "       'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
    "        'Fitness_Endurance-Max_Stage',\n",
    "       'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n",
    "        'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n",
    "       'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n",
    "       'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n",
    "       'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', \n",
    "       'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n",
    "       'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n",
    "       'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n",
    "       'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n",
    "       'BIA-BIA_TBW', 'PAQ_A-PAQ_A_Total', \n",
    "       'PAQ_C-PAQ_C_Total',\n",
    "       'SDS-SDS_Total_Raw', 'SDS-SDS_Total_T', \n",
    "       'PreInt_EduHx-computerinternet_hoursday']\n",
    "\n",
    "df=train_imp_MICE[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New packages needed.\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IterativeImputer has a bunch of options, including what type of regression is used for the imputation. Here, I've just gone with the default.\n",
    "\n",
    "imputer = IterativeImputer(max_iter=10, random_state=497)\n",
    "\n",
    "df2= imputer.fit_transform(df)\n",
    "\n",
    "df3 = pd.DataFrame(df2, columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now I fill in the missing values in train_imp_MICE with the MICE-imputed values. I am still using KNN for the pciats values. \n",
    "\n",
    "train_imp_MICE[features]=train_imp_MICE[features].fillna(df3[features])\n",
    "train_imp_MICE[pciats]=train_imp_KNN[pciats]\n",
    "train_imp_MICE['PCIAT_Total_Imputed']=train_imp_KNN['PCIAT_Total_Imputed']\n",
    "train_imp_MICE['sii_Imputed']=train_imp_KNN['sii_Imputed']\n",
    "\n",
    "#Now I can export to a csv.\n",
    "\n",
    "train_imp_MICE.to_csv('train_imp_MICE.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3168 entries, 0 to 3167\n",
      "Data columns (total 84 columns):\n",
      " #   Column                                  Non-Null Count  Dtype   \n",
      "---  ------                                  --------------  -----   \n",
      " 0   id                                      3168 non-null   object  \n",
      " 1   Basic_Demos-Enroll_Season               3168 non-null   object  \n",
      " 2   Basic_Demos-Age                         3168 non-null   int64   \n",
      " 3   Basic_Demos-Sex                         3168 non-null   int64   \n",
      " 4   CGAS-Season                             2065 non-null   object  \n",
      " 5   CGAS-CGAS_Score                         3168 non-null   float64 \n",
      " 6   Physical-Season                         2642 non-null   object  \n",
      " 7   Physical-BMI                            3168 non-null   float64 \n",
      " 8   Physical-Height                         3168 non-null   float64 \n",
      " 9   Physical-Weight                         3168 non-null   float64 \n",
      " 10  Physical-Waist_Circumference            3168 non-null   float64 \n",
      " 11  Physical-Diastolic_BP                   3168 non-null   float64 \n",
      " 12  Physical-HeartRate                      3168 non-null   float64 \n",
      " 13  Physical-Systolic_BP                    3168 non-null   float64 \n",
      " 14  Fitness_Endurance-Season                1060 non-null   object  \n",
      " 15  Fitness_Endurance-Max_Stage             3168 non-null   float64 \n",
      " 16  Fitness_Endurance-Time_Mins             3168 non-null   float64 \n",
      " 17  Fitness_Endurance-Time_Sec              3168 non-null   float64 \n",
      " 18  FGC-Season                              2674 non-null   object  \n",
      " 19  FGC-FGC_CU                              3168 non-null   float64 \n",
      " 20  FGC-FGC_CU_Zone                         3168 non-null   float64 \n",
      " 21  FGC-FGC_GSND                            3168 non-null   float64 \n",
      " 22  FGC-FGC_GSND_Zone                       3168 non-null   float64 \n",
      " 23  FGC-FGC_GSD                             3168 non-null   float64 \n",
      " 24  FGC-FGC_GSD_Zone                        3168 non-null   float64 \n",
      " 25  FGC-FGC_PU                              3168 non-null   float64 \n",
      " 26  FGC-FGC_PU_Zone                         3168 non-null   float64 \n",
      " 27  FGC-FGC_SRL                             3168 non-null   float64 \n",
      " 28  FGC-FGC_SRL_Zone                        3168 non-null   float64 \n",
      " 29  FGC-FGC_SRR                             3168 non-null   float64 \n",
      " 30  FGC-FGC_SRR_Zone                        3168 non-null   float64 \n",
      " 31  FGC-FGC_TL                              3168 non-null   float64 \n",
      " 32  FGC-FGC_TL_Zone                         3168 non-null   float64 \n",
      " 33  BIA-Season                              1712 non-null   object  \n",
      " 34  BIA-BIA_Activity_Level_num              3168 non-null   float64 \n",
      " 35  BIA-BIA_BMC                             3168 non-null   float64 \n",
      " 36  BIA-BIA_BMI                             3168 non-null   float64 \n",
      " 37  BIA-BIA_BMR                             3168 non-null   float64 \n",
      " 38  BIA-BIA_DEE                             3168 non-null   float64 \n",
      " 39  BIA-BIA_ECW                             3168 non-null   float64 \n",
      " 40  BIA-BIA_FFM                             3168 non-null   float64 \n",
      " 41  BIA-BIA_FFMI                            3168 non-null   float64 \n",
      " 42  BIA-BIA_FMI                             3168 non-null   float64 \n",
      " 43  BIA-BIA_Fat                             3168 non-null   float64 \n",
      " 44  BIA-BIA_Frame_num                       3168 non-null   float64 \n",
      " 45  BIA-BIA_ICW                             3168 non-null   float64 \n",
      " 46  BIA-BIA_LDM                             3168 non-null   float64 \n",
      " 47  BIA-BIA_LST                             3168 non-null   float64 \n",
      " 48  BIA-BIA_SMM                             3168 non-null   float64 \n",
      " 49  BIA-BIA_TBW                             3168 non-null   float64 \n",
      " 50  PAQ_A-Season                            382 non-null    object  \n",
      " 51  PAQ_A-PAQ_A_Total                       3168 non-null   float64 \n",
      " 52  PAQ_C-Season                            1373 non-null   object  \n",
      " 53  PAQ_C-PAQ_C_Total                       3168 non-null   float64 \n",
      " 54  PCIAT-Season                            2195 non-null   object  \n",
      " 55  PCIAT-PCIAT_01                          2194 non-null   float64 \n",
      " 56  PCIAT-PCIAT_02                          2194 non-null   float64 \n",
      " 57  PCIAT-PCIAT_03                          2194 non-null   float64 \n",
      " 58  PCIAT-PCIAT_04                          2194 non-null   float64 \n",
      " 59  PCIAT-PCIAT_05                          2194 non-null   float64 \n",
      " 60  PCIAT-PCIAT_06                          2194 non-null   float64 \n",
      " 61  PCIAT-PCIAT_07                          2194 non-null   float64 \n",
      " 62  PCIAT-PCIAT_08                          2194 non-null   float64 \n",
      " 63  PCIAT-PCIAT_09                          2194 non-null   float64 \n",
      " 64  PCIAT-PCIAT_10                          2194 non-null   float64 \n",
      " 65  PCIAT-PCIAT_11                          2194 non-null   float64 \n",
      " 66  PCIAT-PCIAT_12                          2194 non-null   float64 \n",
      " 67  PCIAT-PCIAT_13                          2194 non-null   float64 \n",
      " 68  PCIAT-PCIAT_14                          2194 non-null   float64 \n",
      " 69  PCIAT-PCIAT_15                          2194 non-null   float64 \n",
      " 70  PCIAT-PCIAT_16                          2194 non-null   float64 \n",
      " 71  PCIAT-PCIAT_17                          2194 non-null   float64 \n",
      " 72  PCIAT-PCIAT_18                          2194 non-null   float64 \n",
      " 73  PCIAT-PCIAT_19                          2194 non-null   float64 \n",
      " 74  PCIAT-PCIAT_20                          2194 non-null   float64 \n",
      " 75  PCIAT-PCIAT_Total                       2195 non-null   float64 \n",
      " 76  SDS-Season                              2103 non-null   object  \n",
      " 77  SDS-SDS_Total_Raw                       3168 non-null   float64 \n",
      " 78  SDS-SDS_Total_T                         3168 non-null   float64 \n",
      " 79  PreInt_EduHx-Season                     2823 non-null   object  \n",
      " 80  PreInt_EduHx-computerinternet_hoursday  3168 non-null   float64 \n",
      " 81  sii                                     2195 non-null   float64 \n",
      " 82  PCIAT_Total_Imputed                     2194 non-null   float64 \n",
      " 83  sii_Imputed                             2194 non-null   category\n",
      "dtypes: category(1), float64(69), int64(2), object(12)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "#We have now imputed all missing data except for seasons.\n",
    " \n",
    "train_imp_MICE.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_fall_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
