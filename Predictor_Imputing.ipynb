{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goals**\n",
    "\n",
    "The goal of this notebook is to create KNN and MICE imputation functions or pipe-able classes that we can use as part of our model generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the Data**\n",
    "\n",
    "For the purpose of developing our model(s), we'll work with data that include the imputed outcome (PCIAT_Total and/or sii) scores AND have cleaned predictors.\n",
    "\n",
    "In the final version of our code, we'll work with data with cleaned predictors but won't have any access to the outcome scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the cleaned & predictor-imputed data\n",
    "#train_cleaned=pd.read_csv('train_cleaned_predictor_imputed.csv')\n",
    "\n",
    "#Load the cleaned data\n",
    "train_cleaned=pd.read_csv('train_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using KNN to Impute Values of Predictor Variables**\n",
    "\n",
    "Our first code chunk will use a KNN algorithm with all available predictor columns, excluding the Zone and Season columns\n",
    "\n",
    "We'll start by making a list of quantitative predictor variables. Note that:\n",
    "* The Zone variables are computed from others; we'll re-compute their values after doing imputation\n",
    "* The list includes Basic_Demos-Sex. Although this is categorical, all participants have data for this variable, and it's useful for imputing other variables\n",
    "* We *could* convert the Season variables into dummy variables, but this seems like it would over-weight them for KNN imputation. So we're leaving them out.\n",
    "\n",
    "Then, we'll construct and use a KNN imputer with 5 neighbors to impute missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Because we will be using multiple imputation strategies, \n",
    "# I am going to define a new dataframe that will record all of the imputations using KNN.\n",
    "train_cleaned_knn_imputed=train_cleaned.copy()\n",
    "\n",
    "\n",
    "# Create a list of columns that doesn't include id, sii, PCIAT, Zone, or Season\n",
    "# This is written in a way to avoid exceptions in case one of the columns is missing\n",
    "feature_list = train_cleaned_knn_imputed.columns.tolist()\n",
    "if 'id' in feature_list:\n",
    "    feature_list.remove('id')\n",
    "if 'sii' in feature_list:\n",
    "    feature_list.remove('sii')\n",
    "feature_list = [x for x in feature_list if 'PCIAT' not in x]\n",
    "feature_list = [x for x in feature_list if 'Zone' not in x]\n",
    "feature_list = [x for x in feature_list if 'Season' not in x]\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# define a pipe that first scales the variables and then does a KNN imputation. \n",
    "# Note that when there is a case with no values at all, KNNImputer replaces fills in each variable with the group average.\n",
    "\n",
    "Number_Neighbors=5\n",
    "knn_impute_pipe = Pipeline([('scale', StandardScaler()),\n",
    "                 ('KNN_impute', KNNImputer(n_neighbors=Number_Neighbors, weights='uniform', metric='nan_euclidean'))])\n",
    "\n",
    "#Now I run the impute pipe on this dataframe. First I fit the pipe to the data. I record the transform of the dataframe as imputation. \n",
    "# Imputation is a numpy array, so it needs to be converted back to a pandas dataframe.\n",
    "#Also, I reverse-transformed the data. My reasoning for doing this is that we want it in terms of the original scale to be able to make sense of things. \n",
    "#But since we are scaling twice, more rounding issues arise.\n",
    "\n",
    "knn_impute_pipe.fit(train_cleaned_knn_imputed[feature_list])\n",
    "knn_imputation=knn_impute_pipe.transform(train_cleaned_knn_imputed[feature_list])\n",
    "knn_imputation=knn_impute_pipe.named_steps['scale'].inverse_transform(knn_imputation)\n",
    "df2 = pd.DataFrame(knn_imputation, columns=feature_list)\n",
    "\n",
    "#Lastly, I replace the original values in the dataframe with the newly imputed values.\n",
    "\n",
    "train_cleaned_knn_imputed[feature_list]=train_cleaned[feature_list].fillna(df2[feature_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A Custom KNN Imputer**\n",
    "\n",
    "Next, we'll try to take the above code and turn it into a custom imputer that can be used inside a pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We'll need these\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "## Define our custom imputer\n",
    "class Custom_KNN_Imputer(BaseEstimator, TransformerMixin):\n",
    "    # Class Constructor \n",
    "    # This allows you to initiate the class when you call Custom_KNN_Imputer\n",
    "    def __init__(self):\n",
    "        # I want to initiate each object with both a KNNImputer and StandardScaler object/method\n",
    "        self.KNNImputer = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')\n",
    "        self.StandardScaler = StandardScaler()\n",
    "\n",
    "    \n",
    "    # For my fit method I'm just going to \"steal\" KNNImputers's fit method using a curated collection of predictors\n",
    "    def fit(self, X, y = None ):\n",
    "        feature_list = X.columns.tolist()\n",
    "        if 'id' in feature_list:\n",
    "            feature_list.remove('id')\n",
    "        if 'sii' in feature_list:\n",
    "            feature_list.remove('sii')\n",
    "        feature_list = [x for x in feature_list if 'PCIAT' not in x]\n",
    "        feature_list = [x for x in feature_list if 'Zone' not in x]\n",
    "        feature_list = [x for x in feature_list if 'Season' not in x]\n",
    "        self.StandardScaler.fit(X[feature_list])\n",
    "        # I'm never sure if we need the .values and/or .reshape(-1,1)\n",
    "        #self.KNNImputer.fit(X[feature_list].values.reshape(-1,1))\n",
    "        self.KNNImputer.fit(X[feature_list])\n",
    "        return self\n",
    "    \n",
    "    # Now I want to transform the columns in feature list and return it with imputed values that have been un-transformed\n",
    "    def transform(self, X, y = None):\n",
    "        copy_X = X.copy()\n",
    "        copy_X[feature_list] = self.KNNImputer.transform(copy_X[feature_list])\n",
    "        copy_X2 = self.StandardScaler.inverse_transform(copy_X[feature_list])\n",
    "        df2 = pd.DataFrame(copy_X2, columns=feature_list)\n",
    "        copy_X[feature_list]=copy_X[feature_list].fillna(df2[feature_list])\n",
    "        return copy_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it out\n",
    "\n",
    "imp_knn = Custom_KNN_Imputer()\n",
    "\n",
    "df_imp_knn = pd.DataFrame(imp_knn.fit_transform(train_cleaned))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using MICE to Impute Predictor Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am going to define a new dataframe that will record all of the imputations using MICE. I only want to apply MICE to the input variables, so I separate those out.\n",
    "#Also, MICE doesn't like categorical variables. I have just removed those--the seasons--for now.\n",
    "\n",
    "#New packages needed.\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "#Because we will be using multiple imputation strategies, \n",
    "# I am going to define a new dataframe that will record all of the imputations using KNN.\n",
    "train_imp_MICE=train_cleaned.copy()\n",
    "\n",
    "\n",
    "# Create a list of columns that doesn't include id, sii, PCIAT, Zone, or Season\n",
    "# This is written in a way to avoid exceptions in case one of the columns is missing\n",
    "feature_list = train_imp_MICE.columns.tolist()\n",
    "if 'id' in feature_list:\n",
    "    feature_list.remove('id')\n",
    "if 'sii' in feature_list:\n",
    "    feature_list.remove('sii')\n",
    "feature_list = [x for x in feature_list if 'PCIAT' not in x]\n",
    "feature_list = [x for x in feature_list if 'Zone' not in x]\n",
    "feature_list = [x for x in feature_list if 'Season' not in x]\n",
    "\n",
    "df=train_imp_MICE[feature_list]\n",
    "\n",
    "#IterativeImputer has a bunch of options, including what type of regression is used for the imputation. Here, I've just gone with the default.\n",
    "\n",
    "imputer = IterativeImputer(max_iter=10, random_state=497)\n",
    "\n",
    "df2= imputer.fit_transform(df)\n",
    "\n",
    "df3 = pd.DataFrame(df2, columns=feature_list)\n",
    "\n",
    "#Now I fill in the missing values in train_imp_MICE with the MICE-imputed values. I am still using KNN for the pciats values. \n",
    "\n",
    "train_imp_MICE[feature_list]=train_imp_MICE[feature_list].fillna(df3[feature_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A Custom MICE Imputer**\n",
    "\n",
    "Next, we'll try to take the above code and turn it into a custom imputer that can be used inside a pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We'll need these\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "## Define our custom imputer\n",
    "class Custom_MICE_Imputer(BaseEstimator, TransformerMixin):\n",
    "    # Class Constructor \n",
    "    # This allows you to initiate the class when you call Custom_KNN_Imputer\n",
    "    def __init__(self):\n",
    "        # I want to initiate each object with both a KNNImputer and StandardScaler object/method\n",
    "        self.MICEImputer = IterativeImputer(max_iter=10, random_state=497)\n",
    "\n",
    "    \n",
    "    # For my fit method I'm just going to \"steal\" IterativeImputers's fit method using a curated collection of predictors\n",
    "    def fit(self, X, y = None ):\n",
    "        feature_list = X.columns.tolist()\n",
    "        if 'id' in feature_list:\n",
    "            feature_list.remove('id')\n",
    "        if 'sii' in feature_list:\n",
    "            feature_list.remove('sii')\n",
    "        feature_list = [x for x in feature_list if 'PCIAT' not in x]\n",
    "        feature_list = [x for x in feature_list if 'Zone' not in x]\n",
    "        feature_list = [x for x in feature_list if 'Season' not in x]\n",
    "        self.MICEImputer.fit(X[feature_list])\n",
    "        return self\n",
    "    \n",
    "    # Now I want to transform the columns in feature list and return it with imputed values that have been un-transformed\n",
    "    def transform(self, X, y = None):\n",
    "        copy_X = X.copy()\n",
    "        df2 = self.MICEImputer.transform(copy_X[feature_list])\n",
    "        df3 = pd.DataFrame(df2, columns=feature_list)\n",
    "        copy_X[feature_list]=copy_X[feature_list].fillna(df3[feature_list])\n",
    "        return copy_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it out\n",
    "\n",
    "imp_mice = Custom_MICE_Imputer()\n",
    "\n",
    "df_imp_mice = pd.DataFrame(imp_mice.fit_transform(train_cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values in train_cleaned: 85609\n",
      "Number of NaN values in df_imp_knn: 42438\n",
      "Number of NaN values in df_imp_mice: 42438\n"
     ]
    }
   ],
   "source": [
    "#Compute the number of NaN values in train_clained, df_imp_knn and df_imp_mice\n",
    "print('Number of NaN values in train_cleaned:', train_cleaned.isnull().sum().sum())\n",
    "print('Number of NaN values in df_imp_knn:', df_imp_knn.isnull().sum().sum())\n",
    "print('Number of NaN values in df_imp_mice:', df_imp_mice.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computing Zone Values**\n",
    "\n",
    "In this section, we'll create functions that compute the FGC Zone and PAQ_Zone values from the corresponding FGC raw and PAQ_Total (imputed) scores\n",
    "\n",
    "FitnessGram Healthy Fitness Zones are documented at https://pftdata.org/files/hfz-standards.pdf for:\n",
    "* FGC-FGC_CU_Zone\n",
    "* FGC-FGC_PU_Zone\n",
    "* FGC-FGC_TL_Zone\n",
    "* FGC-FGC_SR_Zone\n",
    "\n",
    "FitnessGram Grip Strength Zones appear to be documented at https://www.topendsports.com/testing/norms/handgrip.htm. However, these zones are only defined for ages 10 and up. And it appears that no participants under the age of 10 had their grip strength measured. So maybe it doesn't make sense to include this predictor at all?\n",
    "\n",
    "For the PAQ numbers, some research (https://pubmed.ncbi.nlm.nih.gov/27759968/) has identified a cut-off score of 2.75 (ages 14-20) and 2.73 (ages 8-14) to discriminate >60 minutes of MVPA. However, the study suggests that, while the cutoff is significant for the older group, it isn't for for the younger.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute values for the 'FGC-FGC_SR_Zone' that is equal to 1 if any of the following are true:\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_SR >= 8\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_SR >= 9 and Basic_Demos-Age is between 5 and 10\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_SR >= 10 and Basic_Demos-Age is between 11 and 14\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_SR >= 12 and Basic_Demos-Age is at least 15\n",
    "# Note that Basic_Demos-Sex is coded as 0=Male and 1=Female\n",
    "\n",
    "def sitreachzone(sex, age, sr):\n",
    "    try:\n",
    "        if np.isnan(sr) or np.isnan(sex) or np.isnan(age):\n",
    "            return np.nan\n",
    "        elif sex == 0 and sr>=8:\n",
    "            return 1\n",
    "        elif sex == 1 and age >= 15 and sr >= 12:\n",
    "            return 1\n",
    "        elif sex == 1 and age >= 11 and sr >= 10:\n",
    "            return 1\n",
    "        elif sex == 1 and age >= 5 and sr >= 9:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute values for the 'FGC-FGC_CU_Zone' that is equal to 1 if any of the following are true:\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_CU >= 2 and Basic_Demos-Age is between 5 and 6\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_CU >= 4 and Basic_Demos-Age is 7\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_CU >= 6 and Basic_Demos-Age is 8\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_CU >= 9 and Basic_Demos-Age is 9\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_CU >= 12 and Basic_Demos-Age is 10\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_CU >= 15 and Basic_Demos-Age is 11\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_CU >= 18 and Basic_Demos-Age is 12\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_CU >= 21 and Basic_Demos-Age is 13\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_CU >= 24 and Basic_Demos-Age is at least 14\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_CU >= 2 and Basic_Demos-Age is between 5 and 6\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_CU >= 4 and Basic_Demos-Age is 7\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_CU >= 6 and Basic_Demos-Age is 8\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_CU >= 9 and Basic_Demos-Age is 9\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_CU >= 12 and Basic_Demos-Age is 10\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_CU >= 15 and Basic_Demos-Age is 11\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_CU >= 18 and Basic_Demos-Age is at least 12\n",
    "\n",
    "def curlupzone(sex, age, cu):\n",
    "    try:\n",
    "        if np.isnan(sex) or np.isnan(age) or np.isnan(cu):\n",
    "            return np.nan\n",
    "        elif sex == 0:\n",
    "            if (age >= 14 and cu >= 24) or (age == 13 and cu >= 21) or (age == 12 and cu >= 18) or (age == 11 and cu >= 15) or (age == 10 and cu >= 12) or (age == 9 and cu >= 9) or (age == 8 and cu >= 6) or (age == 7 and cu >= 4) or (age <= 6 and cu >= 2):\n",
    "            return 1\n",
    "        elif sex == 1:\n",
    "            if (age >= 12 and cu >= 18) or (age == 11 and cu >= 15) or (age == 10 and cu >= 12) or (age == 9 and cu >= 9) or (age == 8 and cu >= 6) or (age == 7 and cu >= 4) or (age <= 6 and cu >= 2):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute values for the 'FGC-FGC_PU_Zone' that is equal to 1 if any of the following are true:\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_PU >= 3 and Basic_Demos-Age is between 5 and 6\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_PU >= 4 and Basic_Demos-Age is 7\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_PU >= 5 and Basic_Demos-Age is 8\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_PU >= 6 and Basic_Demos-Age is 9\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_PU >= 7 and Basic_Demos-Age is 10\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_PU >= 8 and Basic_Demos-Age is 11\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_PU >= 10 and Basic_Demos-Age is 12\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_PU >= 12 and Basic_Demos-Age is 13\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_PU >= 14 and Basic_Demos-Age is 14\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_PU >= 16 and Basic_Demos-Age is 15\n",
    "# Basic_Demos-Sex==0 and FGC-FGC_PU >= 18 and Basic_Demos-Age is at least 16\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_PU >= 3 and Basic_Demos-Age is between 5 and 6\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_PU >= 4 and Basic_Demos-Age is 7\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_PU >= 5 and Basic_Demos-Age is 8\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_PU >= 6 and Basic_Demos-Age is 9\n",
    "# Basic_Demos-Sex==1 and FGC-FGC_PU >= 7 and Basic_Demos-Age is at least 10\n",
    "\n",
    "def pullupzone(sex, age, pu):\n",
    "    try:\n",
    "        if np.isnan(sex) or np.isnan(age) or np.isnan(pu):\n",
    "            return np.nan\n",
    "        elif sex == 0:\n",
    "            if (age >= 16 and pu >= 18) or (age == 15 and pu >= 16) or (age == 14 and pu >= 14) or (age == 13 and pu >= 12) or (age == 12 and pu >= 10) or (age == 11 and pu >= 8) or (age == 10 and pu >= 7) or (age == 9 and pu >= 6) or (age == 8 and pu >= 5) or (age == 7 and pu >= 4) or (age <= 6 and pu >= 2):\n",
    "            return 1\n",
    "        elif sex == 1:\n",
    "            if (age >= 10 and pu >= 7) or (age == 9 and pu >= 6) or (age == 8 and pu >= 5) or (age == 7 and pu >= 4) or (age <= 6 and pu >= 3):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comtlte values for the 'FGC-FGC_TL_Zone' that is equal to 1 if any of the following are true:\n",
    "# FGC-FGC_TL >= 6 and Basic_Demos-Age is between 5 and 9\n",
    "# FGC-FGC_TL >= 9 and Basic_Demos-Age is at least 10\n",
    "\n",
    "def tlzone(age, tl):\n",
    "    try:\n",
    "        if np.isnan(tl) or np.isnan(age):\n",
    "            return np.nan\n",
    "        elif (age >= 10 and tl >= 9) or (age <= 9 and tl >= 6):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comtlte values for the 'PAQ_MVPA' that is equal to 1 if any of the following are true:\n",
    "# PAQ_Total >= 2.73 and Basic_Demos-Age is between 5 and 13\n",
    "# PAQ_Total >= 2.75 and Basic_Demos-Age is at least 14\n",
    "\n",
    "def paqzone(age, paq):\n",
    "    try:\n",
    "        if np.isnan(paq) or np.isnan(age):\n",
    "            return np.nan\n",
    "        elif (age >= 14 and paq >= 2.75) or (age <= 13 and paq >= 2.73):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A Custom Encoder for Zone Variables**\n",
    "\n",
    "The goal of this next section is to define a function that will take in a dataframe and return one with the codes for the Zone variables based on the functions defined above\n",
    "\n",
    "It's possible that the dataframe might lack and age, sex, or one of the raw \"score\" variables that we'd use to do this encoding, so the encoder will need to check for the presence of these variables.\n",
    "\n",
    "If one of these variables is missing, then we'll need to decide what to do. One option is to drop the Zone variable. Another is to impute values, although we'd need to decide how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zone_encoder(df):\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # first check to see if age and sex are among the columns of df_copy\n",
    "    if 'Basic_Demos-Age' not in df_copy.columns or 'Basic_Demos-Sex' not in df_copy.columns:\n",
    "        raise ValueError('Basic_Demos-Age and Basic_Demos-Sex not present')\n",
    "    else:\n",
    "        # Check to see if FGC-FGC_SR_Zone is in the columns of df_copy\n",
    "        if 'FGC-FGC_SR_Zone' in df_copy.columns:\n",
    "            # check to see if GC-FGC_SR is in the columns of df_copy\n",
    "            if 'FGC-FGC_SR' in df_copy.columns:\n",
    "                df_copy['FGC-FGC_SR_Zone'] = df_copy.apply(lambda x: sitreachzone(x['Basic_Demos-Sex'], x['Basic_Demos-Age'], x['FGC-FGC_SR']), axis=1)\n",
    "            else: \n",
    "        if 'FGC-FGC_CU_Zone' in df_copy.columns:\n",
    "            if 'FGC-FGC_CU' in df_copy.columns:\n",
    "                df_copy['FGC-FGC_CU_Zone'] = df_copy.apply(lambda x: curlupzone(x['Basic_Demos-Sex'], x['Basic_Demos-Age'], x['FGC-FGC_CU']), axis=1)\n",
    "            else:     \n",
    "         if 'FGC-FGC_PU_Zone' in df_copy.columns:\n",
    "            if 'FGC-FGC_PU' in df_copy.columns:\n",
    "                df_copy['FGC-FGC_PU_Zone'] = df_copy.apply(lambda x: pullupzone(x['Basic_Demos-Sex'], x['Basic_Demos-Age'], x['FGC-FGC_PU']), axis=1)\n",
    "            else:     \n",
    "         if 'FGC-FGC_TL_Zone' in df_copy.columns:\n",
    "            if 'FGC-FGC_TL' in df_copy.columns:\n",
    "                df_copy['FGC-FGC_TL_Zone'] = df_copy.apply(lambda x: tlzone(x['Basic_Demos-Sex'], x['Basic_Demos-Age'], x['FGC-FGC_TL']), axis=1)\n",
    "            else:   \n",
    "         if 'PAQ_Zone' in df_copy.columns:\n",
    "            if 'PAQ_Total' in df_copy.columns:\n",
    "                df_copy['PAQ_Zone'] = df_copy.apply(lambda x: paqzone(x['Basic_Demos-Sex'], x['Basic_Demos-Age'], x['PAQ_Total']), axis=1)\n",
    "            else:   \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can now wrap the function `zone_encoder` in the `FunctionTransformer` object to turn it into a transformer object that does the one hot encoding we would like.\n",
    "zone_transformer = FunctionTransformer(zone_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a Pipeline with the Custom Imputer and Transformer**\n",
    "\n",
    "Below is some code that is based on the 2_More_Advanced_Pipelines notebook from optional_extra_practice in Week 3\n",
    "\n",
    "In that code, their desired pipeline was:\n",
    "1 Impute the missing values of `body_mass_g` with the `median` value,\n",
    "2 Impute the missing values of `sex` with the most common value,\n",
    "3 One hot encode `island` and `sex` and\n",
    "4 Fit a random forest model to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "predictors = train_cleaned.columns.tolist()\n",
    "if 'id' in feature_list:\n",
    "    feature_list.remove('id')\n",
    "if 'sii' in feature_list:\n",
    "    feature_list.remove('sii')\n",
    "feature_list = [x for x in feature_list if 'PCIAT' not in x]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pipe = Pipeline([('knn_impute', KNNImputer()),\n",
    "                    ('add_zones', zone_transformer()),\n",
    "                    ('rf', RandomForestRegressor(n_estimators = 300, max_features = 'sqrt', max_depth = 5, random_state = 216))])\n",
    "\n",
    "pipe.fit(,\n",
    "         peng_train['species'])\n",
    "\n",
    "train_pred = pipe.predict(peng_train[['bill_length_mm', 'bill_depth_mm',\n",
    "                       'flipper_length_mm', 'body_mass_g',\n",
    "                       'island', 'sex']])\n",
    "\n",
    "pipe.predict(peng_train[['bill_length_mm', 'bill_depth_mm',\n",
    "                       'flipper_length_mm', 'body_mass_g',\n",
    "                       'island', 'sex']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_fall_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
